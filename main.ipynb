{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b025d9a-e21d-4d29-a481-850c238f09c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (11.2.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (0.25.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.15.2)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2025.3.30)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy matplotlib pillow torch torchvision scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec6565-4526-4d7e-8cba-3235892c22c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Libraries imported.\n",
      "[INFO] Setting up advanced training...\n",
      "[INFO] Loaded 3450 training images and 100 validation images.\n",
      "[INFO] Using 2 GPUs\n",
      "[INFO] Advanced training setup complete.\n",
      "[INFO] Starting advanced training...\n",
      "Epoch 1/150 | Batch 50/3450 | Loss: 0.5687\n",
      "Epoch 1/150 | Batch 100/3450 | Loss: 0.5586\n",
      "Epoch 1/150 | Batch 150/3450 | Loss: 0.3954\n",
      "Epoch 1/150 | Batch 200/3450 | Loss: 0.2434\n",
      "Epoch 1/150 | Batch 250/3450 | Loss: 0.2952\n",
      "Epoch 1/150 | Batch 300/3450 | Loss: 0.2663\n",
      "Epoch 1/150 | Batch 350/3450 | Loss: 0.1907\n",
      "Epoch 1/150 | Batch 400/3450 | Loss: 0.1639\n",
      "Epoch 1/150 | Batch 450/3450 | Loss: 0.1559\n",
      "Epoch 1/150 | Batch 500/3450 | Loss: 0.1640\n",
      "Epoch 1/150 | Batch 550/3450 | Loss: 0.1106\n",
      "Epoch 1/150 | Batch 600/3450 | Loss: 0.1088\n",
      "Epoch 1/150 | Batch 650/3450 | Loss: 0.0998\n",
      "Epoch 1/150 | Batch 700/3450 | Loss: 0.0862\n",
      "Epoch 1/150 | Batch 750/3450 | Loss: 0.1430\n",
      "Epoch 1/150 | Batch 800/3450 | Loss: 0.1092\n",
      "Epoch 1/150 | Batch 850/3450 | Loss: 0.0807\n",
      "Epoch 1/150 | Batch 900/3450 | Loss: 0.1334\n",
      "Epoch 1/150 | Batch 950/3450 | Loss: 0.0727\n",
      "Epoch 1/150 | Batch 1000/3450 | Loss: 0.0743\n",
      "Epoch 1/150 | Batch 1050/3450 | Loss: 0.0887\n",
      "Epoch 1/150 | Batch 1100/3450 | Loss: 0.0634\n",
      "Epoch 1/150 | Batch 1150/3450 | Loss: 0.1538\n",
      "Epoch 1/150 | Batch 1200/3450 | Loss: 0.0989\n",
      "Epoch 1/150 | Batch 1250/3450 | Loss: 0.1211\n",
      "Epoch 1/150 | Batch 1300/3450 | Loss: 0.0623\n",
      "Epoch 1/150 | Batch 1350/3450 | Loss: 0.0905\n",
      "Epoch 1/150 | Batch 1400/3450 | Loss: 0.0440\n",
      "Epoch 1/150 | Batch 1450/3450 | Loss: 0.1077\n",
      "Epoch 1/150 | Batch 1500/3450 | Loss: 0.0697\n",
      "Epoch 1/150 | Batch 1550/3450 | Loss: 0.0765\n",
      "Epoch 1/150 | Batch 1600/3450 | Loss: 0.1376\n",
      "Epoch 1/150 | Batch 1650/3450 | Loss: 0.0525\n",
      "Epoch 1/150 | Batch 1700/3450 | Loss: 0.0757\n",
      "Epoch 1/150 | Batch 1750/3450 | Loss: 0.0769\n",
      "Epoch 1/150 | Batch 1800/3450 | Loss: 0.0924\n",
      "Epoch 1/150 | Batch 1850/3450 | Loss: 0.0813\n",
      "Epoch 1/150 | Batch 1900/3450 | Loss: 0.0874\n",
      "Epoch 1/150 | Batch 1950/3450 | Loss: 0.0826\n",
      "Epoch 1/150 | Batch 2000/3450 | Loss: 0.0695\n",
      "Epoch 1/150 | Batch 2050/3450 | Loss: 0.0919\n",
      "Epoch 1/150 | Batch 2100/3450 | Loss: 0.0785\n",
      "Epoch 1/150 | Batch 2150/3450 | Loss: 0.0900\n",
      "Epoch 1/150 | Batch 2200/3450 | Loss: 0.0497\n",
      "Epoch 1/150 | Batch 2250/3450 | Loss: 0.0635\n",
      "Epoch 1/150 | Batch 2300/3450 | Loss: 0.0305\n",
      "Epoch 1/150 | Batch 2350/3450 | Loss: 0.0598\n",
      "Epoch 1/150 | Batch 2400/3450 | Loss: 0.0836\n",
      "Epoch 1/150 | Batch 2450/3450 | Loss: 0.0419\n",
      "Epoch 1/150 | Batch 2500/3450 | Loss: 0.0457\n",
      "Epoch 1/150 | Batch 2550/3450 | Loss: 0.0474\n",
      "Epoch 1/150 | Batch 2600/3450 | Loss: 0.0581\n",
      "Epoch 1/150 | Batch 2650/3450 | Loss: 0.0890\n",
      "Epoch 1/150 | Batch 2700/3450 | Loss: 0.0848\n",
      "Epoch 1/150 | Batch 2750/3450 | Loss: 0.0531\n",
      "Epoch 1/150 | Batch 2800/3450 | Loss: 0.0393\n",
      "Epoch 1/150 | Batch 2850/3450 | Loss: 0.0674\n",
      "Epoch 1/150 | Batch 2900/3450 | Loss: 0.0403\n",
      "Epoch 1/150 | Batch 2950/3450 | Loss: 0.0432\n",
      "Epoch 1/150 | Batch 3000/3450 | Loss: 0.0787\n",
      "Epoch 1/150 | Batch 3050/3450 | Loss: 0.0559\n",
      "Epoch 1/150 | Batch 3100/3450 | Loss: 0.0380\n",
      "Epoch 1/150 | Batch 3150/3450 | Loss: 0.0261\n",
      "Epoch 1/150 | Batch 3200/3450 | Loss: 0.0391\n",
      "Epoch 1/150 | Batch 3250/3450 | Loss: 0.0346\n",
      "Epoch 1/150 | Batch 3300/3450 | Loss: 0.1055\n",
      "Epoch 1/150 | Batch 3350/3450 | Loss: 0.0932\n",
      "Epoch 1/150 | Batch 3400/3450 | Loss: 0.0867\n",
      "Epoch 1/150 | Batch 3450/3450 | Loss: 0.0522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:09<00:00, 60.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150 Summary:\n",
      "Time: 5103.60s | Total: 1:25:03\n",
      "LR: 0.00004000\n",
      "Train Loss: 0.1081\n",
      "Val Loss: 0.0500\n",
      "PSNR: 22.91 | SSIM: 0.7033 | LPIPS: 3.7862\n",
      "[INFO] New best model saved with PSNR: 22.91\n",
      "Best PSNR so far: 22.91 at epoch 1\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2/150 | Batch 50/3450 | Loss: 0.0463\n",
      "Epoch 2/150 | Batch 100/3450 | Loss: 0.0745\n",
      "Epoch 2/150 | Batch 150/3450 | Loss: 0.0411\n",
      "Epoch 2/150 | Batch 200/3450 | Loss: 0.0518\n",
      "Epoch 2/150 | Batch 250/3450 | Loss: 0.0567\n",
      "Epoch 2/150 | Batch 300/3450 | Loss: 0.0668\n",
      "Epoch 2/150 | Batch 350/3450 | Loss: 0.0662\n",
      "Epoch 2/150 | Batch 400/3450 | Loss: 0.0483\n",
      "Epoch 2/150 | Batch 450/3450 | Loss: 0.0256\n",
      "Epoch 2/150 | Batch 500/3450 | Loss: 0.0539\n",
      "Epoch 2/150 | Batch 550/3450 | Loss: 0.0452\n",
      "Epoch 2/150 | Batch 600/3450 | Loss: 0.0335\n",
      "Epoch 2/150 | Batch 650/3450 | Loss: 0.0410\n",
      "Epoch 2/150 | Batch 700/3450 | Loss: 0.0327\n",
      "Epoch 2/150 | Batch 750/3450 | Loss: 0.0397\n",
      "Epoch 2/150 | Batch 800/3450 | Loss: 0.0525\n",
      "Epoch 2/150 | Batch 850/3450 | Loss: 0.0695\n",
      "Epoch 2/150 | Batch 900/3450 | Loss: 0.0374\n",
      "Epoch 2/150 | Batch 950/3450 | Loss: 0.0561\n",
      "Epoch 2/150 | Batch 1000/3450 | Loss: 0.0811\n",
      "Epoch 2/150 | Batch 1050/3450 | Loss: 0.0505\n",
      "Epoch 2/150 | Batch 1100/3450 | Loss: 0.0270\n",
      "Epoch 2/150 | Batch 1150/3450 | Loss: 0.0511\n",
      "Epoch 2/150 | Batch 1200/3450 | Loss: 0.0569\n",
      "Epoch 2/150 | Batch 1250/3450 | Loss: 0.0585\n",
      "Epoch 2/150 | Batch 1300/3450 | Loss: 0.0672\n",
      "Epoch 2/150 | Batch 1350/3450 | Loss: 0.0515\n",
      "Epoch 2/150 | Batch 1400/3450 | Loss: 0.0412\n",
      "Epoch 2/150 | Batch 1450/3450 | Loss: 0.0719\n",
      "Epoch 2/150 | Batch 1500/3450 | Loss: 0.0287\n",
      "Epoch 2/150 | Batch 1550/3450 | Loss: 0.0310\n",
      "Epoch 2/150 | Batch 1600/3450 | Loss: 0.0457\n",
      "Epoch 2/150 | Batch 1650/3450 | Loss: 0.0424\n",
      "Epoch 2/150 | Batch 1700/3450 | Loss: 0.0427\n",
      "Epoch 2/150 | Batch 1750/3450 | Loss: 0.0693\n",
      "Epoch 2/150 | Batch 1800/3450 | Loss: 0.0425\n",
      "Epoch 2/150 | Batch 1850/3450 | Loss: 0.0839\n",
      "Epoch 2/150 | Batch 1900/3450 | Loss: 0.0365\n",
      "Epoch 2/150 | Batch 1950/3450 | Loss: 0.0905\n",
      "Epoch 2/150 | Batch 2000/3450 | Loss: 0.0349\n",
      "Epoch 2/150 | Batch 2050/3450 | Loss: 0.0702\n",
      "Epoch 2/150 | Batch 2100/3450 | Loss: 0.0380\n",
      "Epoch 2/150 | Batch 2150/3450 | Loss: 0.0539\n",
      "Epoch 2/150 | Batch 2200/3450 | Loss: 0.0367\n",
      "Epoch 2/150 | Batch 2250/3450 | Loss: 0.0349\n",
      "Epoch 2/150 | Batch 2300/3450 | Loss: 0.0573\n",
      "Epoch 2/150 | Batch 2350/3450 | Loss: 0.1167\n",
      "Epoch 2/150 | Batch 2400/3450 | Loss: 0.0587\n",
      "Epoch 2/150 | Batch 2450/3450 | Loss: 0.0893\n",
      "Epoch 2/150 | Batch 2500/3450 | Loss: 0.0427\n",
      "Epoch 2/150 | Batch 2550/3450 | Loss: 0.1038\n",
      "Epoch 2/150 | Batch 2600/3450 | Loss: 0.0633\n",
      "Epoch 2/150 | Batch 2650/3450 | Loss: 0.0425\n",
      "Epoch 2/150 | Batch 2700/3450 | Loss: 0.0442\n",
      "Epoch 2/150 | Batch 2750/3450 | Loss: 0.0447\n",
      "Epoch 2/150 | Batch 2800/3450 | Loss: 0.0625\n",
      "Epoch 2/150 | Batch 2850/3450 | Loss: 0.0600\n",
      "Epoch 2/150 | Batch 2900/3450 | Loss: 0.0347\n",
      "Epoch 2/150 | Batch 2950/3450 | Loss: 0.0357\n",
      "Epoch 2/150 | Batch 3000/3450 | Loss: 0.0727\n",
      "Epoch 2/150 | Batch 3050/3450 | Loss: 0.0705\n",
      "Epoch 2/150 | Batch 3100/3450 | Loss: 0.0323\n",
      "Epoch 2/150 | Batch 3150/3450 | Loss: 0.0407\n",
      "Epoch 2/150 | Batch 3200/3450 | Loss: 0.0598\n",
      "Epoch 2/150 | Batch 3250/3450 | Loss: 0.0411\n",
      "Epoch 2/150 | Batch 3300/3450 | Loss: 0.0480\n",
      "Epoch 2/150 | Batch 3350/3450 | Loss: 0.0416\n",
      "Epoch 2/150 | Batch 3400/3450 | Loss: 0.0413\n",
      "Epoch 2/150 | Batch 3450/3450 | Loss: 0.0329\n",
      "\n",
      "Epoch 2/150 Summary:\n",
      "Time: 5034.75s | Total: 2:49:00\n",
      "LR: 0.00008000\n",
      "Train Loss: 0.0548\n",
      "Val Loss: 0.0344\n",
      "PSNR: 25.66 | SSIM: 0.7867 | LPIPS: 3.3265\n",
      "[INFO] New best model saved with PSNR: 25.66\n",
      "Best PSNR so far: 25.66 at epoch 2\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3/150 | Batch 50/3450 | Loss: 0.0204\n",
      "Epoch 3/150 | Batch 100/3450 | Loss: 0.0334\n",
      "Epoch 3/150 | Batch 150/3450 | Loss: 0.0635\n",
      "Epoch 3/150 | Batch 200/3450 | Loss: 0.0453\n",
      "Epoch 3/150 | Batch 250/3450 | Loss: 0.0534\n",
      "Epoch 3/150 | Batch 300/3450 | Loss: 0.0686\n",
      "Epoch 3/150 | Batch 350/3450 | Loss: 0.0469\n",
      "Epoch 3/150 | Batch 400/3450 | Loss: 0.0299\n",
      "Epoch 3/150 | Batch 450/3450 | Loss: 0.0358\n",
      "Epoch 3/150 | Batch 500/3450 | Loss: 0.0342\n",
      "Epoch 3/150 | Batch 550/3450 | Loss: 0.0845\n",
      "Epoch 3/150 | Batch 600/3450 | Loss: 0.0424\n",
      "Epoch 3/150 | Batch 650/3450 | Loss: 0.0824\n",
      "Epoch 3/150 | Batch 700/3450 | Loss: 0.0484\n",
      "Epoch 3/150 | Batch 750/3450 | Loss: 0.0448\n",
      "Epoch 3/150 | Batch 800/3450 | Loss: 0.0553\n",
      "Epoch 3/150 | Batch 850/3450 | Loss: 0.0445\n",
      "Epoch 3/150 | Batch 900/3450 | Loss: 0.0374\n",
      "Epoch 3/150 | Batch 950/3450 | Loss: 0.0406\n",
      "Epoch 3/150 | Batch 1000/3450 | Loss: 0.0524\n",
      "Epoch 3/150 | Batch 1050/3450 | Loss: 0.0517\n",
      "Epoch 3/150 | Batch 1100/3450 | Loss: 0.0485\n",
      "Epoch 3/150 | Batch 1150/3450 | Loss: 0.0396\n",
      "Epoch 3/150 | Batch 1200/3450 | Loss: 0.0944\n",
      "Epoch 3/150 | Batch 1250/3450 | Loss: 0.0450\n",
      "Epoch 3/150 | Batch 1300/3450 | Loss: 0.0437\n",
      "Epoch 3/150 | Batch 1350/3450 | Loss: 0.0376\n",
      "Epoch 3/150 | Batch 1400/3450 | Loss: 0.0345\n",
      "Epoch 3/150 | Batch 1450/3450 | Loss: 0.0511\n",
      "Epoch 3/150 | Batch 1500/3450 | Loss: 0.0500\n",
      "Epoch 3/150 | Batch 1550/3450 | Loss: 0.0350\n",
      "Epoch 3/150 | Batch 1600/3450 | Loss: 0.0384\n",
      "Epoch 3/150 | Batch 1650/3450 | Loss: 0.0465\n",
      "Epoch 3/150 | Batch 1700/3450 | Loss: 0.0161\n",
      "Epoch 3/150 | Batch 1750/3450 | Loss: 0.0549\n",
      "Epoch 3/150 | Batch 1800/3450 | Loss: 0.0309\n",
      "Epoch 3/150 | Batch 1850/3450 | Loss: 0.0718\n",
      "Epoch 3/150 | Batch 1900/3450 | Loss: 0.0527\n",
      "Epoch 3/150 | Batch 1950/3450 | Loss: 0.0766\n",
      "Epoch 3/150 | Batch 2000/3450 | Loss: 0.0408\n",
      "Epoch 3/150 | Batch 2050/3450 | Loss: 0.0171\n",
      "Epoch 3/150 | Batch 2100/3450 | Loss: 0.0636\n",
      "Epoch 3/150 | Batch 2150/3450 | Loss: 0.0224\n",
      "Epoch 3/150 | Batch 2200/3450 | Loss: 0.0432\n",
      "Epoch 3/150 | Batch 2250/3450 | Loss: 0.0505\n",
      "Epoch 3/150 | Batch 2300/3450 | Loss: 0.0554\n",
      "Epoch 3/150 | Batch 2350/3450 | Loss: 0.0624\n",
      "Epoch 3/150 | Batch 2400/3450 | Loss: 0.0587\n",
      "Epoch 3/150 | Batch 2450/3450 | Loss: 0.0274\n",
      "Epoch 3/150 | Batch 2500/3450 | Loss: 0.0209\n",
      "Epoch 3/150 | Batch 2550/3450 | Loss: 0.0224\n",
      "Epoch 3/150 | Batch 2600/3450 | Loss: 0.0261\n",
      "Epoch 3/150 | Batch 2650/3450 | Loss: 0.0298\n",
      "Epoch 3/150 | Batch 2700/3450 | Loss: 0.0870\n",
      "Epoch 3/150 | Batch 2750/3450 | Loss: 0.0330\n",
      "Epoch 3/150 | Batch 2800/3450 | Loss: 0.0374\n",
      "Epoch 3/150 | Batch 2850/3450 | Loss: 0.0307\n",
      "Epoch 3/150 | Batch 2900/3450 | Loss: 0.0296\n",
      "Epoch 3/150 | Batch 2950/3450 | Loss: 0.0369\n",
      "Epoch 3/150 | Batch 3000/3450 | Loss: 0.0270\n",
      "Epoch 3/150 | Batch 3050/3450 | Loss: 0.0265\n",
      "Epoch 3/150 | Batch 3100/3450 | Loss: 0.0692\n",
      "Epoch 3/150 | Batch 3150/3450 | Loss: 0.0999\n",
      "Epoch 3/150 | Batch 3200/3450 | Loss: 0.0402\n",
      "Epoch 3/150 | Batch 3250/3450 | Loss: 0.0350\n",
      "Epoch 3/150 | Batch 3300/3450 | Loss: 0.0617\n",
      "Epoch 3/150 | Batch 3350/3450 | Loss: 0.0384\n",
      "Epoch 3/150 | Batch 3400/3450 | Loss: 0.0273\n",
      "Epoch 3/150 | Batch 3450/3450 | Loss: 0.0338\n",
      "\n",
      "Epoch 3/150 Summary:\n",
      "Time: 5031.34s | Total: 4:12:54\n",
      "LR: 0.00012000\n",
      "Train Loss: 0.0451\n",
      "Val Loss: 0.0369\n",
      "PSNR: 25.84 | SSIM: 0.8418 | LPIPS: 2.8929\n",
      "[INFO] New best model saved with PSNR: 25.84\n",
      "Best PSNR so far: 25.84 at epoch 3\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4/150 | Batch 50/3450 | Loss: 0.0303\n",
      "Epoch 4/150 | Batch 100/3450 | Loss: 0.0593\n",
      "Epoch 4/150 | Batch 150/3450 | Loss: 0.0561\n",
      "Epoch 4/150 | Batch 200/3450 | Loss: 0.0466\n",
      "Epoch 4/150 | Batch 250/3450 | Loss: 0.0199\n",
      "Epoch 4/150 | Batch 300/3450 | Loss: 0.0244\n",
      "Epoch 4/150 | Batch 350/3450 | Loss: 0.0664\n",
      "Epoch 4/150 | Batch 400/3450 | Loss: 0.0325\n",
      "Epoch 4/150 | Batch 450/3450 | Loss: 0.0508\n",
      "Epoch 4/150 | Batch 500/3450 | Loss: 0.0446\n",
      "Epoch 4/150 | Batch 550/3450 | Loss: 0.0875\n",
      "Epoch 4/150 | Batch 600/3450 | Loss: 0.0955\n",
      "Epoch 4/150 | Batch 650/3450 | Loss: 0.0302\n",
      "Epoch 4/150 | Batch 700/3450 | Loss: 0.0770\n",
      "Epoch 4/150 | Batch 750/3450 | Loss: 0.0576\n",
      "Epoch 4/150 | Batch 800/3450 | Loss: 0.0593\n",
      "Epoch 4/150 | Batch 850/3450 | Loss: 0.0405\n",
      "Epoch 4/150 | Batch 900/3450 | Loss: 0.0347\n",
      "Epoch 4/150 | Batch 950/3450 | Loss: 0.0776\n",
      "Epoch 4/150 | Batch 1000/3450 | Loss: 0.0628\n",
      "Epoch 4/150 | Batch 1050/3450 | Loss: 0.0681\n",
      "Epoch 4/150 | Batch 1100/3450 | Loss: 0.0286\n",
      "Epoch 4/150 | Batch 1150/3450 | Loss: 0.0264\n",
      "Epoch 4/150 | Batch 1200/3450 | Loss: 0.0667\n",
      "Epoch 4/150 | Batch 1250/3450 | Loss: 0.0431\n",
      "Epoch 4/150 | Batch 1300/3450 | Loss: 0.0619\n",
      "Epoch 4/150 | Batch 1350/3450 | Loss: 0.0362\n",
      "Epoch 4/150 | Batch 1400/3450 | Loss: 0.0586\n",
      "Epoch 4/150 | Batch 1450/3450 | Loss: 0.0453\n",
      "Epoch 4/150 | Batch 1500/3450 | Loss: 0.0384\n",
      "Epoch 4/150 | Batch 1550/3450 | Loss: 0.0441\n",
      "Epoch 4/150 | Batch 1600/3450 | Loss: 0.0304\n",
      "Epoch 4/150 | Batch 1650/3450 | Loss: 0.0533\n",
      "Epoch 4/150 | Batch 1700/3450 | Loss: 0.0377\n",
      "Epoch 4/150 | Batch 1750/3450 | Loss: 0.0518\n",
      "Epoch 4/150 | Batch 1800/3450 | Loss: 0.0325\n",
      "Epoch 4/150 | Batch 1850/3450 | Loss: 0.0695\n",
      "Epoch 4/150 | Batch 1900/3450 | Loss: 0.0415\n",
      "Epoch 4/150 | Batch 1950/3450 | Loss: 0.0264\n",
      "Epoch 4/150 | Batch 2000/3450 | Loss: 0.0754\n",
      "Epoch 4/150 | Batch 2050/3450 | Loss: 0.0216\n",
      "Epoch 4/150 | Batch 2100/3450 | Loss: 0.1059\n",
      "Epoch 4/150 | Batch 2150/3450 | Loss: 0.0421\n",
      "Epoch 4/150 | Batch 2200/3450 | Loss: 0.0173\n",
      "Epoch 4/150 | Batch 2250/3450 | Loss: 0.0485\n",
      "Epoch 4/150 | Batch 2300/3450 | Loss: 0.0324\n",
      "Epoch 4/150 | Batch 2350/3450 | Loss: 0.0228\n",
      "Epoch 4/150 | Batch 2400/3450 | Loss: 0.0693\n",
      "Epoch 4/150 | Batch 2450/3450 | Loss: 0.0335\n",
      "Epoch 4/150 | Batch 2500/3450 | Loss: 0.0228\n",
      "Epoch 4/150 | Batch 2550/3450 | Loss: 0.0182\n",
      "Epoch 4/150 | Batch 2600/3450 | Loss: 0.0327\n",
      "Epoch 4/150 | Batch 2650/3450 | Loss: 0.0105\n",
      "Epoch 4/150 | Batch 2700/3450 | Loss: 0.0455\n",
      "Epoch 4/150 | Batch 2750/3450 | Loss: 0.0679\n",
      "Epoch 4/150 | Batch 2800/3450 | Loss: 0.0199\n",
      "Epoch 4/150 | Batch 2850/3450 | Loss: 0.0476\n",
      "Epoch 4/150 | Batch 2900/3450 | Loss: 0.0543\n",
      "Epoch 4/150 | Batch 2950/3450 | Loss: 0.0329\n",
      "Epoch 4/150 | Batch 3000/3450 | Loss: 0.0231\n",
      "Epoch 4/150 | Batch 3050/3450 | Loss: 0.0749\n",
      "Epoch 4/150 | Batch 3100/3450 | Loss: 0.0675\n",
      "Epoch 4/150 | Batch 3150/3450 | Loss: 0.0432\n",
      "Epoch 4/150 | Batch 3200/3450 | Loss: 0.0324\n",
      "Epoch 4/150 | Batch 3250/3450 | Loss: 0.0159\n",
      "Epoch 4/150 | Batch 3300/3450 | Loss: 0.0261\n",
      "Epoch 4/150 | Batch 3350/3450 | Loss: 0.0362\n",
      "Epoch 4/150 | Batch 3400/3450 | Loss: 0.0394\n",
      "Epoch 4/150 | Batch 3450/3450 | Loss: 0.0613\n",
      "\n",
      "Epoch 4/150 Summary:\n",
      "Time: 5031.61s | Total: 5:36:48\n",
      "LR: 0.00016000\n",
      "Train Loss: 0.0419\n",
      "Val Loss: 0.0247\n",
      "PSNR: 28.55 | SSIM: 0.8806 | LPIPS: 2.5341\n",
      "[INFO] New best model saved with PSNR: 28.55\n",
      "Best PSNR so far: 28.55 at epoch 4\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5/150 | Batch 50/3450 | Loss: 0.0576\n",
      "Epoch 5/150 | Batch 100/3450 | Loss: 0.0346\n",
      "Epoch 5/150 | Batch 150/3450 | Loss: 0.0494\n",
      "Epoch 5/150 | Batch 200/3450 | Loss: 0.0490\n",
      "Epoch 5/150 | Batch 250/3450 | Loss: 0.0371\n",
      "Epoch 5/150 | Batch 300/3450 | Loss: 0.0388\n",
      "Epoch 5/150 | Batch 350/3450 | Loss: 0.0442\n",
      "Epoch 5/150 | Batch 400/3450 | Loss: 0.0346\n",
      "Epoch 5/150 | Batch 450/3450 | Loss: 0.0285\n",
      "Epoch 5/150 | Batch 500/3450 | Loss: 0.0315\n",
      "Epoch 5/150 | Batch 550/3450 | Loss: 0.0534\n",
      "Epoch 5/150 | Batch 600/3450 | Loss: 0.0615\n",
      "Epoch 5/150 | Batch 650/3450 | Loss: 0.0502\n",
      "Epoch 5/150 | Batch 700/3450 | Loss: 0.0688\n",
      "Epoch 5/150 | Batch 750/3450 | Loss: 0.0216\n",
      "Epoch 5/150 | Batch 800/3450 | Loss: 0.0567\n",
      "Epoch 5/150 | Batch 850/3450 | Loss: 0.0204\n",
      "Epoch 5/150 | Batch 900/3450 | Loss: 0.0296\n",
      "Epoch 5/150 | Batch 950/3450 | Loss: 0.0784\n",
      "Epoch 5/150 | Batch 1000/3450 | Loss: 0.0255\n",
      "Epoch 5/150 | Batch 1050/3450 | Loss: 0.0501\n",
      "Epoch 5/150 | Batch 1100/3450 | Loss: 0.0461\n",
      "Epoch 5/150 | Batch 1150/3450 | Loss: 0.0532\n",
      "Epoch 5/150 | Batch 1200/3450 | Loss: 0.0403\n",
      "Epoch 5/150 | Batch 1250/3450 | Loss: 0.0437\n",
      "Epoch 5/150 | Batch 1300/3450 | Loss: 0.0627\n",
      "Epoch 5/150 | Batch 1350/3450 | Loss: 0.0456\n",
      "Epoch 5/150 | Batch 1400/3450 | Loss: 0.0517\n",
      "Epoch 5/150 | Batch 1450/3450 | Loss: 0.0240\n",
      "Epoch 5/150 | Batch 1500/3450 | Loss: 0.0420\n",
      "Epoch 5/150 | Batch 1550/3450 | Loss: 0.0324\n",
      "Epoch 5/150 | Batch 1600/3450 | Loss: 0.0541\n",
      "Epoch 5/150 | Batch 1650/3450 | Loss: 0.0262\n",
      "Epoch 5/150 | Batch 1700/3450 | Loss: 0.0352\n",
      "Epoch 5/150 | Batch 1750/3450 | Loss: 0.0404\n",
      "Epoch 5/150 | Batch 1800/3450 | Loss: 0.0497\n",
      "Epoch 5/150 | Batch 1850/3450 | Loss: 0.0362\n",
      "Epoch 5/150 | Batch 1900/3450 | Loss: 0.0626\n",
      "Epoch 5/150 | Batch 1950/3450 | Loss: 0.0212\n",
      "Epoch 5/150 | Batch 2000/3450 | Loss: 0.0308\n",
      "Epoch 5/150 | Batch 2050/3450 | Loss: 0.0496\n",
      "Epoch 5/150 | Batch 2100/3450 | Loss: 0.0170\n",
      "Epoch 5/150 | Batch 2150/3450 | Loss: 0.0593\n",
      "Epoch 5/150 | Batch 2200/3450 | Loss: 0.0358\n",
      "Epoch 5/150 | Batch 2250/3450 | Loss: 0.0207\n",
      "Epoch 5/150 | Batch 2300/3450 | Loss: 0.0320\n",
      "Epoch 5/150 | Batch 2350/3450 | Loss: 0.0455\n",
      "Epoch 5/150 | Batch 2400/3450 | Loss: 0.0290\n",
      "Epoch 5/150 | Batch 2450/3450 | Loss: 0.0364\n",
      "Epoch 5/150 | Batch 2500/3450 | Loss: 0.0526\n",
      "Epoch 5/150 | Batch 2550/3450 | Loss: 0.0344\n",
      "Epoch 5/150 | Batch 2600/3450 | Loss: 0.0491\n",
      "Epoch 5/150 | Batch 2650/3450 | Loss: 0.0207\n",
      "Epoch 5/150 | Batch 2700/3450 | Loss: 0.0531\n",
      "Epoch 5/150 | Batch 2750/3450 | Loss: 0.0205\n",
      "Epoch 5/150 | Batch 2800/3450 | Loss: 0.0258\n",
      "Epoch 5/150 | Batch 2850/3450 | Loss: 0.0457\n",
      "Epoch 5/150 | Batch 2900/3450 | Loss: 0.0514\n",
      "Epoch 5/150 | Batch 2950/3450 | Loss: 0.0427\n",
      "Epoch 5/150 | Batch 3000/3450 | Loss: 0.0361\n",
      "Epoch 5/150 | Batch 3050/3450 | Loss: 0.0499\n",
      "Epoch 5/150 | Batch 3100/3450 | Loss: 0.0585\n",
      "Epoch 5/150 | Batch 3150/3450 | Loss: 0.0210\n",
      "Epoch 5/150 | Batch 3200/3450 | Loss: 0.0386\n",
      "Epoch 5/150 | Batch 3250/3450 | Loss: 0.0650\n",
      "Epoch 5/150 | Batch 3300/3450 | Loss: 0.0638\n",
      "Epoch 5/150 | Batch 3350/3450 | Loss: 0.0381\n",
      "Epoch 5/150 | Batch 3400/3450 | Loss: 0.0211\n",
      "Epoch 5/150 | Batch 3450/3450 | Loss: 0.0895\n",
      "\n",
      "Epoch 5/150 Summary:\n",
      "Time: 5035.12s | Total: 7:00:46\n",
      "LR: 0.00020000\n",
      "Train Loss: 0.0410\n",
      "Val Loss: 0.0287\n",
      "PSNR: 28.17 | SSIM: 0.8856 | LPIPS: 2.4331\n",
      "Best PSNR so far: 28.55 at epoch 4\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6/150 | Batch 50/3450 | Loss: 0.0493\n",
      "Epoch 6/150 | Batch 100/3450 | Loss: 0.0604\n",
      "Epoch 6/150 | Batch 150/3450 | Loss: 0.0206\n",
      "Epoch 6/150 | Batch 200/3450 | Loss: 0.0957\n",
      "Epoch 6/150 | Batch 250/3450 | Loss: 0.0476\n",
      "Epoch 6/150 | Batch 300/3450 | Loss: 0.0564\n",
      "Epoch 6/150 | Batch 350/3450 | Loss: 0.0367\n",
      "Epoch 6/150 | Batch 400/3450 | Loss: 0.0398\n",
      "Epoch 6/150 | Batch 450/3450 | Loss: 0.0396\n",
      "Epoch 6/150 | Batch 500/3450 | Loss: 0.0299\n",
      "Epoch 6/150 | Batch 550/3450 | Loss: 0.0232\n",
      "Epoch 6/150 | Batch 600/3450 | Loss: 0.0694\n",
      "Epoch 6/150 | Batch 650/3450 | Loss: 0.0469\n",
      "Epoch 6/150 | Batch 700/3450 | Loss: 0.0316\n",
      "Epoch 6/150 | Batch 750/3450 | Loss: 0.0587\n",
      "Epoch 6/150 | Batch 800/3450 | Loss: 0.0246\n",
      "Epoch 6/150 | Batch 850/3450 | Loss: 0.0576\n",
      "Epoch 6/150 | Batch 900/3450 | Loss: 0.0279\n",
      "Epoch 6/150 | Batch 950/3450 | Loss: 0.0497\n",
      "Epoch 6/150 | Batch 1000/3450 | Loss: 0.0232\n",
      "Epoch 6/150 | Batch 1050/3450 | Loss: 0.0178\n",
      "Epoch 6/150 | Batch 1100/3450 | Loss: 0.0317\n",
      "Epoch 6/150 | Batch 1150/3450 | Loss: 0.0136\n",
      "Epoch 6/150 | Batch 1200/3450 | Loss: 0.0326\n",
      "Epoch 6/150 | Batch 1250/3450 | Loss: 0.0349\n",
      "Epoch 6/150 | Batch 1300/3450 | Loss: 0.0328\n",
      "Epoch 6/150 | Batch 1350/3450 | Loss: 0.0424\n",
      "Epoch 6/150 | Batch 1400/3450 | Loss: 0.0419\n",
      "Epoch 6/150 | Batch 1450/3450 | Loss: 0.0679\n",
      "Epoch 6/150 | Batch 1500/3450 | Loss: 0.0427\n",
      "Epoch 6/150 | Batch 1550/3450 | Loss: 0.0299\n",
      "Epoch 6/150 | Batch 1600/3450 | Loss: 0.0307\n",
      "Epoch 6/150 | Batch 1650/3450 | Loss: 0.0197\n",
      "Epoch 6/150 | Batch 1700/3450 | Loss: 0.0639\n",
      "Epoch 6/150 | Batch 1750/3450 | Loss: 0.0343\n",
      "Epoch 6/150 | Batch 1800/3450 | Loss: 0.0456\n",
      "Epoch 6/150 | Batch 1850/3450 | Loss: 0.0369\n",
      "Epoch 6/150 | Batch 1900/3450 | Loss: 0.0431\n",
      "Epoch 6/150 | Batch 1950/3450 | Loss: 0.0301\n",
      "Epoch 6/150 | Batch 2000/3450 | Loss: 0.0591\n",
      "Epoch 6/150 | Batch 2050/3450 | Loss: 0.0332\n",
      "Epoch 6/150 | Batch 2100/3450 | Loss: 0.0219\n",
      "Epoch 6/150 | Batch 2150/3450 | Loss: 0.0535\n",
      "Epoch 6/150 | Batch 2200/3450 | Loss: 0.0395\n",
      "Epoch 6/150 | Batch 2250/3450 | Loss: 0.0260\n",
      "Epoch 6/150 | Batch 2300/3450 | Loss: 0.0483\n",
      "Epoch 6/150 | Batch 2350/3450 | Loss: 0.0680\n",
      "Epoch 6/150 | Batch 2400/3450 | Loss: 0.0263\n",
      "Epoch 6/150 | Batch 2450/3450 | Loss: 0.0445\n",
      "Epoch 6/150 | Batch 2500/3450 | Loss: 0.0238\n",
      "Epoch 6/150 | Batch 2550/3450 | Loss: 0.0335\n",
      "Epoch 6/150 | Batch 2600/3450 | Loss: 0.0563\n",
      "Epoch 6/150 | Batch 2650/3450 | Loss: 0.0147\n",
      "Epoch 6/150 | Batch 2700/3450 | Loss: 0.0526\n",
      "Epoch 6/150 | Batch 2750/3450 | Loss: 0.0263\n",
      "Epoch 6/150 | Batch 2800/3450 | Loss: 0.0426\n",
      "Epoch 6/150 | Batch 2850/3450 | Loss: 0.0296\n",
      "Epoch 6/150 | Batch 2900/3450 | Loss: 0.0322\n",
      "Epoch 6/150 | Batch 2950/3450 | Loss: 0.0694\n",
      "Epoch 6/150 | Batch 3000/3450 | Loss: 0.0330\n",
      "Epoch 6/150 | Batch 3050/3450 | Loss: 0.0263\n",
      "Epoch 6/150 | Batch 3100/3450 | Loss: 0.0553\n",
      "Epoch 6/150 | Batch 3150/3450 | Loss: 0.0258\n",
      "Epoch 6/150 | Batch 3200/3450 | Loss: 0.0349\n",
      "Epoch 6/150 | Batch 3250/3450 | Loss: 0.0279\n",
      "Epoch 6/150 | Batch 3300/3450 | Loss: 0.0227\n",
      "Epoch 6/150 | Batch 3350/3450 | Loss: 0.0567\n",
      "Epoch 6/150 | Batch 3400/3450 | Loss: 0.1100\n",
      "Epoch 6/150 | Batch 3450/3450 | Loss: 0.0533\n",
      "\n",
      "Epoch 6/150 Summary:\n",
      "Time: 5037.22s | Total: 8:24:45\n",
      "LR: 0.00020000\n",
      "Train Loss: 0.0398\n",
      "Val Loss: 0.0225\n",
      "PSNR: 29.93 | SSIM: 0.8960 | LPIPS: 2.3279\n",
      "[INFO] New best model saved with PSNR: 29.93\n",
      "Best PSNR so far: 29.93 at epoch 6\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7/150 | Batch 50/3450 | Loss: 0.0506\n",
      "Epoch 7/150 | Batch 100/3450 | Loss: 0.0107\n",
      "Epoch 7/150 | Batch 150/3450 | Loss: 0.0226\n",
      "Epoch 7/150 | Batch 200/3450 | Loss: 0.0179\n",
      "Epoch 7/150 | Batch 250/3450 | Loss: 0.0187\n",
      "Epoch 7/150 | Batch 300/3450 | Loss: 0.0343\n",
      "Epoch 7/150 | Batch 350/3450 | Loss: 0.0283\n",
      "Epoch 7/150 | Batch 400/3450 | Loss: 0.0235\n",
      "Epoch 7/150 | Batch 450/3450 | Loss: 0.0566\n",
      "Epoch 7/150 | Batch 500/3450 | Loss: 0.0311\n",
      "Epoch 7/150 | Batch 550/3450 | Loss: 0.0341\n",
      "Epoch 7/150 | Batch 600/3450 | Loss: 0.0489\n",
      "Epoch 7/150 | Batch 650/3450 | Loss: 0.0521\n",
      "Epoch 7/150 | Batch 700/3450 | Loss: 0.0271\n",
      "Epoch 7/150 | Batch 750/3450 | Loss: 0.0388\n",
      "Epoch 7/150 | Batch 800/3450 | Loss: 0.0242\n",
      "Epoch 7/150 | Batch 850/3450 | Loss: 0.0391\n",
      "Epoch 7/150 | Batch 900/3450 | Loss: 0.0344\n",
      "Epoch 7/150 | Batch 950/3450 | Loss: 0.0169\n",
      "Epoch 7/150 | Batch 1000/3450 | Loss: 0.0336\n",
      "Epoch 7/150 | Batch 1050/3450 | Loss: 0.0504\n",
      "Epoch 7/150 | Batch 1100/3450 | Loss: 0.0865\n",
      "Epoch 7/150 | Batch 1150/3450 | Loss: 0.0308\n",
      "Epoch 7/150 | Batch 1200/3450 | Loss: 0.0260\n",
      "Epoch 7/150 | Batch 1250/3450 | Loss: 0.0421\n",
      "Epoch 7/150 | Batch 1300/3450 | Loss: 0.0197\n",
      "Epoch 7/150 | Batch 1350/3450 | Loss: 0.0316\n",
      "Epoch 7/150 | Batch 1400/3450 | Loss: 0.0284\n",
      "Epoch 7/150 | Batch 1450/3450 | Loss: 0.0249\n",
      "Epoch 7/150 | Batch 1500/3450 | Loss: 0.0266\n",
      "Epoch 7/150 | Batch 1550/3450 | Loss: 0.0679\n",
      "Epoch 7/150 | Batch 1600/3450 | Loss: 0.0158\n",
      "Epoch 7/150 | Batch 1650/3450 | Loss: 0.0198\n",
      "Epoch 7/150 | Batch 1700/3450 | Loss: 0.0361\n",
      "Epoch 7/150 | Batch 1750/3450 | Loss: 0.0317\n",
      "Epoch 7/150 | Batch 1800/3450 | Loss: 0.0318\n",
      "Epoch 7/150 | Batch 1850/3450 | Loss: 0.0219\n",
      "Epoch 7/150 | Batch 1900/3450 | Loss: 0.0190\n",
      "Epoch 7/150 | Batch 1950/3450 | Loss: 0.0576\n",
      "Epoch 7/150 | Batch 2000/3450 | Loss: 0.0329\n",
      "Epoch 7/150 | Batch 2050/3450 | Loss: 0.0270\n",
      "Epoch 7/150 | Batch 2100/3450 | Loss: 0.0347\n",
      "Epoch 7/150 | Batch 2150/3450 | Loss: 0.0384\n",
      "Epoch 7/150 | Batch 2200/3450 | Loss: 0.0234\n",
      "Epoch 7/150 | Batch 2250/3450 | Loss: 0.0481\n",
      "Epoch 7/150 | Batch 2300/3450 | Loss: 0.0579\n",
      "Epoch 7/150 | Batch 2350/3450 | Loss: 0.0848\n",
      "Epoch 7/150 | Batch 2400/3450 | Loss: 0.0315\n",
      "Epoch 7/150 | Batch 2450/3450 | Loss: 0.0210\n",
      "Epoch 7/150 | Batch 2500/3450 | Loss: 0.0244\n",
      "Epoch 7/150 | Batch 2550/3450 | Loss: 0.0135\n",
      "Epoch 7/150 | Batch 2600/3450 | Loss: 0.0426\n",
      "Epoch 7/150 | Batch 2650/3450 | Loss: 0.0163\n",
      "Epoch 7/150 | Batch 2700/3450 | Loss: 0.0188\n",
      "Epoch 7/150 | Batch 2750/3450 | Loss: 0.0474\n",
      "Epoch 7/150 | Batch 2800/3450 | Loss: 0.0248\n",
      "Epoch 7/150 | Batch 2850/3450 | Loss: 0.0646\n",
      "Epoch 7/150 | Batch 2900/3450 | Loss: 0.0173\n",
      "Epoch 7/150 | Batch 2950/3450 | Loss: 0.0356\n",
      "Epoch 7/150 | Batch 3000/3450 | Loss: 0.0137\n",
      "Epoch 7/150 | Batch 3050/3450 | Loss: 0.0227\n",
      "Epoch 7/150 | Batch 3100/3450 | Loss: 0.0442\n",
      "Epoch 7/150 | Batch 3150/3450 | Loss: 0.0264\n",
      "Epoch 7/150 | Batch 3200/3450 | Loss: 0.0230\n",
      "Epoch 7/150 | Batch 3250/3450 | Loss: 0.0410\n",
      "Epoch 7/150 | Batch 3300/3450 | Loss: 0.0443\n",
      "Epoch 7/150 | Batch 3350/3450 | Loss: 0.0229\n",
      "Epoch 7/150 | Batch 3400/3450 | Loss: 0.0265\n",
      "Epoch 7/150 | Batch 3450/3450 | Loss: 0.0742\n",
      "\n",
      "Epoch 7/150 Summary:\n",
      "Time: 5033.77s | Total: 9:48:41\n",
      "LR: 0.00019998\n",
      "Train Loss: 0.0381\n",
      "Val Loss: 0.0262\n",
      "PSNR: 29.21 | SSIM: 0.8868 | LPIPS: 2.3940\n",
      "Best PSNR so far: 29.93 at epoch 6\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8/150 | Batch 50/3450 | Loss: 0.0723\n",
      "Epoch 8/150 | Batch 100/3450 | Loss: 0.0349\n",
      "Epoch 8/150 | Batch 150/3450 | Loss: 0.0182\n",
      "Epoch 8/150 | Batch 200/3450 | Loss: 0.0605\n",
      "Epoch 8/150 | Batch 250/3450 | Loss: 0.0164\n",
      "Epoch 8/150 | Batch 300/3450 | Loss: 0.0275\n",
      "Epoch 8/150 | Batch 350/3450 | Loss: 0.0105\n",
      "Epoch 8/150 | Batch 400/3450 | Loss: 0.0257\n",
      "Epoch 8/150 | Batch 450/3450 | Loss: 0.0208\n",
      "Epoch 8/150 | Batch 500/3450 | Loss: 0.0512\n",
      "Epoch 8/150 | Batch 550/3450 | Loss: 0.0378\n",
      "Epoch 8/150 | Batch 600/3450 | Loss: 0.0483\n",
      "Epoch 8/150 | Batch 650/3450 | Loss: 0.0179\n",
      "Epoch 8/150 | Batch 700/3450 | Loss: 0.0310\n",
      "Epoch 8/150 | Batch 750/3450 | Loss: 0.0116\n",
      "Epoch 8/150 | Batch 800/3450 | Loss: 0.0426\n",
      "Epoch 8/150 | Batch 850/3450 | Loss: 0.0338\n",
      "Epoch 8/150 | Batch 900/3450 | Loss: 0.0585\n",
      "Epoch 8/150 | Batch 950/3450 | Loss: 0.0232\n",
      "Epoch 8/150 | Batch 1000/3450 | Loss: 0.0435\n",
      "Epoch 8/150 | Batch 1050/3450 | Loss: 0.0644\n",
      "Epoch 8/150 | Batch 1100/3450 | Loss: 0.0242\n",
      "Epoch 8/150 | Batch 1150/3450 | Loss: 0.0292\n",
      "Epoch 8/150 | Batch 1200/3450 | Loss: 0.0304\n",
      "Epoch 8/150 | Batch 1250/3450 | Loss: 0.0356\n",
      "Epoch 8/150 | Batch 1300/3450 | Loss: 0.0603\n",
      "Epoch 8/150 | Batch 1350/3450 | Loss: 0.0198\n",
      "Epoch 8/150 | Batch 1400/3450 | Loss: 0.0287\n",
      "Epoch 8/150 | Batch 1450/3450 | Loss: 0.0448\n",
      "Epoch 8/150 | Batch 1500/3450 | Loss: 0.0377\n",
      "Epoch 8/150 | Batch 1550/3450 | Loss: 0.0493\n",
      "Epoch 8/150 | Batch 1600/3450 | Loss: 0.0247\n",
      "Epoch 8/150 | Batch 1650/3450 | Loss: 0.0347\n",
      "Epoch 8/150 | Batch 1700/3450 | Loss: 0.0198\n",
      "Epoch 8/150 | Batch 1750/3450 | Loss: 0.0411\n",
      "Epoch 8/150 | Batch 1800/3450 | Loss: 0.0231\n",
      "Epoch 8/150 | Batch 1850/3450 | Loss: 0.0187\n",
      "Epoch 8/150 | Batch 1900/3450 | Loss: 0.0428\n",
      "Epoch 8/150 | Batch 1950/3450 | Loss: 0.0322\n",
      "Epoch 8/150 | Batch 2000/3450 | Loss: 0.0387\n",
      "Epoch 8/150 | Batch 2050/3450 | Loss: 0.0324\n",
      "Epoch 8/150 | Batch 2100/3450 | Loss: 0.0662\n",
      "Epoch 8/150 | Batch 2150/3450 | Loss: 0.0467\n",
      "Epoch 8/150 | Batch 2200/3450 | Loss: 0.0350\n",
      "Epoch 8/150 | Batch 2250/3450 | Loss: 0.0212\n",
      "Epoch 8/150 | Batch 2300/3450 | Loss: 0.0512\n",
      "Epoch 8/150 | Batch 2350/3450 | Loss: 0.0198\n",
      "Epoch 8/150 | Batch 2400/3450 | Loss: 0.0532\n",
      "Epoch 8/150 | Batch 2450/3450 | Loss: 0.0338\n",
      "Epoch 8/150 | Batch 2500/3450 | Loss: 0.0217\n",
      "Epoch 8/150 | Batch 2550/3450 | Loss: 0.0187\n",
      "Epoch 8/150 | Batch 2600/3450 | Loss: 0.0310\n",
      "Epoch 8/150 | Batch 2650/3450 | Loss: 0.0171\n",
      "Epoch 8/150 | Batch 2700/3450 | Loss: 0.0430\n",
      "Epoch 8/150 | Batch 2750/3450 | Loss: 0.0301\n",
      "Epoch 8/150 | Batch 2800/3450 | Loss: 0.0573\n",
      "Epoch 8/150 | Batch 2850/3450 | Loss: 0.0584\n",
      "Epoch 8/150 | Batch 2900/3450 | Loss: 0.0390\n",
      "Epoch 8/150 | Batch 2950/3450 | Loss: 0.0293\n",
      "Epoch 8/150 | Batch 3000/3450 | Loss: 0.0333\n",
      "Epoch 8/150 | Batch 3050/3450 | Loss: 0.0276\n",
      "Epoch 8/150 | Batch 3100/3450 | Loss: 0.0558\n",
      "Epoch 8/150 | Batch 3150/3450 | Loss: 0.0336\n",
      "Epoch 8/150 | Batch 3200/3450 | Loss: 0.0183\n",
      "Epoch 8/150 | Batch 3250/3450 | Loss: 0.0272\n",
      "Epoch 8/150 | Batch 3300/3450 | Loss: 0.0346\n",
      "Epoch 8/150 | Batch 3350/3450 | Loss: 0.0282\n",
      "Epoch 8/150 | Batch 3400/3450 | Loss: 0.0287\n",
      "Epoch 8/150 | Batch 3450/3450 | Loss: 0.0143\n",
      "\n",
      "Epoch 8/150 Summary:\n",
      "Time: 5024.49s | Total: 11:12:27\n",
      "LR: 0.00019991\n",
      "Train Loss: 0.0373\n",
      "Val Loss: 0.0256\n",
      "PSNR: 28.92 | SSIM: 0.8778 | LPIPS: 2.4229\n",
      "Best PSNR so far: 29.93 at epoch 6\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9/150 | Batch 50/3450 | Loss: 0.0370\n",
      "Epoch 9/150 | Batch 100/3450 | Loss: 0.0241\n",
      "Epoch 9/150 | Batch 150/3450 | Loss: 0.0304\n",
      "Epoch 9/150 | Batch 200/3450 | Loss: 0.0279\n",
      "Epoch 9/150 | Batch 250/3450 | Loss: 0.0125\n",
      "Epoch 9/150 | Batch 300/3450 | Loss: 0.0289\n",
      "Epoch 9/150 | Batch 350/3450 | Loss: 0.0580\n",
      "Epoch 9/150 | Batch 400/3450 | Loss: 0.0459\n",
      "Epoch 9/150 | Batch 450/3450 | Loss: 0.0181\n",
      "Epoch 9/150 | Batch 500/3450 | Loss: 0.0670\n",
      "Epoch 9/150 | Batch 550/3450 | Loss: 0.0641\n",
      "Epoch 9/150 | Batch 600/3450 | Loss: 0.0512\n",
      "Epoch 9/150 | Batch 650/3450 | Loss: 0.0688\n",
      "Epoch 9/150 | Batch 700/3450 | Loss: 0.0415\n",
      "Epoch 9/150 | Batch 750/3450 | Loss: 0.0176\n",
      "Epoch 9/150 | Batch 800/3450 | Loss: 0.0209\n",
      "Epoch 9/150 | Batch 850/3450 | Loss: 0.0176\n",
      "Epoch 9/150 | Batch 900/3450 | Loss: 0.0219\n",
      "Epoch 9/150 | Batch 950/3450 | Loss: 0.0481\n",
      "Epoch 9/150 | Batch 1000/3450 | Loss: 0.0251\n",
      "Epoch 9/150 | Batch 1050/3450 | Loss: 0.0339\n",
      "Epoch 9/150 | Batch 1100/3450 | Loss: 0.0420\n",
      "Epoch 9/150 | Batch 1150/3450 | Loss: 0.0442\n",
      "Epoch 9/150 | Batch 1200/3450 | Loss: 0.0343\n",
      "Epoch 9/150 | Batch 1250/3450 | Loss: 0.0234\n",
      "Epoch 9/150 | Batch 1300/3450 | Loss: 0.0642\n",
      "Epoch 9/150 | Batch 1350/3450 | Loss: 0.0815\n",
      "Epoch 9/150 | Batch 1400/3450 | Loss: 0.0950\n",
      "Epoch 9/150 | Batch 1450/3450 | Loss: 0.0531\n",
      "Epoch 9/150 | Batch 1500/3450 | Loss: 0.0166\n",
      "Epoch 9/150 | Batch 1550/3450 | Loss: 0.0325\n",
      "Epoch 9/150 | Batch 1600/3450 | Loss: 0.0382\n",
      "Epoch 9/150 | Batch 1650/3450 | Loss: 0.0092\n",
      "Epoch 9/150 | Batch 1700/3450 | Loss: 0.0342\n",
      "Epoch 9/150 | Batch 1750/3450 | Loss: 0.0247\n",
      "Epoch 9/150 | Batch 1800/3450 | Loss: 0.0787\n",
      "Epoch 9/150 | Batch 1850/3450 | Loss: 0.0214\n",
      "Epoch 9/150 | Batch 1900/3450 | Loss: 0.0267\n",
      "Epoch 9/150 | Batch 1950/3450 | Loss: 0.0149\n",
      "Epoch 9/150 | Batch 2000/3450 | Loss: 0.0788\n",
      "Epoch 9/150 | Batch 2050/3450 | Loss: 0.0267\n",
      "Epoch 9/150 | Batch 2100/3450 | Loss: 0.0794\n",
      "Epoch 9/150 | Batch 2150/3450 | Loss: 0.0521\n",
      "Epoch 9/150 | Batch 2200/3450 | Loss: 0.0382\n",
      "Epoch 9/150 | Batch 2250/3450 | Loss: 0.0470\n",
      "Epoch 9/150 | Batch 2300/3450 | Loss: 0.0534\n",
      "Epoch 9/150 | Batch 2350/3450 | Loss: 0.0289\n",
      "Epoch 9/150 | Batch 2400/3450 | Loss: 0.0515\n",
      "Epoch 9/150 | Batch 2450/3450 | Loss: 0.0151\n",
      "Epoch 9/150 | Batch 2500/3450 | Loss: 0.0495\n",
      "Epoch 9/150 | Batch 2550/3450 | Loss: 0.0428\n",
      "Epoch 9/150 | Batch 2600/3450 | Loss: 0.0483\n",
      "Epoch 9/150 | Batch 2650/3450 | Loss: 0.0383\n",
      "Epoch 9/150 | Batch 2700/3450 | Loss: 0.0240\n",
      "Epoch 9/150 | Batch 2750/3450 | Loss: 0.0192\n",
      "Epoch 9/150 | Batch 2800/3450 | Loss: 0.0290\n",
      "Epoch 9/150 | Batch 2850/3450 | Loss: 0.0169\n",
      "Epoch 9/150 | Batch 2900/3450 | Loss: 0.0462\n",
      "Epoch 9/150 | Batch 2950/3450 | Loss: 0.0423\n",
      "Epoch 9/150 | Batch 3000/3450 | Loss: 0.0404\n",
      "Epoch 9/150 | Batch 3050/3450 | Loss: 0.0252\n",
      "Epoch 9/150 | Batch 3100/3450 | Loss: 0.0714\n",
      "Epoch 9/150 | Batch 3150/3450 | Loss: 0.0373\n",
      "Epoch 9/150 | Batch 3200/3450 | Loss: 0.0619\n",
      "Epoch 9/150 | Batch 3250/3450 | Loss: 0.0628\n",
      "Epoch 9/150 | Batch 3300/3450 | Loss: 0.0218\n",
      "Epoch 9/150 | Batch 3350/3450 | Loss: 0.0674\n",
      "Epoch 9/150 | Batch 3400/3450 | Loss: 0.0341\n",
      "Epoch 9/150 | Batch 3450/3450 | Loss: 0.0185\n",
      "\n",
      "Epoch 9/150 Summary:\n",
      "Time: 5022.92s | Total: 12:36:12\n",
      "LR: 0.00019979\n",
      "Train Loss: 0.0373\n",
      "Val Loss: 0.0213\n",
      "PSNR: 30.67 | SSIM: 0.9083 | LPIPS: 2.1933\n",
      "[INFO] New best model saved with PSNR: 30.67\n",
      "Best PSNR so far: 30.67 at epoch 9\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10/150 | Batch 50/3450 | Loss: 0.0202\n",
      "Epoch 10/150 | Batch 100/3450 | Loss: 0.0363\n",
      "Epoch 10/150 | Batch 150/3450 | Loss: 0.0198\n",
      "Epoch 10/150 | Batch 200/3450 | Loss: 0.0277\n",
      "Epoch 10/150 | Batch 250/3450 | Loss: 0.0157\n",
      "Epoch 10/150 | Batch 300/3450 | Loss: 0.0594\n",
      "Epoch 10/150 | Batch 350/3450 | Loss: 0.0275\n",
      "Epoch 10/150 | Batch 400/3450 | Loss: 0.0590\n",
      "Epoch 10/150 | Batch 450/3450 | Loss: 0.0097\n",
      "Epoch 10/150 | Batch 500/3450 | Loss: 0.0136\n",
      "Epoch 10/150 | Batch 550/3450 | Loss: 0.0242\n",
      "Epoch 10/150 | Batch 600/3450 | Loss: 0.0410\n",
      "Epoch 10/150 | Batch 650/3450 | Loss: 0.0510\n",
      "Epoch 10/150 | Batch 700/3450 | Loss: 0.0711\n",
      "Epoch 10/150 | Batch 750/3450 | Loss: 0.0992\n",
      "Epoch 10/150 | Batch 800/3450 | Loss: 0.0517\n",
      "Epoch 10/150 | Batch 850/3450 | Loss: 0.0446\n",
      "Epoch 10/150 | Batch 900/3450 | Loss: 0.0322\n",
      "Epoch 10/150 | Batch 950/3450 | Loss: 0.0859\n",
      "Epoch 10/150 | Batch 1000/3450 | Loss: 0.0175\n",
      "Epoch 10/150 | Batch 1050/3450 | Loss: 0.0553\n",
      "Epoch 10/150 | Batch 1100/3450 | Loss: 0.0309\n",
      "Epoch 10/150 | Batch 1150/3450 | Loss: 0.0155\n",
      "Epoch 10/150 | Batch 1200/3450 | Loss: 0.0305\n",
      "Epoch 10/150 | Batch 1250/3450 | Loss: 0.0280\n",
      "Epoch 10/150 | Batch 1300/3450 | Loss: 0.0275\n",
      "Epoch 10/150 | Batch 1350/3450 | Loss: 0.0154\n",
      "Epoch 10/150 | Batch 1400/3450 | Loss: 0.0335\n",
      "Epoch 10/150 | Batch 1450/3450 | Loss: 0.0688\n",
      "Epoch 10/150 | Batch 1500/3450 | Loss: 0.0847\n",
      "Epoch 10/150 | Batch 1550/3450 | Loss: 0.0188\n",
      "Epoch 10/150 | Batch 1600/3450 | Loss: 0.0161\n",
      "Epoch 10/150 | Batch 1650/3450 | Loss: 0.0333\n",
      "Epoch 10/150 | Batch 1700/3450 | Loss: 0.0480\n",
      "Epoch 10/150 | Batch 1750/3450 | Loss: 0.0217\n",
      "Epoch 10/150 | Batch 1800/3450 | Loss: 0.0570\n",
      "Epoch 10/150 | Batch 1850/3450 | Loss: 0.0646\n",
      "Epoch 10/150 | Batch 1900/3450 | Loss: 0.0299\n",
      "Epoch 10/150 | Batch 1950/3450 | Loss: 0.0262\n",
      "Epoch 10/150 | Batch 2000/3450 | Loss: 0.0315\n",
      "Epoch 10/150 | Batch 2050/3450 | Loss: 0.0265\n",
      "Epoch 10/150 | Batch 2100/3450 | Loss: 0.0285\n",
      "Epoch 10/150 | Batch 2150/3450 | Loss: 0.0502\n",
      "Epoch 10/150 | Batch 2200/3450 | Loss: 0.0263\n",
      "Epoch 10/150 | Batch 2250/3450 | Loss: 0.0172\n",
      "Epoch 10/150 | Batch 2300/3450 | Loss: 0.0652\n",
      "Epoch 10/150 | Batch 2350/3450 | Loss: 0.0372\n",
      "Epoch 10/150 | Batch 2400/3450 | Loss: 0.0388\n",
      "Epoch 10/150 | Batch 2450/3450 | Loss: 0.0477\n",
      "Epoch 10/150 | Batch 2500/3450 | Loss: 0.0124\n",
      "Epoch 10/150 | Batch 2550/3450 | Loss: 0.0161\n",
      "Epoch 10/150 | Batch 2600/3450 | Loss: 0.0572\n",
      "Epoch 10/150 | Batch 2650/3450 | Loss: 0.0320\n",
      "Epoch 10/150 | Batch 2700/3450 | Loss: 0.0183\n",
      "Epoch 10/150 | Batch 2750/3450 | Loss: 0.0238\n",
      "Epoch 10/150 | Batch 2800/3450 | Loss: 0.0379\n",
      "Epoch 10/150 | Batch 2850/3450 | Loss: 0.0188\n",
      "Epoch 10/150 | Batch 2900/3450 | Loss: 0.0151\n",
      "Epoch 10/150 | Batch 2950/3450 | Loss: 0.0300\n",
      "Epoch 10/150 | Batch 3000/3450 | Loss: 0.0306\n",
      "Epoch 10/150 | Batch 3050/3450 | Loss: 0.0405\n",
      "Epoch 10/150 | Batch 3100/3450 | Loss: 0.0303\n",
      "Epoch 10/150 | Batch 3150/3450 | Loss: 0.0146\n",
      "Epoch 10/150 | Batch 3200/3450 | Loss: 0.0264\n",
      "Epoch 10/150 | Batch 3250/3450 | Loss: 0.0385\n",
      "Epoch 10/150 | Batch 3300/3450 | Loss: 0.0159\n",
      "Epoch 10/150 | Batch 3350/3450 | Loss: 0.0336\n",
      "Epoch 10/150 | Batch 3400/3450 | Loss: 0.0283\n",
      "Epoch 10/150 | Batch 3450/3450 | Loss: 0.0248\n",
      "\n",
      "Epoch 10/150 Summary:\n",
      "Time: 5024.37s | Total: 13:59:59\n",
      "LR: 0.00019962\n",
      "Train Loss: 0.0361\n",
      "Val Loss: 0.0254\n",
      "PSNR: 29.49 | SSIM: 0.9122 | LPIPS: 2.1241\n",
      "Best PSNR so far: 30.67 at epoch 9\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11/150 | Batch 50/3450 | Loss: 0.0246\n",
      "Epoch 11/150 | Batch 100/3450 | Loss: 0.0165\n",
      "Epoch 11/150 | Batch 150/3450 | Loss: 0.0102\n",
      "Epoch 11/150 | Batch 200/3450 | Loss: 0.0185\n",
      "Epoch 11/150 | Batch 250/3450 | Loss: 0.0260\n",
      "Epoch 11/150 | Batch 300/3450 | Loss: 0.0163\n",
      "Epoch 11/150 | Batch 350/3450 | Loss: 0.0365\n",
      "Epoch 11/150 | Batch 400/3450 | Loss: 0.0200\n",
      "Epoch 11/150 | Batch 450/3450 | Loss: 0.0220\n",
      "Epoch 11/150 | Batch 500/3450 | Loss: 0.0653\n",
      "Epoch 11/150 | Batch 550/3450 | Loss: 0.0324\n",
      "Epoch 11/150 | Batch 600/3450 | Loss: 0.0359\n",
      "Epoch 11/150 | Batch 650/3450 | Loss: 0.0487\n",
      "Epoch 11/150 | Batch 700/3450 | Loss: 0.0297\n",
      "Epoch 11/150 | Batch 750/3450 | Loss: 0.0281\n",
      "Epoch 11/150 | Batch 800/3450 | Loss: 0.0420\n",
      "Epoch 11/150 | Batch 850/3450 | Loss: 0.0382\n",
      "Epoch 11/150 | Batch 900/3450 | Loss: 0.0235\n",
      "Epoch 11/150 | Batch 950/3450 | Loss: 0.0191\n",
      "Epoch 11/150 | Batch 1000/3450 | Loss: 0.0752\n",
      "Epoch 11/150 | Batch 1050/3450 | Loss: 0.0395\n",
      "Epoch 11/150 | Batch 1100/3450 | Loss: 0.0760\n",
      "Epoch 11/150 | Batch 1150/3450 | Loss: 0.0226\n",
      "Epoch 11/150 | Batch 1200/3450 | Loss: 0.0162\n",
      "Epoch 11/150 | Batch 1250/3450 | Loss: 0.0517\n",
      "Epoch 11/150 | Batch 1300/3450 | Loss: 0.0215\n",
      "Epoch 11/150 | Batch 1350/3450 | Loss: 0.0590\n",
      "Epoch 11/150 | Batch 1400/3450 | Loss: 0.0198\n",
      "Epoch 11/150 | Batch 1450/3450 | Loss: 0.0494\n",
      "Epoch 11/150 | Batch 1500/3450 | Loss: 0.0118\n",
      "Epoch 11/150 | Batch 1550/3450 | Loss: 0.0205\n",
      "Epoch 11/150 | Batch 1600/3450 | Loss: 0.0222\n",
      "Epoch 11/150 | Batch 1650/3450 | Loss: 0.0107\n",
      "Epoch 11/150 | Batch 1700/3450 | Loss: 0.0185\n",
      "Epoch 11/150 | Batch 1750/3450 | Loss: 0.0298\n",
      "Epoch 11/150 | Batch 1800/3450 | Loss: 0.0346\n",
      "Epoch 11/150 | Batch 1850/3450 | Loss: 0.0398\n",
      "Epoch 11/150 | Batch 1900/3450 | Loss: 0.0385\n",
      "Epoch 11/150 | Batch 1950/3450 | Loss: 0.0260\n",
      "Epoch 11/150 | Batch 2000/3450 | Loss: 0.0255\n",
      "Epoch 11/150 | Batch 2050/3450 | Loss: 0.0152\n",
      "Epoch 11/150 | Batch 2100/3450 | Loss: 0.0209\n",
      "Epoch 11/150 | Batch 2150/3450 | Loss: 0.0521\n",
      "Epoch 11/150 | Batch 2200/3450 | Loss: 0.0464\n",
      "Epoch 11/150 | Batch 2250/3450 | Loss: 0.0092\n",
      "Epoch 11/150 | Batch 2300/3450 | Loss: 0.0484\n",
      "Epoch 11/150 | Batch 2350/3450 | Loss: 0.0594\n",
      "Epoch 11/150 | Batch 2400/3450 | Loss: 0.0451\n",
      "Epoch 11/150 | Batch 2450/3450 | Loss: 0.0324\n",
      "Epoch 11/150 | Batch 2500/3450 | Loss: 0.0278\n",
      "Epoch 11/150 | Batch 2550/3450 | Loss: 0.0255\n",
      "Epoch 11/150 | Batch 2600/3450 | Loss: 0.0237\n",
      "Epoch 11/150 | Batch 2650/3450 | Loss: 0.0447\n",
      "Epoch 11/150 | Batch 2700/3450 | Loss: 0.0270\n",
      "Epoch 11/150 | Batch 2750/3450 | Loss: 0.0458\n",
      "Epoch 11/150 | Batch 2800/3450 | Loss: 0.0358\n",
      "Epoch 11/150 | Batch 2850/3450 | Loss: 0.0585\n",
      "Epoch 11/150 | Batch 2900/3450 | Loss: 0.0265\n",
      "Epoch 11/150 | Batch 2950/3450 | Loss: 0.0547\n",
      "Epoch 11/150 | Batch 3000/3450 | Loss: 0.0373\n",
      "Epoch 11/150 | Batch 3050/3450 | Loss: 0.0160\n",
      "Epoch 11/150 | Batch 3100/3450 | Loss: 0.0113\n",
      "Epoch 11/150 | Batch 3150/3450 | Loss: 0.0271\n",
      "Epoch 11/150 | Batch 3200/3450 | Loss: 0.0168\n",
      "Epoch 11/150 | Batch 3250/3450 | Loss: 0.0352\n",
      "Epoch 11/150 | Batch 3300/3450 | Loss: 0.0119\n",
      "Epoch 11/150 | Batch 3350/3450 | Loss: 0.0539\n",
      "Epoch 11/150 | Batch 3400/3450 | Loss: 0.0595\n",
      "Epoch 11/150 | Batch 3450/3450 | Loss: 0.0223\n",
      "\n",
      "Epoch 11/150 Summary:\n",
      "Time: 5022.74s | Total: 15:23:43\n",
      "LR: 0.00019941\n",
      "Train Loss: 0.0355\n",
      "Val Loss: 0.0207\n",
      "PSNR: 30.84 | SSIM: 0.9098 | LPIPS: 2.0860\n",
      "[INFO] New best model saved with PSNR: 30.84\n",
      "Best PSNR so far: 30.84 at epoch 11\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12/150 | Batch 50/3450 | Loss: 0.0165\n",
      "Epoch 12/150 | Batch 100/3450 | Loss: 0.0270\n",
      "Epoch 12/150 | Batch 150/3450 | Loss: 0.0256\n",
      "Epoch 12/150 | Batch 200/3450 | Loss: 0.0207\n",
      "Epoch 12/150 | Batch 250/3450 | Loss: 0.0151\n",
      "Epoch 12/150 | Batch 300/3450 | Loss: 0.0256\n",
      "Epoch 12/150 | Batch 350/3450 | Loss: 0.0347\n",
      "Epoch 12/150 | Batch 400/3450 | Loss: 0.0383\n",
      "Epoch 12/150 | Batch 450/3450 | Loss: 0.0204\n",
      "Epoch 12/150 | Batch 500/3450 | Loss: 0.0594\n",
      "Epoch 12/150 | Batch 550/3450 | Loss: 0.0163\n",
      "Epoch 12/150 | Batch 600/3450 | Loss: 0.0399\n",
      "Epoch 12/150 | Batch 650/3450 | Loss: 0.0428\n",
      "Epoch 12/150 | Batch 700/3450 | Loss: 0.0504\n",
      "Epoch 12/150 | Batch 750/3450 | Loss: 0.0176\n",
      "Epoch 12/150 | Batch 800/3450 | Loss: 0.0199\n",
      "Epoch 12/150 | Batch 850/3450 | Loss: 0.0614\n",
      "Epoch 12/150 | Batch 900/3450 | Loss: 0.0409\n",
      "Epoch 12/150 | Batch 950/3450 | Loss: 0.0125\n",
      "Epoch 12/150 | Batch 1000/3450 | Loss: 0.0203\n",
      "Epoch 12/150 | Batch 1050/3450 | Loss: 0.0142\n",
      "Epoch 12/150 | Batch 1100/3450 | Loss: 0.0421\n",
      "Epoch 12/150 | Batch 1150/3450 | Loss: 0.0518\n",
      "Epoch 12/150 | Batch 1200/3450 | Loss: 0.0146\n",
      "Epoch 12/150 | Batch 1250/3450 | Loss: 0.0231\n",
      "Epoch 12/150 | Batch 1300/3450 | Loss: 0.0930\n",
      "Epoch 12/150 | Batch 1350/3450 | Loss: 0.0544\n",
      "Epoch 12/150 | Batch 1400/3450 | Loss: 0.0149\n",
      "Epoch 12/150 | Batch 1450/3450 | Loss: 0.0331\n",
      "Epoch 12/150 | Batch 1500/3450 | Loss: 0.0565\n",
      "Epoch 12/150 | Batch 1550/3450 | Loss: 0.0350\n",
      "Epoch 12/150 | Batch 1600/3450 | Loss: 0.0394\n",
      "Epoch 12/150 | Batch 1650/3450 | Loss: 0.0291\n",
      "Epoch 12/150 | Batch 1700/3450 | Loss: 0.0292\n",
      "Epoch 12/150 | Batch 1750/3450 | Loss: 0.0598\n",
      "Epoch 12/150 | Batch 1800/3450 | Loss: 0.0308\n",
      "Epoch 12/150 | Batch 1850/3450 | Loss: 0.0225\n",
      "Epoch 12/150 | Batch 1900/3450 | Loss: 0.0121\n",
      "Epoch 12/150 | Batch 1950/3450 | Loss: 0.0361\n",
      "Epoch 12/150 | Batch 2000/3450 | Loss: 0.0919\n",
      "Epoch 12/150 | Batch 2050/3450 | Loss: 0.0151\n",
      "Epoch 12/150 | Batch 2100/3450 | Loss: 0.0222\n",
      "Epoch 12/150 | Batch 2150/3450 | Loss: 0.0514\n",
      "Epoch 12/150 | Batch 2200/3450 | Loss: 0.0370\n",
      "Epoch 12/150 | Batch 2250/3450 | Loss: 0.0253\n",
      "Epoch 12/150 | Batch 2300/3450 | Loss: 0.0319\n",
      "Epoch 12/150 | Batch 2350/3450 | Loss: 0.0210\n",
      "Epoch 12/150 | Batch 2400/3450 | Loss: 0.0164\n",
      "Epoch 12/150 | Batch 2450/3450 | Loss: 0.0158\n",
      "Epoch 12/150 | Batch 2500/3450 | Loss: 0.0219\n",
      "Epoch 12/150 | Batch 2550/3450 | Loss: 0.0144\n",
      "Epoch 12/150 | Batch 2600/3450 | Loss: 0.0262\n",
      "Epoch 12/150 | Batch 2650/3450 | Loss: 0.0647\n",
      "Epoch 12/150 | Batch 2700/3450 | Loss: 0.0349\n",
      "Epoch 12/150 | Batch 2750/3450 | Loss: 0.0386\n",
      "Epoch 12/150 | Batch 2800/3450 | Loss: 0.0636\n",
      "Epoch 12/150 | Batch 2850/3450 | Loss: 0.0524\n",
      "Epoch 12/150 | Batch 2900/3450 | Loss: 0.0291\n",
      "Epoch 12/150 | Batch 2950/3450 | Loss: 0.0141\n",
      "Epoch 12/150 | Batch 3000/3450 | Loss: 0.0192\n",
      "Epoch 12/150 | Batch 3050/3450 | Loss: 0.0440\n",
      "Epoch 12/150 | Batch 3100/3450 | Loss: 0.0348\n",
      "Epoch 12/150 | Batch 3150/3450 | Loss: 0.0209\n",
      "Epoch 12/150 | Batch 3200/3450 | Loss: 0.0683\n",
      "Epoch 12/150 | Batch 3250/3450 | Loss: 0.0145\n",
      "Epoch 12/150 | Batch 3300/3450 | Loss: 0.0289\n",
      "Epoch 12/150 | Batch 3350/3450 | Loss: 0.0223\n",
      "Epoch 12/150 | Batch 3400/3450 | Loss: 0.0607\n",
      "Epoch 12/150 | Batch 3450/3450 | Loss: 0.0585\n",
      "\n",
      "Epoch 12/150 Summary:\n",
      "Time: 5012.06s | Total: 16:47:18\n",
      "LR: 0.00019916\n",
      "Train Loss: 0.0353\n",
      "Val Loss: 0.0236\n",
      "PSNR: 29.89 | SSIM: 0.9087 | LPIPS: 2.1396\n",
      "Best PSNR so far: 30.84 at epoch 11\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13/150 | Batch 50/3450 | Loss: 0.0226\n",
      "Epoch 13/150 | Batch 100/3450 | Loss: 0.0278\n",
      "Epoch 13/150 | Batch 150/3450 | Loss: 0.0294\n",
      "Epoch 13/150 | Batch 200/3450 | Loss: 0.0147\n",
      "Epoch 13/150 | Batch 250/3450 | Loss: 0.0398\n",
      "Epoch 13/150 | Batch 300/3450 | Loss: 0.0362\n",
      "Epoch 13/150 | Batch 350/3450 | Loss: 0.0673\n",
      "Epoch 13/150 | Batch 400/3450 | Loss: 0.0298\n",
      "Epoch 13/150 | Batch 450/3450 | Loss: 0.0208\n",
      "Epoch 13/150 | Batch 500/3450 | Loss: 0.0428\n",
      "Epoch 13/150 | Batch 550/3450 | Loss: 0.0198\n",
      "Epoch 13/150 | Batch 600/3450 | Loss: 0.0184\n",
      "Epoch 13/150 | Batch 650/3450 | Loss: 0.0197\n",
      "Epoch 13/150 | Batch 700/3450 | Loss: 0.0672\n",
      "Epoch 13/150 | Batch 750/3450 | Loss: 0.0094\n",
      "Epoch 13/150 | Batch 800/3450 | Loss: 0.0948\n",
      "Epoch 13/150 | Batch 850/3450 | Loss: 0.0298\n",
      "Epoch 13/150 | Batch 900/3450 | Loss: 0.0364\n",
      "Epoch 13/150 | Batch 950/3450 | Loss: 0.0149\n",
      "Epoch 13/150 | Batch 1000/3450 | Loss: 0.0146\n",
      "Epoch 13/150 | Batch 1050/3450 | Loss: 0.0397\n",
      "Epoch 13/150 | Batch 1100/3450 | Loss: 0.0195\n",
      "Epoch 13/150 | Batch 1150/3450 | Loss: 0.0630\n",
      "Epoch 13/150 | Batch 1200/3450 | Loss: 0.0338\n",
      "Epoch 13/150 | Batch 1250/3450 | Loss: 0.0635\n",
      "Epoch 13/150 | Batch 1300/3450 | Loss: 0.0193\n",
      "Epoch 13/150 | Batch 1350/3450 | Loss: 0.0182\n",
      "Epoch 13/150 | Batch 1400/3450 | Loss: 0.0713\n",
      "Epoch 13/150 | Batch 1450/3450 | Loss: 0.0391\n",
      "Epoch 13/150 | Batch 1500/3450 | Loss: 0.0736\n",
      "Epoch 13/150 | Batch 1550/3450 | Loss: 0.0207\n",
      "Epoch 13/150 | Batch 1600/3450 | Loss: 0.0180\n",
      "Epoch 13/150 | Batch 1650/3450 | Loss: 0.0180\n",
      "Epoch 13/150 | Batch 1700/3450 | Loss: 0.0440\n",
      "Epoch 13/150 | Batch 1750/3450 | Loss: 0.0111\n",
      "Epoch 13/150 | Batch 1800/3450 | Loss: 0.0177\n",
      "Epoch 13/150 | Batch 1850/3450 | Loss: 0.0192\n",
      "Epoch 13/150 | Batch 1900/3450 | Loss: 0.0215\n",
      "Epoch 13/150 | Batch 1950/3450 | Loss: 0.0524\n",
      "Epoch 13/150 | Batch 2000/3450 | Loss: 0.0859\n",
      "Epoch 13/150 | Batch 2050/3450 | Loss: 0.0265\n",
      "Epoch 13/150 | Batch 2100/3450 | Loss: 0.0274\n",
      "Epoch 13/150 | Batch 2150/3450 | Loss: 0.0276\n",
      "Epoch 13/150 | Batch 2200/3450 | Loss: 0.0155\n",
      "Epoch 13/150 | Batch 2250/3450 | Loss: 0.0252\n",
      "Epoch 13/150 | Batch 2300/3450 | Loss: 0.0200\n",
      "Epoch 13/150 | Batch 2350/3450 | Loss: 0.0315\n",
      "Epoch 13/150 | Batch 2400/3450 | Loss: 0.0306\n",
      "Epoch 13/150 | Batch 2450/3450 | Loss: 0.0208\n",
      "Epoch 13/150 | Batch 2500/3450 | Loss: 0.0191\n",
      "Epoch 13/150 | Batch 2550/3450 | Loss: 0.0339\n",
      "Epoch 13/150 | Batch 2600/3450 | Loss: 0.0168\n",
      "Epoch 13/150 | Batch 2650/3450 | Loss: 0.0254\n",
      "Epoch 13/150 | Batch 2700/3450 | Loss: 0.0111\n",
      "Epoch 13/150 | Batch 2750/3450 | Loss: 0.0726\n",
      "Epoch 13/150 | Batch 2800/3450 | Loss: 0.0299\n",
      "Epoch 13/150 | Batch 2850/3450 | Loss: 0.0240\n",
      "Epoch 13/150 | Batch 2900/3450 | Loss: 0.0392\n",
      "Epoch 13/150 | Batch 2950/3450 | Loss: 0.0169\n",
      "Epoch 13/150 | Batch 3000/3450 | Loss: 0.0180\n",
      "Epoch 13/150 | Batch 3050/3450 | Loss: 0.0176\n",
      "Epoch 13/150 | Batch 3100/3450 | Loss: 0.0511\n",
      "Epoch 13/150 | Batch 3150/3450 | Loss: 0.0427\n",
      "Epoch 13/150 | Batch 3200/3450 | Loss: 0.0242\n",
      "Epoch 13/150 | Batch 3250/3450 | Loss: 0.0171\n",
      "Epoch 13/150 | Batch 3300/3450 | Loss: 0.0383\n",
      "Epoch 13/150 | Batch 3350/3450 | Loss: 0.0259\n",
      "Epoch 13/150 | Batch 3400/3450 | Loss: 0.0128\n",
      "Epoch 13/150 | Batch 3450/3450 | Loss: 0.0288\n",
      "\n",
      "Epoch 13/150 Summary:\n",
      "Time: 5042.08s | Total: 18:11:22\n",
      "LR: 0.00019885\n",
      "Train Loss: 0.0346\n",
      "Val Loss: 0.0201\n",
      "PSNR: 31.01 | SSIM: 0.9156 | LPIPS: 2.0803\n",
      "[INFO] New best model saved with PSNR: 31.01\n",
      "Best PSNR so far: 31.01 at epoch 13\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14/150 | Batch 50/3450 | Loss: 0.0165\n",
      "Epoch 14/150 | Batch 100/3450 | Loss: 0.0089\n",
      "Epoch 14/150 | Batch 150/3450 | Loss: 0.0425\n",
      "Epoch 14/150 | Batch 200/3450 | Loss: 0.0531\n",
      "Epoch 14/150 | Batch 250/3450 | Loss: 0.0205\n",
      "Epoch 14/150 | Batch 300/3450 | Loss: 0.0229\n",
      "Epoch 14/150 | Batch 350/3450 | Loss: 0.0161\n",
      "Epoch 14/150 | Batch 400/3450 | Loss: 0.0108\n",
      "Epoch 14/150 | Batch 450/3450 | Loss: 0.0367\n",
      "Epoch 14/150 | Batch 500/3450 | Loss: 0.0252\n",
      "Epoch 14/150 | Batch 550/3450 | Loss: 0.0480\n",
      "Epoch 14/150 | Batch 600/3450 | Loss: 0.0212\n",
      "Epoch 14/150 | Batch 650/3450 | Loss: 0.0244\n",
      "Epoch 14/150 | Batch 700/3450 | Loss: 0.0225\n",
      "Epoch 14/150 | Batch 750/3450 | Loss: 0.0289\n",
      "Epoch 14/150 | Batch 800/3450 | Loss: 0.0281\n",
      "Epoch 14/150 | Batch 850/3450 | Loss: 0.0313\n",
      "Epoch 14/150 | Batch 900/3450 | Loss: 0.0167\n",
      "Epoch 14/150 | Batch 950/3450 | Loss: 0.0192\n",
      "Epoch 14/150 | Batch 1000/3450 | Loss: 0.0232\n",
      "Epoch 14/150 | Batch 1050/3450 | Loss: 0.0115\n",
      "Epoch 14/150 | Batch 1100/3450 | Loss: 0.0113\n",
      "Epoch 14/150 | Batch 1150/3450 | Loss: 0.0434\n",
      "Epoch 14/150 | Batch 1200/3450 | Loss: 0.0218\n",
      "Epoch 14/150 | Batch 1250/3450 | Loss: 0.0296\n",
      "Epoch 14/150 | Batch 1300/3450 | Loss: 0.0171\n",
      "Epoch 14/150 | Batch 1350/3450 | Loss: 0.0197\n",
      "Epoch 14/150 | Batch 1400/3450 | Loss: 0.0757\n",
      "Epoch 14/150 | Batch 1450/3450 | Loss: 0.0211\n",
      "Epoch 14/150 | Batch 1500/3450 | Loss: 0.0444\n",
      "Epoch 14/150 | Batch 1550/3450 | Loss: 0.0304\n",
      "Epoch 14/150 | Batch 1600/3450 | Loss: 0.0239\n",
      "Epoch 14/150 | Batch 1650/3450 | Loss: 0.0225\n",
      "Epoch 14/150 | Batch 1700/3450 | Loss: 0.0624\n",
      "Epoch 14/150 | Batch 1750/3450 | Loss: 0.0169\n",
      "Epoch 14/150 | Batch 1800/3450 | Loss: 0.0219\n",
      "Epoch 14/150 | Batch 1850/3450 | Loss: 0.0442\n",
      "Epoch 14/150 | Batch 1900/3450 | Loss: 0.0549\n",
      "Epoch 14/150 | Batch 1950/3450 | Loss: 0.0414\n",
      "Epoch 14/150 | Batch 2000/3450 | Loss: 0.0354\n",
      "Epoch 14/150 | Batch 2050/3450 | Loss: 0.0146\n",
      "Epoch 14/150 | Batch 2100/3450 | Loss: 0.0244\n",
      "Epoch 14/150 | Batch 2150/3450 | Loss: 0.0422\n",
      "Epoch 14/150 | Batch 2200/3450 | Loss: 0.0479\n",
      "Epoch 14/150 | Batch 2250/3450 | Loss: 0.0564\n",
      "Epoch 14/150 | Batch 2300/3450 | Loss: 0.0599\n",
      "Epoch 14/150 | Batch 2350/3450 | Loss: 0.0189\n",
      "Epoch 14/150 | Batch 2400/3450 | Loss: 0.0137\n",
      "Epoch 14/150 | Batch 2450/3450 | Loss: 0.0243\n",
      "Epoch 14/150 | Batch 2500/3450 | Loss: 0.0145\n",
      "Epoch 14/150 | Batch 2550/3450 | Loss: 0.0535\n",
      "Epoch 14/150 | Batch 2600/3450 | Loss: 0.0245\n",
      "Epoch 14/150 | Batch 2650/3450 | Loss: 0.0152\n",
      "Epoch 14/150 | Batch 2700/3450 | Loss: 0.0431\n",
      "Epoch 14/150 | Batch 2750/3450 | Loss: 0.0385\n",
      "Epoch 14/150 | Batch 2800/3450 | Loss: 0.0293\n",
      "Epoch 14/150 | Batch 2850/3450 | Loss: 0.0183\n",
      "Epoch 14/150 | Batch 2900/3450 | Loss: 0.0446\n",
      "Epoch 14/150 | Batch 2950/3450 | Loss: 0.0333\n",
      "Epoch 14/150 | Batch 3000/3450 | Loss: 0.0272\n",
      "Epoch 14/150 | Batch 3050/3450 | Loss: 0.0360\n",
      "Epoch 14/150 | Batch 3100/3450 | Loss: 0.0858\n",
      "Epoch 14/150 | Batch 3150/3450 | Loss: 0.0225\n",
      "Epoch 14/150 | Batch 3200/3450 | Loss: 0.0262\n",
      "Epoch 14/150 | Batch 3250/3450 | Loss: 0.0502\n",
      "Epoch 14/150 | Batch 3300/3450 | Loss: 0.0289\n",
      "Epoch 14/150 | Batch 3350/3450 | Loss: 0.0231\n",
      "Epoch 14/150 | Batch 3400/3450 | Loss: 0.0460\n",
      "Epoch 14/150 | Batch 3450/3450 | Loss: 0.0296\n",
      "\n",
      "Epoch 14/150 Summary:\n",
      "Time: 5069.21s | Total: 19:35:54\n",
      "LR: 0.00019850\n",
      "Train Loss: 0.0344\n",
      "Val Loss: 0.0210\n",
      "PSNR: 30.65 | SSIM: 0.9059 | LPIPS: 2.1077\n",
      "Best PSNR so far: 31.01 at epoch 13\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15/150 | Batch 50/3450 | Loss: 0.0146\n",
      "Epoch 15/150 | Batch 100/3450 | Loss: 0.0288\n",
      "Epoch 15/150 | Batch 150/3450 | Loss: 0.0118\n",
      "Epoch 15/150 | Batch 200/3450 | Loss: 0.0185\n",
      "Epoch 15/150 | Batch 250/3450 | Loss: 0.0504\n",
      "Epoch 15/150 | Batch 300/3450 | Loss: 0.0205\n",
      "Epoch 15/150 | Batch 350/3450 | Loss: 0.0308\n",
      "Epoch 15/150 | Batch 400/3450 | Loss: 0.0192\n",
      "Epoch 15/150 | Batch 450/3450 | Loss: 0.0161\n",
      "Epoch 15/150 | Batch 500/3450 | Loss: 0.0794\n",
      "Epoch 15/150 | Batch 550/3450 | Loss: 0.0155\n",
      "Epoch 15/150 | Batch 600/3450 | Loss: 0.0554\n",
      "Epoch 15/150 | Batch 650/3450 | Loss: 0.0179\n",
      "Epoch 15/150 | Batch 700/3450 | Loss: 0.0246\n",
      "Epoch 15/150 | Batch 750/3450 | Loss: 0.0452\n",
      "Epoch 15/150 | Batch 800/3450 | Loss: 0.0197\n",
      "Epoch 15/150 | Batch 850/3450 | Loss: 0.0105\n",
      "Epoch 15/150 | Batch 900/3450 | Loss: 0.0328\n",
      "Epoch 15/150 | Batch 950/3450 | Loss: 0.0138\n",
      "Epoch 15/150 | Batch 1000/3450 | Loss: 0.0172\n",
      "Epoch 15/150 | Batch 1050/3450 | Loss: 0.0133\n",
      "Epoch 15/150 | Batch 1100/3450 | Loss: 0.0136\n",
      "Epoch 15/150 | Batch 1150/3450 | Loss: 0.0334\n",
      "Epoch 15/150 | Batch 1200/3450 | Loss: 0.0236\n",
      "Epoch 15/150 | Batch 1250/3450 | Loss: 0.0321\n",
      "Epoch 15/150 | Batch 1300/3450 | Loss: 0.0195\n",
      "Epoch 15/150 | Batch 1350/3450 | Loss: 0.0613\n",
      "Epoch 15/150 | Batch 1400/3450 | Loss: 0.0380\n",
      "Epoch 15/150 | Batch 1450/3450 | Loss: 0.0806\n",
      "Epoch 15/150 | Batch 1500/3450 | Loss: 0.0571\n",
      "Epoch 15/150 | Batch 1550/3450 | Loss: 0.0248\n",
      "Epoch 15/150 | Batch 1600/3450 | Loss: 0.0556\n",
      "Epoch 15/150 | Batch 1650/3450 | Loss: 0.0244\n",
      "Epoch 15/150 | Batch 1700/3450 | Loss: 0.0601\n",
      "Epoch 15/150 | Batch 1750/3450 | Loss: 0.0710\n",
      "Epoch 15/150 | Batch 1800/3450 | Loss: 0.0102\n",
      "Epoch 15/150 | Batch 1850/3450 | Loss: 0.0752\n",
      "Epoch 15/150 | Batch 1900/3450 | Loss: 0.0134\n",
      "Epoch 15/150 | Batch 1950/3450 | Loss: 0.0448\n",
      "Epoch 15/150 | Batch 2000/3450 | Loss: 0.0231\n",
      "Epoch 15/150 | Batch 2050/3450 | Loss: 0.0172\n",
      "Epoch 15/150 | Batch 2100/3450 | Loss: 0.0481\n",
      "Epoch 15/150 | Batch 2150/3450 | Loss: 0.0210\n",
      "Epoch 15/150 | Batch 2200/3450 | Loss: 0.0387\n",
      "Epoch 15/150 | Batch 2250/3450 | Loss: 0.0719\n",
      "Epoch 15/150 | Batch 2300/3450 | Loss: 0.0250\n",
      "Epoch 15/150 | Batch 2350/3450 | Loss: 0.0400\n",
      "Epoch 15/150 | Batch 2400/3450 | Loss: 0.0146\n",
      "Epoch 15/150 | Batch 2450/3450 | Loss: 0.0120\n",
      "Epoch 15/150 | Batch 2500/3450 | Loss: 0.0353\n",
      "Epoch 15/150 | Batch 2550/3450 | Loss: 0.0502\n",
      "Epoch 15/150 | Batch 2600/3450 | Loss: 0.0993\n",
      "Epoch 15/150 | Batch 2650/3450 | Loss: 0.0227\n",
      "Epoch 15/150 | Batch 2700/3450 | Loss: 0.0297\n",
      "Epoch 15/150 | Batch 2750/3450 | Loss: 0.0280\n",
      "Epoch 15/150 | Batch 2800/3450 | Loss: 0.0541\n",
      "Epoch 15/150 | Batch 2850/3450 | Loss: 0.0782\n",
      "Epoch 15/150 | Batch 2900/3450 | Loss: 0.0816\n",
      "Epoch 15/150 | Batch 2950/3450 | Loss: 0.0384\n",
      "Epoch 15/150 | Batch 3000/3450 | Loss: 0.0146\n",
      "Epoch 15/150 | Batch 3050/3450 | Loss: 0.0275\n",
      "Epoch 15/150 | Batch 3100/3450 | Loss: 0.0168\n",
      "Epoch 15/150 | Batch 3150/3450 | Loss: 0.0747\n",
      "Epoch 15/150 | Batch 3200/3450 | Loss: 0.0534\n",
      "Epoch 15/150 | Batch 3250/3450 | Loss: 0.0186\n",
      "Epoch 15/150 | Batch 3300/3450 | Loss: 0.0450\n",
      "Epoch 15/150 | Batch 3350/3450 | Loss: 0.0159\n",
      "Epoch 15/150 | Batch 3400/3450 | Loss: 0.0156\n",
      "Epoch 15/150 | Batch 3450/3450 | Loss: 0.0157\n",
      "\n",
      "Epoch 15/150 Summary:\n",
      "Time: 5029.39s | Total: 20:59:45\n",
      "LR: 0.00019810\n",
      "Train Loss: 0.0344\n",
      "Val Loss: 0.0190\n",
      "PSNR: 31.50 | SSIM: 0.9188 | LPIPS: 1.9719\n",
      "[INFO] New best model saved with PSNR: 31.50\n",
      "Best PSNR so far: 31.50 at epoch 15\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16/150 | Batch 50/3450 | Loss: 0.0191\n",
      "Epoch 16/150 | Batch 100/3450 | Loss: 0.0601\n",
      "Epoch 16/150 | Batch 150/3450 | Loss: 0.0530\n",
      "Epoch 16/150 | Batch 200/3450 | Loss: 0.0373\n",
      "Epoch 16/150 | Batch 250/3450 | Loss: 0.0222\n",
      "Epoch 16/150 | Batch 300/3450 | Loss: 0.0561\n",
      "Epoch 16/150 | Batch 350/3450 | Loss: 0.0144\n",
      "Epoch 16/150 | Batch 400/3450 | Loss: 0.0406\n",
      "Epoch 16/150 | Batch 450/3450 | Loss: 0.0493\n",
      "Epoch 16/150 | Batch 500/3450 | Loss: 0.0188\n",
      "Epoch 16/150 | Batch 550/3450 | Loss: 0.0173\n",
      "Epoch 16/150 | Batch 600/3450 | Loss: 0.0351\n",
      "Epoch 16/150 | Batch 650/3450 | Loss: 0.0222\n",
      "Epoch 16/150 | Batch 700/3450 | Loss: 0.0682\n",
      "Epoch 16/150 | Batch 750/3450 | Loss: 0.0252\n",
      "Epoch 16/150 | Batch 800/3450 | Loss: 0.0283\n",
      "Epoch 16/150 | Batch 850/3450 | Loss: 0.0255\n",
      "Epoch 16/150 | Batch 900/3450 | Loss: 0.0267\n",
      "Epoch 16/150 | Batch 950/3450 | Loss: 0.0428\n",
      "Epoch 16/150 | Batch 1000/3450 | Loss: 0.0108\n",
      "Epoch 16/150 | Batch 1050/3450 | Loss: 0.0221\n",
      "Epoch 16/150 | Batch 1100/3450 | Loss: 0.0408\n",
      "Epoch 16/150 | Batch 1150/3450 | Loss: 0.0366\n",
      "Epoch 16/150 | Batch 1200/3450 | Loss: 0.0294\n",
      "Epoch 16/150 | Batch 1250/3450 | Loss: 0.0436\n",
      "Epoch 16/150 | Batch 1300/3450 | Loss: 0.0387\n",
      "Epoch 16/150 | Batch 1350/3450 | Loss: 0.0553\n",
      "Epoch 16/150 | Batch 1400/3450 | Loss: 0.0371\n",
      "Epoch 16/150 | Batch 1450/3450 | Loss: 0.0318\n",
      "Epoch 16/150 | Batch 1500/3450 | Loss: 0.0683\n",
      "Epoch 16/150 | Batch 1550/3450 | Loss: 0.0195\n",
      "Epoch 16/150 | Batch 1600/3450 | Loss: 0.0171\n",
      "Epoch 16/150 | Batch 1650/3450 | Loss: 0.0159\n",
      "Epoch 16/150 | Batch 1700/3450 | Loss: 0.0170\n",
      "Epoch 16/150 | Batch 1750/3450 | Loss: 0.0338\n",
      "Epoch 16/150 | Batch 1800/3450 | Loss: 0.0573\n",
      "Epoch 16/150 | Batch 1850/3450 | Loss: 0.0526\n",
      "Epoch 16/150 | Batch 1900/3450 | Loss: 0.0467\n",
      "Epoch 16/150 | Batch 1950/3450 | Loss: 0.0395\n",
      "Epoch 16/150 | Batch 2000/3450 | Loss: 0.0345\n",
      "Epoch 16/150 | Batch 2050/3450 | Loss: 0.0465\n",
      "Epoch 16/150 | Batch 2100/3450 | Loss: 0.0159\n",
      "Epoch 16/150 | Batch 2150/3450 | Loss: 0.0426\n",
      "Epoch 16/150 | Batch 2200/3450 | Loss: 0.0453\n",
      "Epoch 16/150 | Batch 2250/3450 | Loss: 0.0174\n",
      "Epoch 16/150 | Batch 2300/3450 | Loss: 0.0252\n",
      "Epoch 16/150 | Batch 2350/3450 | Loss: 0.0351\n",
      "Epoch 16/150 | Batch 2400/3450 | Loss: 0.0960\n",
      "Epoch 16/150 | Batch 2450/3450 | Loss: 0.0417\n",
      "Epoch 16/150 | Batch 2500/3450 | Loss: 0.0563\n",
      "Epoch 16/150 | Batch 2550/3450 | Loss: 0.0227\n",
      "Epoch 16/150 | Batch 2600/3450 | Loss: 0.0559\n",
      "Epoch 16/150 | Batch 2650/3450 | Loss: 0.0209\n",
      "Epoch 16/150 | Batch 2700/3450 | Loss: 0.0254\n",
      "Epoch 16/150 | Batch 2750/3450 | Loss: 0.0136\n",
      "Epoch 16/150 | Batch 2800/3450 | Loss: 0.0419\n",
      "Epoch 16/150 | Batch 2850/3450 | Loss: 0.0264\n",
      "Epoch 16/150 | Batch 2900/3450 | Loss: 0.0299\n",
      "Epoch 16/150 | Batch 2950/3450 | Loss: 0.0223\n",
      "Epoch 16/150 | Batch 3000/3450 | Loss: 0.0274\n",
      "Epoch 16/150 | Batch 3050/3450 | Loss: 0.0291\n",
      "Epoch 16/150 | Batch 3100/3450 | Loss: 0.0192\n",
      "Epoch 16/150 | Batch 3150/3450 | Loss: 0.0197\n",
      "Epoch 16/150 | Batch 3200/3450 | Loss: 0.0284\n",
      "Epoch 16/150 | Batch 3250/3450 | Loss: 0.0321\n",
      "Epoch 16/150 | Batch 3300/3450 | Loss: 0.0174\n",
      "Epoch 16/150 | Batch 3350/3450 | Loss: 0.0511\n",
      "Epoch 16/150 | Batch 3400/3450 | Loss: 0.0630\n",
      "Epoch 16/150 | Batch 3450/3450 | Loss: 0.0196\n",
      "\n",
      "Epoch 16/150 Summary:\n",
      "Time: 5052.57s | Total: 22:24:00\n",
      "LR: 0.00019766\n",
      "Train Loss: 0.0336\n",
      "Val Loss: 0.0204\n",
      "PSNR: 30.99 | SSIM: 0.9177 | LPIPS: 2.0100\n",
      "Best PSNR so far: 31.50 at epoch 15\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17/150 | Batch 50/3450 | Loss: 0.0154\n",
      "Epoch 17/150 | Batch 100/3450 | Loss: 0.0580\n",
      "Epoch 17/150 | Batch 150/3450 | Loss: 0.0212\n",
      "Epoch 17/150 | Batch 200/3450 | Loss: 0.0252\n",
      "Epoch 17/150 | Batch 250/3450 | Loss: 0.0134\n",
      "Epoch 17/150 | Batch 300/3450 | Loss: 0.0225\n",
      "Epoch 17/150 | Batch 350/3450 | Loss: 0.0453\n",
      "Epoch 17/150 | Batch 400/3450 | Loss: 0.0658\n",
      "Epoch 17/150 | Batch 450/3450 | Loss: 0.0134\n",
      "Epoch 17/150 | Batch 500/3450 | Loss: 0.0342\n",
      "Epoch 17/150 | Batch 550/3450 | Loss: 0.0670\n",
      "Epoch 17/150 | Batch 600/3450 | Loss: 0.0198\n",
      "Epoch 17/150 | Batch 650/3450 | Loss: 0.0364\n",
      "Epoch 17/150 | Batch 700/3450 | Loss: 0.0415\n",
      "Epoch 17/150 | Batch 750/3450 | Loss: 0.0576\n",
      "Epoch 17/150 | Batch 800/3450 | Loss: 0.0138\n",
      "Epoch 17/150 | Batch 850/3450 | Loss: 0.0272\n",
      "Epoch 17/150 | Batch 900/3450 | Loss: 0.0096\n",
      "Epoch 17/150 | Batch 950/3450 | Loss: 0.0161\n",
      "Epoch 17/150 | Batch 1000/3450 | Loss: 0.0370\n",
      "Epoch 17/150 | Batch 1050/3450 | Loss: 0.0174\n",
      "Epoch 17/150 | Batch 1100/3450 | Loss: 0.0358\n",
      "Epoch 17/150 | Batch 1150/3450 | Loss: 0.0217\n",
      "Epoch 17/150 | Batch 1200/3450 | Loss: 0.0283\n",
      "Epoch 17/150 | Batch 1250/3450 | Loss: 0.0204\n",
      "Epoch 17/150 | Batch 1300/3450 | Loss: 0.0627\n",
      "Epoch 17/150 | Batch 1350/3450 | Loss: 0.0239\n",
      "Epoch 17/150 | Batch 1400/3450 | Loss: 0.0582\n",
      "Epoch 17/150 | Batch 1450/3450 | Loss: 0.0530\n",
      "Epoch 17/150 | Batch 1500/3450 | Loss: 0.0348\n",
      "Epoch 17/150 | Batch 1550/3450 | Loss: 0.0484\n",
      "Epoch 17/150 | Batch 1600/3450 | Loss: 0.0318\n",
      "Epoch 17/150 | Batch 1650/3450 | Loss: 0.0708\n",
      "Epoch 17/150 | Batch 1700/3450 | Loss: 0.0333\n",
      "Epoch 17/150 | Batch 1750/3450 | Loss: 0.0459\n",
      "Epoch 17/150 | Batch 1800/3450 | Loss: 0.0251\n",
      "Epoch 17/150 | Batch 1850/3450 | Loss: 0.0293\n",
      "Epoch 17/150 | Batch 1900/3450 | Loss: 0.0158\n",
      "Epoch 17/150 | Batch 1950/3450 | Loss: 0.0709\n",
      "Epoch 17/150 | Batch 2000/3450 | Loss: 0.0310\n",
      "Epoch 17/150 | Batch 2050/3450 | Loss: 0.0217\n",
      "Epoch 17/150 | Batch 2100/3450 | Loss: 0.0131\n",
      "Epoch 17/150 | Batch 2150/3450 | Loss: 0.0236\n",
      "Epoch 17/150 | Batch 2200/3450 | Loss: 0.0156\n",
      "Epoch 17/150 | Batch 2250/3450 | Loss: 0.0145\n",
      "Epoch 17/150 | Batch 2300/3450 | Loss: 0.0658\n",
      "Epoch 17/150 | Batch 2350/3450 | Loss: 0.0186\n",
      "Epoch 17/150 | Batch 2400/3450 | Loss: 0.0242\n",
      "Epoch 17/150 | Batch 2450/3450 | Loss: 0.0697\n",
      "Epoch 17/150 | Batch 2500/3450 | Loss: 0.0781\n",
      "Epoch 17/150 | Batch 2550/3450 | Loss: 0.0400\n",
      "Epoch 17/150 | Batch 2600/3450 | Loss: 0.0304\n",
      "Epoch 17/150 | Batch 2650/3450 | Loss: 0.0335\n",
      "Epoch 17/150 | Batch 2700/3450 | Loss: 0.0165\n",
      "Epoch 17/150 | Batch 2750/3450 | Loss: 0.0205\n",
      "Epoch 17/150 | Batch 2800/3450 | Loss: 0.0814\n",
      "Epoch 17/150 | Batch 2850/3450 | Loss: 0.0223\n",
      "Epoch 17/150 | Batch 2900/3450 | Loss: 0.0338\n",
      "Epoch 17/150 | Batch 2950/3450 | Loss: 0.0320\n",
      "Epoch 17/150 | Batch 3000/3450 | Loss: 0.0439\n",
      "Epoch 17/150 | Batch 3050/3450 | Loss: 0.0467\n",
      "Epoch 17/150 | Batch 3100/3450 | Loss: 0.0204\n",
      "Epoch 17/150 | Batch 3150/3450 | Loss: 0.0340\n",
      "Epoch 17/150 | Batch 3200/3450 | Loss: 0.0242\n",
      "Epoch 17/150 | Batch 3250/3450 | Loss: 0.0158\n",
      "Epoch 17/150 | Batch 3300/3450 | Loss: 0.0216\n",
      "Epoch 17/150 | Batch 3350/3450 | Loss: 0.0275\n",
      "Epoch 17/150 | Batch 3400/3450 | Loss: 0.0680\n",
      "Epoch 17/150 | Batch 3450/3450 | Loss: 0.0219\n",
      "\n",
      "Epoch 17/150 Summary:\n",
      "Time: 5030.97s | Total: 23:47:53\n",
      "LR: 0.00019717\n",
      "Train Loss: 0.0345\n",
      "Val Loss: 0.0184\n",
      "PSNR: 31.77 | SSIM: 0.9193 | LPIPS: 1.9480\n",
      "[INFO] New best model saved with PSNR: 31.77\n",
      "Best PSNR so far: 31.77 at epoch 17\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18/150 | Batch 50/3450 | Loss: 0.0364\n",
      "Epoch 18/150 | Batch 100/3450 | Loss: 0.0433\n",
      "Epoch 18/150 | Batch 150/3450 | Loss: 0.0374\n",
      "Epoch 18/150 | Batch 200/3450 | Loss: 0.0216\n",
      "Epoch 18/150 | Batch 250/3450 | Loss: 0.0211\n",
      "Epoch 18/150 | Batch 300/3450 | Loss: 0.0454\n",
      "Epoch 18/150 | Batch 350/3450 | Loss: 0.0561\n",
      "Epoch 18/150 | Batch 400/3450 | Loss: 0.0756\n",
      "Epoch 18/150 | Batch 450/3450 | Loss: 0.0398\n",
      "Epoch 18/150 | Batch 500/3450 | Loss: 0.0184\n",
      "Epoch 18/150 | Batch 550/3450 | Loss: 0.0119\n",
      "Epoch 18/150 | Batch 600/3450 | Loss: 0.0410\n",
      "Epoch 18/150 | Batch 650/3450 | Loss: 0.0171\n",
      "Epoch 18/150 | Batch 700/3450 | Loss: 0.0331\n",
      "Epoch 18/150 | Batch 750/3450 | Loss: 0.0421\n",
      "Epoch 18/150 | Batch 800/3450 | Loss: 0.0257\n",
      "Epoch 18/150 | Batch 850/3450 | Loss: 0.0815\n",
      "Epoch 18/150 | Batch 900/3450 | Loss: 0.0677\n",
      "Epoch 18/150 | Batch 950/3450 | Loss: 0.0228\n",
      "Epoch 18/150 | Batch 1000/3450 | Loss: 0.0235\n",
      "Epoch 18/150 | Batch 1050/3450 | Loss: 0.0735\n",
      "Epoch 18/150 | Batch 1100/3450 | Loss: 0.0110\n",
      "Epoch 18/150 | Batch 1150/3450 | Loss: 0.0129\n",
      "Epoch 18/150 | Batch 1200/3450 | Loss: 0.0212\n",
      "Epoch 18/150 | Batch 1250/3450 | Loss: 0.0225\n",
      "Epoch 18/150 | Batch 1300/3450 | Loss: 0.0139\n",
      "Epoch 18/150 | Batch 1350/3450 | Loss: 0.0360\n",
      "Epoch 18/150 | Batch 1400/3450 | Loss: 0.0586\n",
      "Epoch 18/150 | Batch 1450/3450 | Loss: 0.0276\n",
      "Epoch 18/150 | Batch 1500/3450 | Loss: 0.0231\n",
      "Epoch 18/150 | Batch 1550/3450 | Loss: 0.0174\n",
      "Epoch 18/150 | Batch 1600/3450 | Loss: 0.0364\n",
      "Epoch 18/150 | Batch 1650/3450 | Loss: 0.0184\n",
      "Epoch 18/150 | Batch 1700/3450 | Loss: 0.0169\n",
      "Epoch 18/150 | Batch 1750/3450 | Loss: 0.0286\n",
      "Epoch 18/150 | Batch 1800/3450 | Loss: 0.0296\n",
      "Epoch 18/150 | Batch 1850/3450 | Loss: 0.0210\n",
      "Epoch 18/150 | Batch 1900/3450 | Loss: 0.0689\n",
      "Epoch 18/150 | Batch 1950/3450 | Loss: 0.0256\n",
      "Epoch 18/150 | Batch 2000/3450 | Loss: 0.0311\n",
      "Epoch 18/150 | Batch 2050/3450 | Loss: 0.0241\n",
      "Epoch 18/150 | Batch 2100/3450 | Loss: 0.0436\n",
      "Epoch 18/150 | Batch 2150/3450 | Loss: 0.0190\n",
      "Epoch 18/150 | Batch 2200/3450 | Loss: 0.0231\n",
      "Epoch 18/150 | Batch 2250/3450 | Loss: 0.0327\n",
      "Epoch 18/150 | Batch 2300/3450 | Loss: 0.0775\n",
      "Epoch 18/150 | Batch 2350/3450 | Loss: 0.0615\n",
      "Epoch 18/150 | Batch 2400/3450 | Loss: 0.0481\n",
      "Epoch 18/150 | Batch 2450/3450 | Loss: 0.0212\n",
      "Epoch 18/150 | Batch 2500/3450 | Loss: 0.0221\n",
      "Epoch 18/150 | Batch 2550/3450 | Loss: 0.0264\n",
      "Epoch 18/150 | Batch 2600/3450 | Loss: 0.0084\n",
      "Epoch 18/150 | Batch 2650/3450 | Loss: 0.0466\n",
      "Epoch 18/150 | Batch 2700/3450 | Loss: 0.0114\n",
      "Epoch 18/150 | Batch 2750/3450 | Loss: 0.0278\n",
      "Epoch 18/150 | Batch 2800/3450 | Loss: 0.0313\n",
      "Epoch 18/150 | Batch 2850/3450 | Loss: 0.0558\n",
      "Epoch 18/150 | Batch 2900/3450 | Loss: 0.0293\n",
      "Epoch 18/150 | Batch 2950/3450 | Loss: 0.0297\n",
      "Epoch 18/150 | Batch 3000/3450 | Loss: 0.0184\n",
      "Epoch 18/150 | Batch 3050/3450 | Loss: 0.0180\n",
      "Epoch 18/150 | Batch 3100/3450 | Loss: 0.0129\n",
      "Epoch 18/150 | Batch 3150/3450 | Loss: 0.0185\n",
      "Epoch 18/150 | Batch 3200/3450 | Loss: 0.0174\n",
      "Epoch 18/150 | Batch 3250/3450 | Loss: 0.0521\n",
      "Epoch 18/150 | Batch 3300/3450 | Loss: 0.0413\n",
      "Epoch 18/150 | Batch 3350/3450 | Loss: 0.0181\n",
      "Epoch 18/150 | Batch 3400/3450 | Loss: 0.0420\n",
      "Epoch 18/150 | Batch 3450/3450 | Loss: 0.0822\n",
      "\n",
      "Epoch 18/150 Summary:\n",
      "Time: 5020.10s | Total: 1 day, 1:11:36\n",
      "LR: 0.00019664\n",
      "Train Loss: 0.0335\n",
      "Val Loss: 0.0208\n",
      "PSNR: 30.68 | SSIM: 0.9113 | LPIPS: 2.0302\n",
      "Best PSNR so far: 31.77 at epoch 17\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19/150 | Batch 50/3450 | Loss: 0.0247\n",
      "Epoch 19/150 | Batch 100/3450 | Loss: 0.0459\n",
      "Epoch 19/150 | Batch 150/3450 | Loss: 0.0380\n",
      "Epoch 19/150 | Batch 200/3450 | Loss: 0.0505\n",
      "Epoch 19/150 | Batch 250/3450 | Loss: 0.1072\n",
      "Epoch 19/150 | Batch 300/3450 | Loss: 0.0377\n",
      "Epoch 19/150 | Batch 350/3450 | Loss: 0.0589\n",
      "Epoch 19/150 | Batch 400/3450 | Loss: 0.0237\n",
      "Epoch 19/150 | Batch 450/3450 | Loss: 0.0185\n",
      "Epoch 19/150 | Batch 500/3450 | Loss: 0.0785\n",
      "Epoch 19/150 | Batch 550/3450 | Loss: 0.0250\n",
      "Epoch 19/150 | Batch 600/3450 | Loss: 0.0184\n",
      "Epoch 19/150 | Batch 650/3450 | Loss: 0.0258\n",
      "Epoch 19/150 | Batch 700/3450 | Loss: 0.0116\n",
      "Epoch 19/150 | Batch 750/3450 | Loss: 0.0323\n",
      "Epoch 19/150 | Batch 800/3450 | Loss: 0.0220\n",
      "Epoch 19/150 | Batch 850/3450 | Loss: 0.0570\n",
      "Epoch 19/150 | Batch 900/3450 | Loss: 0.0213\n",
      "Epoch 19/150 | Batch 950/3450 | Loss: 0.0582\n",
      "Epoch 19/150 | Batch 1000/3450 | Loss: 0.0223\n",
      "Epoch 19/150 | Batch 1050/3450 | Loss: 0.0167\n",
      "Epoch 19/150 | Batch 1100/3450 | Loss: 0.0295\n",
      "Epoch 19/150 | Batch 1150/3450 | Loss: 0.0337\n",
      "Epoch 19/150 | Batch 1200/3450 | Loss: 0.0497\n",
      "Epoch 19/150 | Batch 1250/3450 | Loss: 0.0298\n",
      "Epoch 19/150 | Batch 1300/3450 | Loss: 0.0446\n",
      "Epoch 19/150 | Batch 1350/3450 | Loss: 0.0283\n",
      "Epoch 19/150 | Batch 1400/3450 | Loss: 0.0224\n",
      "Epoch 19/150 | Batch 1450/3450 | Loss: 0.0163\n",
      "Epoch 19/150 | Batch 1500/3450 | Loss: 0.0218\n",
      "Epoch 19/150 | Batch 1550/3450 | Loss: 0.0463\n",
      "Epoch 19/150 | Batch 1600/3450 | Loss: 0.0384\n",
      "Epoch 19/150 | Batch 1650/3450 | Loss: 0.0306\n",
      "Epoch 19/150 | Batch 1700/3450 | Loss: 0.0357\n",
      "Epoch 19/150 | Batch 1750/3450 | Loss: 0.0227\n",
      "Epoch 19/150 | Batch 1800/3450 | Loss: 0.0142\n",
      "Epoch 19/150 | Batch 1850/3450 | Loss: 0.0292\n",
      "Epoch 19/150 | Batch 1900/3450 | Loss: 0.0608\n",
      "Epoch 19/150 | Batch 1950/3450 | Loss: 0.0281\n",
      "Epoch 19/150 | Batch 2000/3450 | Loss: 0.0228\n",
      "Epoch 19/150 | Batch 2050/3450 | Loss: 0.0140\n",
      "Epoch 19/150 | Batch 2100/3450 | Loss: 0.0396\n",
      "Epoch 19/150 | Batch 2150/3450 | Loss: 0.0148\n",
      "Epoch 19/150 | Batch 2200/3450 | Loss: 0.0152\n",
      "Epoch 19/150 | Batch 2250/3450 | Loss: 0.0360\n",
      "Epoch 19/150 | Batch 2300/3450 | Loss: 0.0153\n",
      "Epoch 19/150 | Batch 2350/3450 | Loss: 0.0423\n",
      "Epoch 19/150 | Batch 2400/3450 | Loss: 0.0434\n",
      "Epoch 19/150 | Batch 2450/3450 | Loss: 0.0301\n",
      "Epoch 19/150 | Batch 2500/3450 | Loss: 0.0276\n",
      "Epoch 19/150 | Batch 2550/3450 | Loss: 0.0712\n",
      "Epoch 19/150 | Batch 2600/3450 | Loss: 0.0226\n",
      "Epoch 19/150 | Batch 2650/3450 | Loss: 0.0134\n",
      "Epoch 19/150 | Batch 2700/3450 | Loss: 0.0225\n",
      "Epoch 19/150 | Batch 2750/3450 | Loss: 0.0563\n",
      "Epoch 19/150 | Batch 2800/3450 | Loss: 0.0155\n",
      "Epoch 19/150 | Batch 2850/3450 | Loss: 0.0179\n",
      "Epoch 19/150 | Batch 2900/3450 | Loss: 0.0229\n",
      "Epoch 19/150 | Batch 2950/3450 | Loss: 0.0187\n",
      "Epoch 19/150 | Batch 3000/3450 | Loss: 0.0172\n",
      "Epoch 19/150 | Batch 3050/3450 | Loss: 0.0245\n",
      "Epoch 19/150 | Batch 3100/3450 | Loss: 0.0221\n",
      "Epoch 19/150 | Batch 3150/3450 | Loss: 0.0262\n",
      "Epoch 19/150 | Batch 3200/3450 | Loss: 0.0184\n",
      "Epoch 19/150 | Batch 3250/3450 | Loss: 0.0562\n",
      "Epoch 19/150 | Batch 3300/3450 | Loss: 0.0680\n",
      "Epoch 19/150 | Batch 3350/3450 | Loss: 0.0572\n",
      "Epoch 19/150 | Batch 3400/3450 | Loss: 0.0302\n",
      "Epoch 19/150 | Batch 3450/3450 | Loss: 0.0336\n",
      "\n",
      "Epoch 19/150 Summary:\n",
      "Time: 5018.12s | Total: 1 day, 2:35:15\n",
      "LR: 0.00019606\n",
      "Train Loss: 0.0339\n",
      "Val Loss: 0.0229\n",
      "PSNR: 30.36 | SSIM: 0.9209 | LPIPS: 1.9635\n",
      "Best PSNR so far: 31.77 at epoch 17\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 20/150 | Batch 50/3450 | Loss: 0.0442\n",
      "Epoch 20/150 | Batch 100/3450 | Loss: 0.0152\n",
      "Epoch 20/150 | Batch 150/3450 | Loss: 0.0135\n",
      "Epoch 20/150 | Batch 200/3450 | Loss: 0.0258\n",
      "Epoch 20/150 | Batch 250/3450 | Loss: 0.0667\n",
      "Epoch 20/150 | Batch 300/3450 | Loss: 0.0122\n",
      "Epoch 20/150 | Batch 350/3450 | Loss: 0.0621\n",
      "Epoch 20/150 | Batch 400/3450 | Loss: 0.0757\n",
      "Epoch 20/150 | Batch 450/3450 | Loss: 0.0284\n",
      "Epoch 20/150 | Batch 500/3450 | Loss: 0.0225\n",
      "Epoch 20/150 | Batch 550/3450 | Loss: 0.0177\n",
      "Epoch 20/150 | Batch 600/3450 | Loss: 0.0172\n",
      "Epoch 20/150 | Batch 650/3450 | Loss: 0.0270\n",
      "Epoch 20/150 | Batch 700/3450 | Loss: 0.0240\n",
      "Epoch 20/150 | Batch 750/3450 | Loss: 0.0402\n",
      "Epoch 20/150 | Batch 800/3450 | Loss: 0.0144\n",
      "Epoch 20/150 | Batch 850/3450 | Loss: 0.0147\n",
      "Epoch 20/150 | Batch 900/3450 | Loss: 0.0311\n",
      "Epoch 20/150 | Batch 950/3450 | Loss: 0.0294\n",
      "Epoch 20/150 | Batch 1000/3450 | Loss: 0.0226\n",
      "Epoch 20/150 | Batch 1050/3450 | Loss: 0.0560\n",
      "Epoch 20/150 | Batch 1100/3450 | Loss: 0.0080\n",
      "Epoch 20/150 | Batch 1150/3450 | Loss: 0.0573\n",
      "Epoch 20/150 | Batch 1200/3450 | Loss: 0.0153\n",
      "Epoch 20/150 | Batch 1250/3450 | Loss: 0.0101\n",
      "Epoch 20/150 | Batch 1300/3450 | Loss: 0.0233\n",
      "Epoch 20/150 | Batch 1350/3450 | Loss: 0.0248\n",
      "Epoch 20/150 | Batch 1400/3450 | Loss: 0.0081\n",
      "Epoch 20/150 | Batch 1450/3450 | Loss: 0.0249\n",
      "Epoch 20/150 | Batch 1500/3450 | Loss: 0.0128\n",
      "Epoch 20/150 | Batch 1550/3450 | Loss: 0.0285\n",
      "Epoch 20/150 | Batch 1600/3450 | Loss: 0.0595\n",
      "Epoch 20/150 | Batch 1650/3450 | Loss: 0.0207\n",
      "Epoch 20/150 | Batch 1700/3450 | Loss: 0.0278\n",
      "Epoch 20/150 | Batch 1750/3450 | Loss: 0.0201\n",
      "Epoch 20/150 | Batch 1800/3450 | Loss: 0.0506\n",
      "Epoch 20/150 | Batch 1850/3450 | Loss: 0.0514\n",
      "Epoch 20/150 | Batch 1900/3450 | Loss: 0.0147\n",
      "Epoch 20/150 | Batch 1950/3450 | Loss: 0.0218\n",
      "Epoch 20/150 | Batch 2000/3450 | Loss: 0.0171\n",
      "Epoch 20/150 | Batch 2050/3450 | Loss: 0.0436\n",
      "Epoch 20/150 | Batch 2100/3450 | Loss: 0.0080\n",
      "Epoch 20/150 | Batch 2150/3450 | Loss: 0.0153\n",
      "Epoch 20/150 | Batch 2200/3450 | Loss: 0.0492\n",
      "Epoch 20/150 | Batch 2250/3450 | Loss: 0.0169\n",
      "Epoch 20/150 | Batch 2300/3450 | Loss: 0.0356\n",
      "Epoch 20/150 | Batch 2350/3450 | Loss: 0.0327\n",
      "Epoch 20/150 | Batch 2400/3450 | Loss: 0.0500\n",
      "Epoch 20/150 | Batch 2450/3450 | Loss: 0.0471\n",
      "Epoch 20/150 | Batch 2500/3450 | Loss: 0.0520\n",
      "Epoch 20/150 | Batch 2550/3450 | Loss: 0.0407\n",
      "Epoch 20/150 | Batch 2600/3450 | Loss: 0.0616\n",
      "Epoch 20/150 | Batch 2650/3450 | Loss: 0.1008\n",
      "Epoch 20/150 | Batch 2700/3450 | Loss: 0.0602\n",
      "Epoch 20/150 | Batch 2750/3450 | Loss: 0.0576\n",
      "Epoch 20/150 | Batch 2800/3450 | Loss: 0.0815\n",
      "Epoch 20/150 | Batch 2850/3450 | Loss: 0.0644\n",
      "Epoch 20/150 | Batch 2900/3450 | Loss: 0.0693\n",
      "Epoch 20/150 | Batch 2950/3450 | Loss: 0.0223\n",
      "Epoch 20/150 | Batch 3000/3450 | Loss: 0.0424\n",
      "Epoch 20/150 | Batch 3050/3450 | Loss: 0.0171\n",
      "Epoch 20/150 | Batch 3100/3450 | Loss: 0.0224\n",
      "Epoch 20/150 | Batch 3150/3450 | Loss: 0.0423\n",
      "Epoch 20/150 | Batch 3200/3450 | Loss: 0.0149\n",
      "Epoch 20/150 | Batch 3250/3450 | Loss: 0.0425\n",
      "Epoch 20/150 | Batch 3300/3450 | Loss: 0.0173\n",
      "Epoch 20/150 | Batch 3350/3450 | Loss: 0.0069\n",
      "Epoch 20/150 | Batch 3400/3450 | Loss: 0.0186\n",
      "Epoch 20/150 | Batch 3450/3450 | Loss: 0.0580\n",
      "\n",
      "Epoch 20/150 Summary:\n",
      "Time: 5020.34s | Total: 1 day, 3:58:58\n",
      "LR: 0.00019543\n",
      "Train Loss: 0.0337\n",
      "Val Loss: 0.0177\n",
      "PSNR: 31.93 | SSIM: 0.9248 | LPIPS: 1.9360\n",
      "[INFO] New best model saved with PSNR: 31.93\n",
      "Best PSNR so far: 31.93 at epoch 20\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 21/150 | Batch 50/3450 | Loss: 0.0114\n",
      "Epoch 21/150 | Batch 100/3450 | Loss: 0.0186\n",
      "Epoch 21/150 | Batch 150/3450 | Loss: 0.0092\n",
      "Epoch 21/150 | Batch 200/3450 | Loss: 0.0332\n",
      "Epoch 21/150 | Batch 250/3450 | Loss: 0.0572\n",
      "Epoch 21/150 | Batch 300/3450 | Loss: 0.0370\n",
      "Epoch 21/150 | Batch 350/3450 | Loss: 0.0299\n",
      "Epoch 21/150 | Batch 400/3450 | Loss: 0.0139\n",
      "Epoch 21/150 | Batch 450/3450 | Loss: 0.0398\n",
      "Epoch 21/150 | Batch 500/3450 | Loss: 0.0269\n",
      "Epoch 21/150 | Batch 550/3450 | Loss: 0.0297\n",
      "Epoch 21/150 | Batch 600/3450 | Loss: 0.0409\n",
      "Epoch 21/150 | Batch 650/3450 | Loss: 0.0458\n",
      "Epoch 21/150 | Batch 700/3450 | Loss: 0.0770\n",
      "Epoch 21/150 | Batch 750/3450 | Loss: 0.0354\n",
      "Epoch 21/150 | Batch 800/3450 | Loss: 0.0260\n",
      "Epoch 21/150 | Batch 850/3450 | Loss: 0.0163\n",
      "Epoch 21/150 | Batch 900/3450 | Loss: 0.0218\n",
      "Epoch 21/150 | Batch 950/3450 | Loss: 0.0348\n",
      "Epoch 21/150 | Batch 1000/3450 | Loss: 0.0283\n",
      "Epoch 21/150 | Batch 1050/3450 | Loss: 0.0294\n",
      "Epoch 21/150 | Batch 1100/3450 | Loss: 0.0168\n",
      "Epoch 21/150 | Batch 1150/3450 | Loss: 0.0326\n",
      "Epoch 21/150 | Batch 1200/3450 | Loss: 0.0541\n",
      "Epoch 21/150 | Batch 1250/3450 | Loss: 0.0512\n",
      "Epoch 21/150 | Batch 1300/3450 | Loss: 0.0251\n",
      "Epoch 21/150 | Batch 1350/3450 | Loss: 0.0679\n",
      "Epoch 21/150 | Batch 1400/3450 | Loss: 0.0447\n",
      "Epoch 21/150 | Batch 1450/3450 | Loss: 0.0303\n",
      "Epoch 21/150 | Batch 1500/3450 | Loss: 0.0213\n",
      "Epoch 21/150 | Batch 1550/3450 | Loss: 0.0341\n",
      "Epoch 21/150 | Batch 1600/3450 | Loss: 0.0270\n",
      "Epoch 21/150 | Batch 1650/3450 | Loss: 0.0399\n",
      "Epoch 21/150 | Batch 1700/3450 | Loss: 0.0559\n",
      "Epoch 21/150 | Batch 1750/3450 | Loss: 0.0078\n",
      "Epoch 21/150 | Batch 1800/3450 | Loss: 0.0187\n",
      "Epoch 21/150 | Batch 1850/3450 | Loss: 0.0169\n",
      "Epoch 21/150 | Batch 1900/3450 | Loss: 0.0412\n",
      "Epoch 21/150 | Batch 1950/3450 | Loss: 0.0190\n",
      "Epoch 21/150 | Batch 2000/3450 | Loss: 0.0191\n",
      "Epoch 21/150 | Batch 2050/3450 | Loss: 0.0172\n",
      "Epoch 21/150 | Batch 2100/3450 | Loss: 0.0169\n",
      "Epoch 21/150 | Batch 2150/3450 | Loss: 0.0197\n",
      "Epoch 21/150 | Batch 2200/3450 | Loss: 0.0194\n",
      "Epoch 21/150 | Batch 2250/3450 | Loss: 0.0108\n",
      "Epoch 21/150 | Batch 2300/3450 | Loss: 0.0272\n",
      "Epoch 21/150 | Batch 2350/3450 | Loss: 0.0365\n",
      "Epoch 21/150 | Batch 2400/3450 | Loss: 0.0638\n",
      "Epoch 21/150 | Batch 2450/3450 | Loss: 0.0309\n",
      "Epoch 21/150 | Batch 2500/3450 | Loss: 0.0315\n",
      "Epoch 21/150 | Batch 2550/3450 | Loss: 0.0359\n",
      "Epoch 21/150 | Batch 2600/3450 | Loss: 0.0448\n",
      "Epoch 21/150 | Batch 2650/3450 | Loss: 0.0207\n",
      "Epoch 21/150 | Batch 2700/3450 | Loss: 0.0160\n",
      "Epoch 21/150 | Batch 2750/3450 | Loss: 0.0151\n",
      "Epoch 21/150 | Batch 2800/3450 | Loss: 0.0495\n",
      "Epoch 21/150 | Batch 2850/3450 | Loss: 0.0123\n",
      "Epoch 21/150 | Batch 2900/3450 | Loss: 0.0115\n",
      "Epoch 21/150 | Batch 2950/3450 | Loss: 0.0268\n",
      "Epoch 21/150 | Batch 3000/3450 | Loss: 0.0470\n",
      "Epoch 21/150 | Batch 3050/3450 | Loss: 0.0102\n",
      "Epoch 21/150 | Batch 3100/3450 | Loss: 0.0232\n",
      "Epoch 21/150 | Batch 3150/3450 | Loss: 0.0280\n",
      "Epoch 21/150 | Batch 3200/3450 | Loss: 0.0693\n",
      "Epoch 21/150 | Batch 3250/3450 | Loss: 0.0678\n",
      "Epoch 21/150 | Batch 3300/3450 | Loss: 0.0165\n",
      "Epoch 21/150 | Batch 3350/3450 | Loss: 0.0313\n",
      "Epoch 21/150 | Batch 3400/3450 | Loss: 0.0281\n",
      "Epoch 21/150 | Batch 3450/3450 | Loss: 0.0223\n",
      "\n",
      "Epoch 21/150 Summary:\n",
      "Time: 5017.85s | Total: 1 day, 5:22:38\n",
      "LR: 0.00019477\n",
      "Train Loss: 0.0331\n",
      "Val Loss: 0.0205\n",
      "PSNR: 31.11 | SSIM: 0.9228 | LPIPS: 1.9206\n",
      "Best PSNR so far: 31.93 at epoch 20\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 22/150 | Batch 50/3450 | Loss: 0.0729\n",
      "Epoch 22/150 | Batch 100/3450 | Loss: 0.0373\n",
      "Epoch 22/150 | Batch 150/3450 | Loss: 0.0168\n",
      "Epoch 22/150 | Batch 200/3450 | Loss: 0.0129\n",
      "Epoch 22/150 | Batch 250/3450 | Loss: 0.0414\n",
      "Epoch 22/150 | Batch 300/3450 | Loss: 0.0136\n",
      "Epoch 22/150 | Batch 350/3450 | Loss: 0.0123\n",
      "Epoch 22/150 | Batch 400/3450 | Loss: 0.0188\n",
      "Epoch 22/150 | Batch 450/3450 | Loss: 0.0329\n",
      "Epoch 22/150 | Batch 500/3450 | Loss: 0.0251\n",
      "Epoch 22/150 | Batch 550/3450 | Loss: 0.0564\n",
      "Epoch 22/150 | Batch 600/3450 | Loss: 0.0105\n",
      "Epoch 22/150 | Batch 650/3450 | Loss: 0.0129\n",
      "Epoch 22/150 | Batch 700/3450 | Loss: 0.0437\n",
      "Epoch 22/150 | Batch 750/3450 | Loss: 0.0101\n",
      "Epoch 22/150 | Batch 800/3450 | Loss: 0.0501\n",
      "Epoch 22/150 | Batch 850/3450 | Loss: 0.0521\n",
      "Epoch 22/150 | Batch 900/3450 | Loss: 0.0309\n",
      "Epoch 22/150 | Batch 950/3450 | Loss: 0.0273\n",
      "Epoch 22/150 | Batch 1000/3450 | Loss: 0.0157\n",
      "Epoch 22/150 | Batch 1050/3450 | Loss: 0.0155\n",
      "Epoch 22/150 | Batch 1100/3450 | Loss: 0.0195\n",
      "Epoch 22/150 | Batch 1150/3450 | Loss: 0.0748\n",
      "Epoch 22/150 | Batch 1200/3450 | Loss: 0.0386\n",
      "Epoch 22/150 | Batch 1250/3450 | Loss: 0.0220\n",
      "Epoch 22/150 | Batch 1300/3450 | Loss: 0.0246\n",
      "Epoch 22/150 | Batch 1350/3450 | Loss: 0.0417\n",
      "Epoch 22/150 | Batch 1400/3450 | Loss: 0.0202\n",
      "Epoch 22/150 | Batch 1450/3450 | Loss: 0.0284\n",
      "Epoch 22/150 | Batch 1500/3450 | Loss: 0.0301\n",
      "Epoch 22/150 | Batch 1550/3450 | Loss: 0.0160\n",
      "Epoch 22/150 | Batch 1600/3450 | Loss: 0.0253\n",
      "Epoch 22/150 | Batch 1650/3450 | Loss: 0.0300\n",
      "Epoch 22/150 | Batch 1700/3450 | Loss: 0.0123\n",
      "Epoch 22/150 | Batch 1750/3450 | Loss: 0.0611\n",
      "Epoch 22/150 | Batch 1800/3450 | Loss: 0.0173\n",
      "Epoch 22/150 | Batch 1850/3450 | Loss: 0.0082\n",
      "Epoch 22/150 | Batch 1900/3450 | Loss: 0.0739\n",
      "Epoch 22/150 | Batch 1950/3450 | Loss: 0.0115\n",
      "Epoch 22/150 | Batch 2000/3450 | Loss: 0.0276\n",
      "Epoch 22/150 | Batch 2050/3450 | Loss: 0.0360\n",
      "Epoch 22/150 | Batch 2100/3450 | Loss: 0.0349\n",
      "Epoch 22/150 | Batch 2150/3450 | Loss: 0.0513\n",
      "Epoch 22/150 | Batch 2200/3450 | Loss: 0.0219\n",
      "Epoch 22/150 | Batch 2250/3450 | Loss: 0.0710\n",
      "Epoch 22/150 | Batch 2300/3450 | Loss: 0.0493\n",
      "Epoch 22/150 | Batch 2350/3450 | Loss: 0.0256\n",
      "Epoch 22/150 | Batch 2400/3450 | Loss: 0.0222\n",
      "Epoch 22/150 | Batch 2450/3450 | Loss: 0.0150\n",
      "Epoch 22/150 | Batch 2500/3450 | Loss: 0.0142\n",
      "Epoch 22/150 | Batch 2550/3450 | Loss: 0.0401\n",
      "Epoch 22/150 | Batch 2600/3450 | Loss: 0.0287\n",
      "Epoch 22/150 | Batch 2650/3450 | Loss: 0.0169\n",
      "Epoch 22/150 | Batch 2700/3450 | Loss: 0.0459\n",
      "Epoch 22/150 | Batch 2750/3450 | Loss: 0.0553\n",
      "Epoch 22/150 | Batch 2800/3450 | Loss: 0.0151\n",
      "Epoch 22/150 | Batch 2850/3450 | Loss: 0.0512\n",
      "Epoch 22/150 | Batch 2900/3450 | Loss: 0.0096\n",
      "Epoch 22/150 | Batch 2950/3450 | Loss: 0.0432\n",
      "Epoch 22/150 | Batch 3000/3450 | Loss: 0.0600\n",
      "Epoch 22/150 | Batch 3050/3450 | Loss: 0.0104\n",
      "Epoch 22/150 | Batch 3100/3450 | Loss: 0.0416\n",
      "Epoch 22/150 | Batch 3150/3450 | Loss: 0.0099\n",
      "Epoch 22/150 | Batch 3200/3450 | Loss: 0.0307\n",
      "Epoch 22/150 | Batch 3250/3450 | Loss: 0.0130\n",
      "Epoch 22/150 | Batch 3300/3450 | Loss: 0.0336\n",
      "Epoch 22/150 | Batch 3350/3450 | Loss: 0.0606\n",
      "Epoch 22/150 | Batch 3400/3450 | Loss: 0.0575\n",
      "Epoch 22/150 | Batch 3450/3450 | Loss: 0.0298\n",
      "\n",
      "Epoch 22/150 Summary:\n",
      "Time: 5022.42s | Total: 1 day, 6:46:22\n",
      "LR: 0.00019405\n",
      "Train Loss: 0.0326\n",
      "Val Loss: 0.0241\n",
      "PSNR: 29.61 | SSIM: 0.9148 | LPIPS: 2.0037\n",
      "Best PSNR so far: 31.93 at epoch 20\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 23/150 | Batch 50/3450 | Loss: 0.0305\n",
      "Epoch 23/150 | Batch 100/3450 | Loss: 0.0203\n",
      "Epoch 23/150 | Batch 150/3450 | Loss: 0.0309\n",
      "Epoch 23/150 | Batch 200/3450 | Loss: 0.0180\n",
      "Epoch 23/150 | Batch 250/3450 | Loss: 0.0507\n",
      "Epoch 23/150 | Batch 300/3450 | Loss: 0.0359\n",
      "Epoch 23/150 | Batch 350/3450 | Loss: 0.0235\n",
      "Epoch 23/150 | Batch 400/3450 | Loss: 0.0317\n",
      "Epoch 23/150 | Batch 450/3450 | Loss: 0.0247\n",
      "Epoch 23/150 | Batch 500/3450 | Loss: 0.0263\n",
      "Epoch 23/150 | Batch 550/3450 | Loss: 0.0197\n",
      "Epoch 23/150 | Batch 600/3450 | Loss: 0.0187\n",
      "Epoch 23/150 | Batch 650/3450 | Loss: 0.0335\n",
      "Epoch 23/150 | Batch 700/3450 | Loss: 0.0928\n",
      "Epoch 23/150 | Batch 750/3450 | Loss: 0.0221\n",
      "Epoch 23/150 | Batch 800/3450 | Loss: 0.0142\n",
      "Epoch 23/150 | Batch 850/3450 | Loss: 0.0160\n",
      "Epoch 23/150 | Batch 900/3450 | Loss: 0.0576\n",
      "Epoch 23/150 | Batch 950/3450 | Loss: 0.0397\n",
      "Epoch 23/150 | Batch 1000/3450 | Loss: 0.0478\n",
      "Epoch 23/150 | Batch 1050/3450 | Loss: 0.0204\n",
      "Epoch 23/150 | Batch 1100/3450 | Loss: 0.0167\n",
      "Epoch 23/150 | Batch 1150/3450 | Loss: 0.0344\n",
      "Epoch 23/150 | Batch 1200/3450 | Loss: 0.0322\n",
      "Epoch 23/150 | Batch 1250/3450 | Loss: 0.0223\n",
      "Epoch 23/150 | Batch 1300/3450 | Loss: 0.0678\n",
      "Epoch 23/150 | Batch 1350/3450 | Loss: 0.0188\n",
      "Epoch 23/150 | Batch 1400/3450 | Loss: 0.0498\n",
      "Epoch 23/150 | Batch 1450/3450 | Loss: 0.0116\n",
      "Epoch 23/150 | Batch 1500/3450 | Loss: 0.0144\n",
      "Epoch 23/150 | Batch 1550/3450 | Loss: 0.0200\n",
      "Epoch 23/150 | Batch 1600/3450 | Loss: 0.0265\n",
      "Epoch 23/150 | Batch 1650/3450 | Loss: 0.0965\n",
      "Epoch 23/150 | Batch 1700/3450 | Loss: 0.0491\n",
      "Epoch 23/150 | Batch 1750/3450 | Loss: 0.0154\n",
      "Epoch 23/150 | Batch 1800/3450 | Loss: 0.0696\n",
      "Epoch 23/150 | Batch 1850/3450 | Loss: 0.0257\n",
      "Epoch 23/150 | Batch 1900/3450 | Loss: 0.0505\n",
      "Epoch 23/150 | Batch 1950/3450 | Loss: 0.0214\n",
      "Epoch 23/150 | Batch 2000/3450 | Loss: 0.0360\n",
      "Epoch 23/150 | Batch 2050/3450 | Loss: 0.0213\n",
      "Epoch 23/150 | Batch 2100/3450 | Loss: 0.0455\n",
      "Epoch 23/150 | Batch 2150/3450 | Loss: 0.0089\n",
      "Epoch 23/150 | Batch 2200/3450 | Loss: 0.0334\n",
      "Epoch 23/150 | Batch 2250/3450 | Loss: 0.0337\n",
      "Epoch 23/150 | Batch 2300/3450 | Loss: 0.0378\n",
      "Epoch 23/150 | Batch 2350/3450 | Loss: 0.0223\n",
      "Epoch 23/150 | Batch 2400/3450 | Loss: 0.0964\n",
      "Epoch 23/150 | Batch 2450/3450 | Loss: 0.0144\n",
      "Epoch 23/150 | Batch 2500/3450 | Loss: 0.0096\n",
      "Epoch 23/150 | Batch 2550/3450 | Loss: 0.0216\n",
      "Epoch 23/150 | Batch 2600/3450 | Loss: 0.0108\n",
      "Epoch 23/150 | Batch 2650/3450 | Loss: 0.0132\n",
      "Epoch 23/150 | Batch 2700/3450 | Loss: 0.0480\n",
      "Epoch 23/150 | Batch 2750/3450 | Loss: 0.0375\n",
      "Epoch 23/150 | Batch 2800/3450 | Loss: 0.0376\n",
      "Epoch 23/150 | Batch 2850/3450 | Loss: 0.0168\n",
      "Epoch 23/150 | Batch 2900/3450 | Loss: 0.0449\n",
      "Epoch 23/150 | Batch 2950/3450 | Loss: 0.0160\n",
      "Epoch 23/150 | Batch 3000/3450 | Loss: 0.0300\n",
      "Epoch 23/150 | Batch 3050/3450 | Loss: 0.0870\n",
      "Epoch 23/150 | Batch 3100/3450 | Loss: 0.0149\n",
      "Epoch 23/150 | Batch 3150/3450 | Loss: 0.0342\n",
      "Epoch 23/150 | Batch 3200/3450 | Loss: 0.0256\n",
      "Epoch 23/150 | Batch 3250/3450 | Loss: 0.0248\n",
      "Epoch 23/150 | Batch 3300/3450 | Loss: 0.0228\n",
      "Epoch 23/150 | Batch 3350/3450 | Loss: 0.0425\n",
      "Epoch 23/150 | Batch 3400/3450 | Loss: 0.0190\n",
      "Epoch 23/150 | Batch 3450/3450 | Loss: 0.0509\n",
      "\n",
      "Epoch 23/150 Summary:\n",
      "Time: 5015.41s | Total: 1 day, 8:10:00\n",
      "LR: 0.00019329\n",
      "Train Loss: 0.0333\n",
      "Val Loss: 0.0199\n",
      "PSNR: 31.08 | SSIM: 0.9233 | LPIPS: 1.9593\n",
      "Best PSNR so far: 31.93 at epoch 20\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 24/150 | Batch 50/3450 | Loss: 0.0136\n",
      "Epoch 24/150 | Batch 100/3450 | Loss: 0.0121\n",
      "Epoch 24/150 | Batch 150/3450 | Loss: 0.0226\n",
      "Epoch 24/150 | Batch 200/3450 | Loss: 0.0797\n",
      "Epoch 24/150 | Batch 250/3450 | Loss: 0.0244\n",
      "Epoch 24/150 | Batch 300/3450 | Loss: 0.0475\n",
      "Epoch 24/150 | Batch 350/3450 | Loss: 0.0282\n",
      "Epoch 24/150 | Batch 400/3450 | Loss: 0.0125\n",
      "Epoch 24/150 | Batch 450/3450 | Loss: 0.0302\n",
      "Epoch 24/150 | Batch 500/3450 | Loss: 0.0187\n",
      "Epoch 24/150 | Batch 550/3450 | Loss: 0.0154\n",
      "Epoch 24/150 | Batch 600/3450 | Loss: 0.0130\n",
      "Epoch 24/150 | Batch 650/3450 | Loss: 0.0142\n",
      "Epoch 24/150 | Batch 700/3450 | Loss: 0.0109\n",
      "Epoch 24/150 | Batch 750/3450 | Loss: 0.0204\n",
      "Epoch 24/150 | Batch 800/3450 | Loss: 0.0263\n",
      "Epoch 24/150 | Batch 850/3450 | Loss: 0.0197\n",
      "Epoch 24/150 | Batch 900/3450 | Loss: 0.0350\n",
      "Epoch 24/150 | Batch 950/3450 | Loss: 0.0203\n",
      "Epoch 24/150 | Batch 1000/3450 | Loss: 0.0174\n",
      "Epoch 24/150 | Batch 1050/3450 | Loss: 0.0204\n",
      "Epoch 24/150 | Batch 1100/3450 | Loss: 0.0336\n",
      "Epoch 24/150 | Batch 1150/3450 | Loss: 0.0652\n",
      "Epoch 24/150 | Batch 1200/3450 | Loss: 0.0537\n",
      "Epoch 24/150 | Batch 1250/3450 | Loss: 0.0460\n",
      "Epoch 24/150 | Batch 1300/3450 | Loss: 0.0393\n",
      "Epoch 24/150 | Batch 1350/3450 | Loss: 0.0366\n",
      "Epoch 24/150 | Batch 1400/3450 | Loss: 0.0122\n",
      "Epoch 24/150 | Batch 1450/3450 | Loss: 0.0409\n",
      "Epoch 24/150 | Batch 1500/3450 | Loss: 0.0280\n",
      "Epoch 24/150 | Batch 1550/3450 | Loss: 0.0287\n",
      "Epoch 24/150 | Batch 1600/3450 | Loss: 0.0529\n",
      "Epoch 24/150 | Batch 1650/3450 | Loss: 0.0420\n",
      "Epoch 24/150 | Batch 1700/3450 | Loss: 0.0194\n",
      "Epoch 24/150 | Batch 1750/3450 | Loss: 0.0208\n",
      "Epoch 24/150 | Batch 1800/3450 | Loss: 0.0228\n",
      "Epoch 24/150 | Batch 1850/3450 | Loss: 0.0725\n",
      "Epoch 24/150 | Batch 1900/3450 | Loss: 0.0378\n",
      "Epoch 24/150 | Batch 1950/3450 | Loss: 0.0301\n",
      "Epoch 24/150 | Batch 2000/3450 | Loss: 0.0526\n",
      "Epoch 24/150 | Batch 2050/3450 | Loss: 0.0249\n",
      "Epoch 24/150 | Batch 2100/3450 | Loss: 0.0199\n",
      "Epoch 24/150 | Batch 2150/3450 | Loss: 0.0200\n",
      "Epoch 24/150 | Batch 2200/3450 | Loss: 0.0177\n",
      "Epoch 24/150 | Batch 2250/3450 | Loss: 0.0357\n",
      "Epoch 24/150 | Batch 2300/3450 | Loss: 0.0517\n",
      "Epoch 24/150 | Batch 2350/3450 | Loss: 0.0342\n",
      "Epoch 24/150 | Batch 2400/3450 | Loss: 0.0161\n",
      "Epoch 24/150 | Batch 2450/3450 | Loss: 0.0336\n",
      "Epoch 24/150 | Batch 2500/3450 | Loss: 0.0074\n",
      "Epoch 24/150 | Batch 2550/3450 | Loss: 0.0192\n",
      "Epoch 24/150 | Batch 2600/3450 | Loss: 0.0187\n",
      "Epoch 24/150 | Batch 2650/3450 | Loss: 0.0666\n",
      "Epoch 24/150 | Batch 2700/3450 | Loss: 0.0249\n",
      "Epoch 24/150 | Batch 2750/3450 | Loss: 0.0213\n",
      "Epoch 24/150 | Batch 2800/3450 | Loss: 0.0313\n",
      "Epoch 24/150 | Batch 2850/3450 | Loss: 0.0481\n",
      "Epoch 24/150 | Batch 2900/3450 | Loss: 0.0135\n",
      "Epoch 24/150 | Batch 2950/3450 | Loss: 0.0092\n",
      "Epoch 24/150 | Batch 3000/3450 | Loss: 0.0190\n",
      "Epoch 24/150 | Batch 3050/3450 | Loss: 0.0218\n",
      "Epoch 24/150 | Batch 3100/3450 | Loss: 0.0674\n",
      "Epoch 24/150 | Batch 3150/3450 | Loss: 0.0186\n",
      "Epoch 24/150 | Batch 3200/3450 | Loss: 0.0443\n",
      "Epoch 24/150 | Batch 3250/3450 | Loss: 0.0252\n",
      "Epoch 24/150 | Batch 3300/3450 | Loss: 0.0171\n",
      "Epoch 24/150 | Batch 3350/3450 | Loss: 0.0880\n",
      "Epoch 24/150 | Batch 3400/3450 | Loss: 0.0249\n",
      "Epoch 24/150 | Batch 3450/3450 | Loss: 0.0141\n",
      "\n",
      "Epoch 24/150 Summary:\n",
      "Time: 5018.67s | Total: 1 day, 9:33:40\n",
      "LR: 0.00019249\n",
      "Train Loss: 0.0333\n",
      "Val Loss: 0.0187\n",
      "PSNR: 31.49 | SSIM: 0.9258 | LPIPS: 1.9006\n",
      "Best PSNR so far: 31.93 at epoch 20\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 25/150 | Batch 50/3450 | Loss: 0.0564\n",
      "Epoch 25/150 | Batch 100/3450 | Loss: 0.0475\n",
      "Epoch 25/150 | Batch 150/3450 | Loss: 0.0357\n",
      "Epoch 25/150 | Batch 200/3450 | Loss: 0.0213\n",
      "Epoch 25/150 | Batch 250/3450 | Loss: 0.0251\n",
      "Epoch 25/150 | Batch 300/3450 | Loss: 0.0209\n",
      "Epoch 25/150 | Batch 350/3450 | Loss: 0.0949\n",
      "Epoch 25/150 | Batch 400/3450 | Loss: 0.0722\n",
      "Epoch 25/150 | Batch 450/3450 | Loss: 0.0413\n",
      "Epoch 25/150 | Batch 500/3450 | Loss: 0.0417\n",
      "Epoch 25/150 | Batch 550/3450 | Loss: 0.0189\n",
      "Epoch 25/150 | Batch 600/3450 | Loss: 0.0305\n",
      "Epoch 25/150 | Batch 650/3450 | Loss: 0.0417\n",
      "Epoch 25/150 | Batch 700/3450 | Loss: 0.0314\n",
      "Epoch 25/150 | Batch 750/3450 | Loss: 0.0253\n",
      "Epoch 25/150 | Batch 800/3450 | Loss: 0.0261\n",
      "Epoch 25/150 | Batch 850/3450 | Loss: 0.0533\n",
      "Epoch 25/150 | Batch 900/3450 | Loss: 0.0159\n",
      "Epoch 25/150 | Batch 950/3450 | Loss: 0.0188\n",
      "Epoch 25/150 | Batch 1000/3450 | Loss: 0.0210\n",
      "Epoch 25/150 | Batch 1050/3450 | Loss: 0.0403\n",
      "Epoch 25/150 | Batch 1100/3450 | Loss: 0.0118\n",
      "Epoch 25/150 | Batch 1150/3450 | Loss: 0.0444\n",
      "Epoch 25/150 | Batch 1200/3450 | Loss: 0.0133\n",
      "Epoch 25/150 | Batch 1250/3450 | Loss: 0.0158\n",
      "Epoch 25/150 | Batch 1300/3450 | Loss: 0.0229\n",
      "Epoch 25/150 | Batch 1350/3450 | Loss: 0.0314\n",
      "Epoch 25/150 | Batch 1400/3450 | Loss: 0.0537\n",
      "Epoch 25/150 | Batch 1450/3450 | Loss: 0.0193\n",
      "Epoch 25/150 | Batch 1500/3450 | Loss: 0.0453\n",
      "Epoch 25/150 | Batch 1550/3450 | Loss: 0.0293\n",
      "Epoch 25/150 | Batch 1600/3450 | Loss: 0.0181\n",
      "Epoch 25/150 | Batch 1650/3450 | Loss: 0.0311\n",
      "Epoch 25/150 | Batch 1700/3450 | Loss: 0.0368\n",
      "Epoch 25/150 | Batch 1750/3450 | Loss: 0.0298\n",
      "Epoch 25/150 | Batch 1800/3450 | Loss: 0.0248\n",
      "Epoch 25/150 | Batch 1850/3450 | Loss: 0.0169\n",
      "Epoch 25/150 | Batch 1900/3450 | Loss: 0.0138\n",
      "Epoch 25/150 | Batch 1950/3450 | Loss: 0.0183\n",
      "Epoch 25/150 | Batch 2000/3450 | Loss: 0.0215\n",
      "Epoch 25/150 | Batch 2050/3450 | Loss: 0.0690\n",
      "Epoch 25/150 | Batch 2100/3450 | Loss: 0.0149\n",
      "Epoch 25/150 | Batch 2150/3450 | Loss: 0.0223\n",
      "Epoch 25/150 | Batch 2200/3450 | Loss: 0.0499\n",
      "Epoch 25/150 | Batch 2250/3450 | Loss: 0.0246\n",
      "Epoch 25/150 | Batch 2300/3450 | Loss: 0.0203\n",
      "Epoch 25/150 | Batch 2350/3450 | Loss: 0.0340\n",
      "Epoch 25/150 | Batch 2400/3450 | Loss: 0.0129\n",
      "Epoch 25/150 | Batch 2450/3450 | Loss: 0.0159\n",
      "Epoch 25/150 | Batch 2500/3450 | Loss: 0.0611\n",
      "Epoch 25/150 | Batch 2550/3450 | Loss: 0.0181\n",
      "Epoch 25/150 | Batch 2600/3450 | Loss: 0.0283\n",
      "Epoch 25/150 | Batch 2650/3450 | Loss: 0.0551\n",
      "Epoch 25/150 | Batch 2700/3450 | Loss: 0.0125\n",
      "Epoch 25/150 | Batch 2750/3450 | Loss: 0.0193\n",
      "Epoch 25/150 | Batch 2800/3450 | Loss: 0.0677\n",
      "Epoch 25/150 | Batch 2850/3450 | Loss: 0.0528\n",
      "Epoch 25/150 | Batch 2900/3450 | Loss: 0.0183\n",
      "Epoch 25/150 | Batch 2950/3450 | Loss: 0.0093\n",
      "Epoch 25/150 | Batch 3000/3450 | Loss: 0.0170\n",
      "Epoch 25/150 | Batch 3050/3450 | Loss: 0.0601\n",
      "Epoch 25/150 | Batch 3100/3450 | Loss: 0.0856\n",
      "Epoch 25/150 | Batch 3150/3450 | Loss: 0.0208\n",
      "Epoch 25/150 | Batch 3200/3450 | Loss: 0.0359\n",
      "Epoch 25/150 | Batch 3250/3450 | Loss: 0.0220\n",
      "Epoch 25/150 | Batch 3300/3450 | Loss: 0.0107\n",
      "Epoch 25/150 | Batch 3350/3450 | Loss: 0.0171\n",
      "Epoch 25/150 | Batch 3400/3450 | Loss: 0.0129\n",
      "Epoch 25/150 | Batch 3450/3450 | Loss: 0.0484\n",
      "\n",
      "Epoch 25/150 Summary:\n",
      "Time: 5021.91s | Total: 1 day, 10:57:24\n",
      "LR: 0.00019165\n",
      "Train Loss: 0.0332\n",
      "Val Loss: 0.0169\n",
      "PSNR: 32.43 | SSIM: 0.9264 | LPIPS: 1.8537\n",
      "[INFO] New best model saved with PSNR: 32.43\n",
      "Best PSNR so far: 32.43 at epoch 25\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 26/150 | Batch 50/3450 | Loss: 0.0119\n",
      "Epoch 26/150 | Batch 100/3450 | Loss: 0.0213\n",
      "Epoch 26/150 | Batch 150/3450 | Loss: 0.0435\n",
      "Epoch 26/150 | Batch 200/3450 | Loss: 0.0164\n",
      "Epoch 26/150 | Batch 250/3450 | Loss: 0.0531\n",
      "Epoch 26/150 | Batch 300/3450 | Loss: 0.0348\n",
      "Epoch 26/150 | Batch 350/3450 | Loss: 0.0181\n",
      "Epoch 26/150 | Batch 400/3450 | Loss: 0.0116\n",
      "Epoch 26/150 | Batch 450/3450 | Loss: 0.0183\n",
      "Epoch 26/150 | Batch 500/3450 | Loss: 0.0221\n",
      "Epoch 26/150 | Batch 550/3450 | Loss: 0.0253\n",
      "Epoch 26/150 | Batch 600/3450 | Loss: 0.0527\n",
      "Epoch 26/150 | Batch 650/3450 | Loss: 0.0418\n",
      "Epoch 26/150 | Batch 700/3450 | Loss: 0.0167\n",
      "Epoch 26/150 | Batch 750/3450 | Loss: 0.0059\n",
      "Epoch 26/150 | Batch 800/3450 | Loss: 0.0135\n",
      "Epoch 26/150 | Batch 850/3450 | Loss: 0.0412\n",
      "Epoch 26/150 | Batch 900/3450 | Loss: 0.0331\n",
      "Epoch 26/150 | Batch 950/3450 | Loss: 0.0355\n",
      "Epoch 26/150 | Batch 1000/3450 | Loss: 0.0296\n",
      "Epoch 26/150 | Batch 1050/3450 | Loss: 0.0780\n",
      "Epoch 26/150 | Batch 1100/3450 | Loss: 0.0403\n",
      "Epoch 26/150 | Batch 1150/3450 | Loss: 0.0114\n",
      "Epoch 26/150 | Batch 1200/3450 | Loss: 0.0449\n",
      "Epoch 26/150 | Batch 1250/3450 | Loss: 0.0547\n",
      "Epoch 26/150 | Batch 1300/3450 | Loss: 0.0697\n",
      "Epoch 26/150 | Batch 1350/3450 | Loss: 0.0406\n",
      "Epoch 26/150 | Batch 1400/3450 | Loss: 0.0095\n",
      "Epoch 26/150 | Batch 1450/3450 | Loss: 0.0446\n",
      "Epoch 26/150 | Batch 1500/3450 | Loss: 0.0091\n",
      "Epoch 26/150 | Batch 1550/3450 | Loss: 0.0397\n",
      "Epoch 26/150 | Batch 1600/3450 | Loss: 0.0090\n",
      "Epoch 26/150 | Batch 1650/3450 | Loss: 0.0730\n",
      "Epoch 26/150 | Batch 1700/3450 | Loss: 0.0071\n",
      "Epoch 26/150 | Batch 1750/3450 | Loss: 0.0552\n",
      "Epoch 26/150 | Batch 1800/3450 | Loss: 0.0152\n",
      "Epoch 26/150 | Batch 1850/3450 | Loss: 0.0202\n",
      "Epoch 26/150 | Batch 1900/3450 | Loss: 0.0553\n",
      "Epoch 26/150 | Batch 1950/3450 | Loss: 0.0458\n",
      "Epoch 26/150 | Batch 2000/3450 | Loss: 0.0273\n",
      "Epoch 26/150 | Batch 2050/3450 | Loss: 0.0153\n",
      "Epoch 26/150 | Batch 2100/3450 | Loss: 0.0201\n",
      "Epoch 26/150 | Batch 2150/3450 | Loss: 0.0426\n",
      "Epoch 26/150 | Batch 2200/3450 | Loss: 0.0103\n",
      "Epoch 26/150 | Batch 2250/3450 | Loss: 0.0547\n",
      "Epoch 26/150 | Batch 2300/3450 | Loss: 0.0097\n",
      "Epoch 26/150 | Batch 2350/3450 | Loss: 0.0409\n",
      "Epoch 26/150 | Batch 2400/3450 | Loss: 0.0197\n",
      "Epoch 26/150 | Batch 2450/3450 | Loss: 0.0252\n",
      "Epoch 26/150 | Batch 2500/3450 | Loss: 0.0548\n",
      "Epoch 26/150 | Batch 2550/3450 | Loss: 0.0115\n",
      "Epoch 26/150 | Batch 2600/3450 | Loss: 0.0590\n",
      "Epoch 26/150 | Batch 2650/3450 | Loss: 0.0821\n",
      "Epoch 26/150 | Batch 2700/3450 | Loss: 0.0165\n",
      "Epoch 26/150 | Batch 2750/3450 | Loss: 0.0467\n",
      "Epoch 26/150 | Batch 2800/3450 | Loss: 0.0262\n",
      "Epoch 26/150 | Batch 2850/3450 | Loss: 0.0244\n",
      "Epoch 26/150 | Batch 2900/3450 | Loss: 0.0397\n",
      "Epoch 26/150 | Batch 2950/3450 | Loss: 0.0591\n",
      "Epoch 26/150 | Batch 3000/3450 | Loss: 0.0089\n",
      "Epoch 26/150 | Batch 3050/3450 | Loss: 0.0108\n",
      "Epoch 26/150 | Batch 3100/3450 | Loss: 0.0491\n",
      "Epoch 26/150 | Batch 3150/3450 | Loss: 0.0179\n",
      "Epoch 26/150 | Batch 3200/3450 | Loss: 0.0153\n",
      "Epoch 26/150 | Batch 3250/3450 | Loss: 0.0638\n",
      "Epoch 26/150 | Batch 3300/3450 | Loss: 0.0458\n",
      "Epoch 26/150 | Batch 3350/3450 | Loss: 0.0129\n",
      "Epoch 26/150 | Batch 3400/3450 | Loss: 0.0349\n",
      "Epoch 26/150 | Batch 3450/3450 | Loss: 0.0730\n",
      "\n",
      "Epoch 26/150 Summary:\n",
      "Time: 5017.62s | Total: 1 day, 12:21:04\n",
      "LR: 0.00019076\n",
      "Train Loss: 0.0327\n",
      "Val Loss: 0.0220\n",
      "PSNR: 30.31 | SSIM: 0.9185 | LPIPS: 1.9796\n",
      "Best PSNR so far: 32.43 at epoch 25\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 27/150 | Batch 50/3450 | Loss: 0.0281\n",
      "Epoch 27/150 | Batch 100/3450 | Loss: 0.0322\n",
      "Epoch 27/150 | Batch 150/3450 | Loss: 0.0183\n",
      "Epoch 27/150 | Batch 200/3450 | Loss: 0.0181\n",
      "Epoch 27/150 | Batch 250/3450 | Loss: 0.0635\n",
      "Epoch 27/150 | Batch 300/3450 | Loss: 0.0080\n",
      "Epoch 27/150 | Batch 350/3450 | Loss: 0.0241\n",
      "Epoch 27/150 | Batch 400/3450 | Loss: 0.0656\n",
      "Epoch 27/150 | Batch 450/3450 | Loss: 0.0127\n",
      "Epoch 27/150 | Batch 500/3450 | Loss: 0.0302\n",
      "Epoch 27/150 | Batch 550/3450 | Loss: 0.0487\n",
      "Epoch 27/150 | Batch 600/3450 | Loss: 0.0177\n",
      "Epoch 27/150 | Batch 650/3450 | Loss: 0.0257\n",
      "Epoch 27/150 | Batch 700/3450 | Loss: 0.0499\n",
      "Epoch 27/150 | Batch 750/3450 | Loss: 0.0828\n",
      "Epoch 27/150 | Batch 800/3450 | Loss: 0.0567\n",
      "Epoch 27/150 | Batch 850/3450 | Loss: 0.0100\n",
      "Epoch 27/150 | Batch 900/3450 | Loss: 0.0094\n",
      "Epoch 27/150 | Batch 950/3450 | Loss: 0.0331\n",
      "Epoch 27/150 | Batch 1000/3450 | Loss: 0.0201\n",
      "Epoch 27/150 | Batch 1050/3450 | Loss: 0.0332\n",
      "Epoch 27/150 | Batch 1100/3450 | Loss: 0.0597\n",
      "Epoch 27/150 | Batch 1150/3450 | Loss: 0.0351\n",
      "Epoch 27/150 | Batch 1200/3450 | Loss: 0.0085\n",
      "Epoch 27/150 | Batch 1250/3450 | Loss: 0.0250\n",
      "Epoch 27/150 | Batch 1300/3450 | Loss: 0.0515\n",
      "Epoch 27/150 | Batch 1350/3450 | Loss: 0.0134\n",
      "Epoch 27/150 | Batch 1400/3450 | Loss: 0.0382\n",
      "Epoch 27/150 | Batch 1450/3450 | Loss: 0.0157\n",
      "Epoch 27/150 | Batch 1500/3450 | Loss: 0.0186\n",
      "Epoch 27/150 | Batch 1550/3450 | Loss: 0.0229\n",
      "Epoch 27/150 | Batch 1600/3450 | Loss: 0.0115\n",
      "Epoch 27/150 | Batch 1650/3450 | Loss: 0.0777\n",
      "Epoch 27/150 | Batch 1700/3450 | Loss: 0.0219\n",
      "Epoch 27/150 | Batch 1750/3450 | Loss: 0.0150\n",
      "Epoch 27/150 | Batch 1800/3450 | Loss: 0.0810\n",
      "Epoch 27/150 | Batch 1850/3450 | Loss: 0.0662\n",
      "Epoch 27/150 | Batch 1900/3450 | Loss: 0.0679\n",
      "Epoch 27/150 | Batch 1950/3450 | Loss: 0.0290\n",
      "Epoch 27/150 | Batch 2000/3450 | Loss: 0.0339\n",
      "Epoch 27/150 | Batch 2050/3450 | Loss: 0.0402\n",
      "Epoch 27/150 | Batch 2100/3450 | Loss: 0.0191\n",
      "Epoch 27/150 | Batch 2150/3450 | Loss: 0.0417\n",
      "Epoch 27/150 | Batch 2200/3450 | Loss: 0.0131\n",
      "Epoch 27/150 | Batch 2250/3450 | Loss: 0.0179\n",
      "Epoch 27/150 | Batch 2300/3450 | Loss: 0.0356\n",
      "Epoch 27/150 | Batch 2350/3450 | Loss: 0.0143\n",
      "Epoch 27/150 | Batch 2400/3450 | Loss: 0.0446\n",
      "Epoch 27/150 | Batch 2450/3450 | Loss: 0.0096\n",
      "Epoch 27/150 | Batch 2500/3450 | Loss: 0.0525\n",
      "Epoch 27/150 | Batch 2550/3450 | Loss: 0.0081\n",
      "Epoch 27/150 | Batch 2600/3450 | Loss: 0.0475\n",
      "Epoch 27/150 | Batch 2650/3450 | Loss: 0.0197\n",
      "Epoch 27/150 | Batch 2700/3450 | Loss: 0.0114\n",
      "Epoch 27/150 | Batch 2750/3450 | Loss: 0.0622\n",
      "Epoch 27/150 | Batch 2800/3450 | Loss: 0.0244\n",
      "Epoch 27/150 | Batch 2850/3450 | Loss: 0.0160\n",
      "Epoch 27/150 | Batch 2900/3450 | Loss: 0.0293\n",
      "Epoch 27/150 | Batch 2950/3450 | Loss: 0.0132\n",
      "Epoch 27/150 | Batch 3000/3450 | Loss: 0.0680\n",
      "Epoch 27/150 | Batch 3050/3450 | Loss: 0.0155\n",
      "Epoch 27/150 | Batch 3100/3450 | Loss: 0.0418\n",
      "Epoch 27/150 | Batch 3150/3450 | Loss: 0.0580\n",
      "Epoch 27/150 | Batch 3200/3450 | Loss: 0.0138\n",
      "Epoch 27/150 | Batch 3250/3450 | Loss: 0.0301\n",
      "Epoch 27/150 | Batch 3300/3450 | Loss: 0.0641\n",
      "Epoch 27/150 | Batch 3350/3450 | Loss: 0.0323\n",
      "Epoch 27/150 | Batch 3400/3450 | Loss: 0.0112\n",
      "Epoch 27/150 | Batch 3450/3450 | Loss: 0.0244\n",
      "\n",
      "Epoch 27/150 Summary:\n",
      "Time: 5022.73s | Total: 1 day, 13:44:48\n",
      "LR: 0.00018983\n",
      "Train Loss: 0.0325\n",
      "Val Loss: 0.0167\n",
      "PSNR: 32.34 | SSIM: 0.9310 | LPIPS: 1.8468\n",
      "Best PSNR so far: 32.43 at epoch 25\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 28/150 | Batch 50/3450 | Loss: 0.0354\n",
      "Epoch 28/150 | Batch 100/3450 | Loss: 0.0312\n",
      "Epoch 28/150 | Batch 150/3450 | Loss: 0.0149\n",
      "Epoch 28/150 | Batch 200/3450 | Loss: 0.0270\n",
      "Epoch 28/150 | Batch 250/3450 | Loss: 0.0197\n",
      "Epoch 28/150 | Batch 300/3450 | Loss: 0.0320\n",
      "Epoch 28/150 | Batch 350/3450 | Loss: 0.0245\n",
      "Epoch 28/150 | Batch 400/3450 | Loss: 0.0513\n",
      "Epoch 28/150 | Batch 450/3450 | Loss: 0.0304\n",
      "Epoch 28/150 | Batch 500/3450 | Loss: 0.0126\n",
      "Epoch 28/150 | Batch 550/3450 | Loss: 0.0135\n",
      "Epoch 28/150 | Batch 600/3450 | Loss: 0.0126\n",
      "Epoch 28/150 | Batch 650/3450 | Loss: 0.0731\n",
      "Epoch 28/150 | Batch 700/3450 | Loss: 0.0315\n",
      "Epoch 28/150 | Batch 750/3450 | Loss: 0.0221\n",
      "Epoch 28/150 | Batch 800/3450 | Loss: 0.0143\n",
      "Epoch 28/150 | Batch 850/3450 | Loss: 0.0215\n",
      "Epoch 28/150 | Batch 900/3450 | Loss: 0.0186\n",
      "Epoch 28/150 | Batch 950/3450 | Loss: 0.0141\n",
      "Epoch 28/150 | Batch 1000/3450 | Loss: 0.0255\n",
      "Epoch 28/150 | Batch 1050/3450 | Loss: 0.0510\n",
      "Epoch 28/150 | Batch 1100/3450 | Loss: 0.0108\n",
      "Epoch 28/150 | Batch 1150/3450 | Loss: 0.0227\n",
      "Epoch 28/150 | Batch 1200/3450 | Loss: 0.0332\n",
      "Epoch 28/150 | Batch 1250/3450 | Loss: 0.0122\n",
      "Epoch 28/150 | Batch 1300/3450 | Loss: 0.0145\n",
      "Epoch 28/150 | Batch 1350/3450 | Loss: 0.0385\n",
      "Epoch 28/150 | Batch 1400/3450 | Loss: 0.0191\n",
      "Epoch 28/150 | Batch 1450/3450 | Loss: 0.0452\n",
      "Epoch 28/150 | Batch 1500/3450 | Loss: 0.0414\n",
      "Epoch 28/150 | Batch 1550/3450 | Loss: 0.0256\n",
      "Epoch 28/150 | Batch 1600/3450 | Loss: 0.0581\n",
      "Epoch 28/150 | Batch 1650/3450 | Loss: 0.0264\n",
      "Epoch 28/150 | Batch 1700/3450 | Loss: 0.0527\n",
      "Epoch 28/150 | Batch 1750/3450 | Loss: 0.0072\n",
      "Epoch 28/150 | Batch 1800/3450 | Loss: 0.0120\n",
      "Epoch 28/150 | Batch 1850/3450 | Loss: 0.0193\n",
      "Epoch 28/150 | Batch 1900/3450 | Loss: 0.0782\n",
      "Epoch 28/150 | Batch 1950/3450 | Loss: 0.0264\n",
      "Epoch 28/150 | Batch 2000/3450 | Loss: 0.0200\n",
      "Epoch 28/150 | Batch 2050/3450 | Loss: 0.0417\n",
      "Epoch 28/150 | Batch 2100/3450 | Loss: 0.0245\n",
      "Epoch 28/150 | Batch 2150/3450 | Loss: 0.0738\n",
      "Epoch 28/150 | Batch 2200/3450 | Loss: 0.0333\n",
      "Epoch 28/150 | Batch 2250/3450 | Loss: 0.0188\n",
      "Epoch 28/150 | Batch 2300/3450 | Loss: 0.0351\n",
      "Epoch 28/150 | Batch 2350/3450 | Loss: 0.0138\n",
      "Epoch 28/150 | Batch 2400/3450 | Loss: 0.0477\n",
      "Epoch 28/150 | Batch 2450/3450 | Loss: 0.0140\n",
      "Epoch 28/150 | Batch 2500/3450 | Loss: 0.0084\n",
      "Epoch 28/150 | Batch 2550/3450 | Loss: 0.0260\n",
      "Epoch 28/150 | Batch 2600/3450 | Loss: 0.0462\n",
      "Epoch 28/150 | Batch 2650/3450 | Loss: 0.0062\n",
      "Epoch 28/150 | Batch 2700/3450 | Loss: 0.0454\n",
      "Epoch 28/150 | Batch 2750/3450 | Loss: 0.0238\n",
      "Epoch 28/150 | Batch 2800/3450 | Loss: 0.0213\n",
      "Epoch 28/150 | Batch 2850/3450 | Loss: 0.0289\n",
      "Epoch 28/150 | Batch 2900/3450 | Loss: 0.0271\n",
      "Epoch 28/150 | Batch 2950/3450 | Loss: 0.0428\n",
      "Epoch 28/150 | Batch 3000/3450 | Loss: 0.0066\n",
      "Epoch 28/150 | Batch 3050/3450 | Loss: 0.0402\n",
      "Epoch 28/150 | Batch 3100/3450 | Loss: 0.0097\n",
      "Epoch 28/150 | Batch 3150/3450 | Loss: 0.0424\n",
      "Epoch 28/150 | Batch 3200/3450 | Loss: 0.0142\n",
      "Epoch 28/150 | Batch 3250/3450 | Loss: 0.0118\n",
      "Epoch 28/150 | Batch 3300/3450 | Loss: 0.0322\n",
      "Epoch 28/150 | Batch 3350/3450 | Loss: 0.0716\n",
      "Epoch 28/150 | Batch 3400/3450 | Loss: 0.0082\n",
      "Epoch 28/150 | Batch 3450/3450 | Loss: 0.0330\n",
      "\n",
      "Epoch 28/150 Summary:\n",
      "Time: 5018.55s | Total: 1 day, 15:08:29\n",
      "LR: 0.00018885\n",
      "Train Loss: 0.0321\n",
      "Val Loss: 0.0173\n",
      "PSNR: 32.21 | SSIM: 0.9320 | LPIPS: 1.8228\n",
      "Best PSNR so far: 32.43 at epoch 25\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 29/150 | Batch 50/3450 | Loss: 0.0303\n",
      "Epoch 29/150 | Batch 100/3450 | Loss: 0.0112\n",
      "Epoch 29/150 | Batch 150/3450 | Loss: 0.0170\n",
      "Epoch 29/150 | Batch 200/3450 | Loss: 0.0357\n",
      "Epoch 29/150 | Batch 250/3450 | Loss: 0.0149\n",
      "Epoch 29/150 | Batch 300/3450 | Loss: 0.0314\n",
      "Epoch 29/150 | Batch 350/3450 | Loss: 0.0502\n",
      "Epoch 29/150 | Batch 400/3450 | Loss: 0.0157\n",
      "Epoch 29/150 | Batch 450/3450 | Loss: 0.0310\n",
      "Epoch 29/150 | Batch 500/3450 | Loss: 0.0395\n",
      "Epoch 29/150 | Batch 550/3450 | Loss: 0.0814\n",
      "Epoch 29/150 | Batch 600/3450 | Loss: 0.0468\n",
      "Epoch 29/150 | Batch 650/3450 | Loss: 0.0395\n",
      "Epoch 29/150 | Batch 700/3450 | Loss: 0.0251\n",
      "Epoch 29/150 | Batch 750/3450 | Loss: 0.0626\n",
      "Epoch 29/150 | Batch 800/3450 | Loss: 0.0479\n",
      "Epoch 29/150 | Batch 850/3450 | Loss: 0.0512\n",
      "Epoch 29/150 | Batch 900/3450 | Loss: 0.0371\n",
      "Epoch 29/150 | Batch 950/3450 | Loss: 0.0472\n",
      "Epoch 29/150 | Batch 1000/3450 | Loss: 0.0204\n",
      "Epoch 29/150 | Batch 1050/3450 | Loss: 0.0207\n",
      "Epoch 29/150 | Batch 1100/3450 | Loss: 0.0513\n",
      "Epoch 29/150 | Batch 1150/3450 | Loss: 0.0063\n",
      "Epoch 29/150 | Batch 1200/3450 | Loss: 0.0295\n",
      "Epoch 29/150 | Batch 1250/3450 | Loss: 0.0333\n",
      "Epoch 29/150 | Batch 1300/3450 | Loss: 0.0446\n",
      "Epoch 29/150 | Batch 1350/3450 | Loss: 0.0150\n",
      "Epoch 29/150 | Batch 1400/3450 | Loss: 0.0101\n",
      "Epoch 29/150 | Batch 1450/3450 | Loss: 0.0072\n",
      "Epoch 29/150 | Batch 1500/3450 | Loss: 0.0459\n",
      "Epoch 29/150 | Batch 1550/3450 | Loss: 0.0279\n",
      "Epoch 29/150 | Batch 1600/3450 | Loss: 0.0572\n",
      "Epoch 29/150 | Batch 1650/3450 | Loss: 0.0630\n",
      "Epoch 29/150 | Batch 1700/3450 | Loss: 0.0107\n",
      "Epoch 29/150 | Batch 1750/3450 | Loss: 0.0305\n",
      "Epoch 29/150 | Batch 1800/3450 | Loss: 0.0178\n",
      "Epoch 29/150 | Batch 1850/3450 | Loss: 0.0158\n",
      "Epoch 29/150 | Batch 1900/3450 | Loss: 0.0117\n",
      "Epoch 29/150 | Batch 1950/3450 | Loss: 0.0275\n",
      "Epoch 29/150 | Batch 2000/3450 | Loss: 0.0212\n",
      "Epoch 29/150 | Batch 2050/3450 | Loss: 0.0309\n",
      "Epoch 29/150 | Batch 2100/3450 | Loss: 0.0693\n",
      "Epoch 29/150 | Batch 2150/3450 | Loss: 0.0129\n",
      "Epoch 29/150 | Batch 2200/3450 | Loss: 0.0594\n",
      "Epoch 29/150 | Batch 2250/3450 | Loss: 0.0328\n",
      "Epoch 29/150 | Batch 2300/3450 | Loss: 0.0255\n",
      "Epoch 29/150 | Batch 2350/3450 | Loss: 0.0094\n",
      "Epoch 29/150 | Batch 2400/3450 | Loss: 0.0417\n",
      "Epoch 29/150 | Batch 2450/3450 | Loss: 0.0369\n",
      "Epoch 29/150 | Batch 2500/3450 | Loss: 0.0321\n",
      "Epoch 29/150 | Batch 2550/3450 | Loss: 0.0192\n",
      "Epoch 29/150 | Batch 2600/3450 | Loss: 0.0433\n",
      "Epoch 29/150 | Batch 2650/3450 | Loss: 0.0246\n",
      "Epoch 29/150 | Batch 2700/3450 | Loss: 0.0413\n",
      "Epoch 29/150 | Batch 2750/3450 | Loss: 0.0096\n",
      "Epoch 29/150 | Batch 2800/3450 | Loss: 0.0162\n",
      "Epoch 29/150 | Batch 2850/3450 | Loss: 0.0124\n",
      "Epoch 29/150 | Batch 2900/3450 | Loss: 0.0752\n",
      "Epoch 29/150 | Batch 2950/3450 | Loss: 0.0752\n",
      "Epoch 29/150 | Batch 3000/3450 | Loss: 0.0182\n",
      "Epoch 29/150 | Batch 3050/3450 | Loss: 0.0525\n",
      "Epoch 29/150 | Batch 3100/3450 | Loss: 0.0538\n",
      "Epoch 29/150 | Batch 3150/3450 | Loss: 0.0298\n",
      "Epoch 29/150 | Batch 3200/3450 | Loss: 0.0318\n",
      "Epoch 29/150 | Batch 3250/3450 | Loss: 0.0232\n",
      "Epoch 29/150 | Batch 3300/3450 | Loss: 0.0326\n",
      "Epoch 29/150 | Batch 3350/3450 | Loss: 0.0154\n",
      "Epoch 29/150 | Batch 3400/3450 | Loss: 0.0189\n",
      "Epoch 29/150 | Batch 3450/3450 | Loss: 0.0339\n",
      "\n",
      "Epoch 29/150 Summary:\n",
      "Time: 5023.93s | Total: 1 day, 16:32:14\n",
      "LR: 0.00018784\n",
      "Train Loss: 0.0322\n",
      "Val Loss: 0.0181\n",
      "PSNR: 31.72 | SSIM: 0.9282 | LPIPS: 1.8799\n",
      "Best PSNR so far: 32.43 at epoch 25\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 30/150 | Batch 50/3450 | Loss: 0.0358\n",
      "Epoch 30/150 | Batch 100/3450 | Loss: 0.0143\n",
      "Epoch 30/150 | Batch 150/3450 | Loss: 0.0129\n",
      "Epoch 30/150 | Batch 200/3450 | Loss: 0.0377\n",
      "Epoch 30/150 | Batch 250/3450 | Loss: 0.0163\n",
      "Epoch 30/150 | Batch 300/3450 | Loss: 0.0088\n",
      "Epoch 30/150 | Batch 350/3450 | Loss: 0.0117\n",
      "Epoch 30/150 | Batch 400/3450 | Loss: 0.0172\n",
      "Epoch 30/150 | Batch 450/3450 | Loss: 0.0477\n",
      "Epoch 30/150 | Batch 500/3450 | Loss: 0.0542\n",
      "Epoch 30/150 | Batch 550/3450 | Loss: 0.0355\n",
      "Epoch 30/150 | Batch 600/3450 | Loss: 0.0367\n",
      "Epoch 30/150 | Batch 650/3450 | Loss: 0.0428\n",
      "Epoch 30/150 | Batch 700/3450 | Loss: 0.0316\n",
      "Epoch 30/150 | Batch 750/3450 | Loss: 0.0297\n",
      "Epoch 30/150 | Batch 800/3450 | Loss: 0.0145\n",
      "Epoch 30/150 | Batch 850/3450 | Loss: 0.0115\n",
      "Epoch 30/150 | Batch 900/3450 | Loss: 0.0085\n",
      "Epoch 30/150 | Batch 950/3450 | Loss: 0.0270\n",
      "Epoch 30/150 | Batch 1000/3450 | Loss: 0.0415\n",
      "Epoch 30/150 | Batch 1050/3450 | Loss: 0.0197\n",
      "Epoch 30/150 | Batch 1100/3450 | Loss: 0.0114\n",
      "Epoch 30/150 | Batch 1150/3450 | Loss: 0.0587\n",
      "Epoch 30/150 | Batch 1200/3450 | Loss: 0.0183\n",
      "Epoch 30/150 | Batch 1250/3450 | Loss: 0.0656\n",
      "Epoch 30/150 | Batch 1300/3450 | Loss: 0.0451\n",
      "Epoch 30/150 | Batch 1350/3450 | Loss: 0.0258\n",
      "Epoch 30/150 | Batch 1400/3450 | Loss: 0.0193\n",
      "Epoch 30/150 | Batch 1450/3450 | Loss: 0.0145\n",
      "Epoch 30/150 | Batch 1500/3450 | Loss: 0.0254\n",
      "Epoch 30/150 | Batch 1550/3450 | Loss: 0.0455\n",
      "Epoch 30/150 | Batch 1600/3450 | Loss: 0.0131\n",
      "Epoch 30/150 | Batch 1650/3450 | Loss: 0.0306\n",
      "Epoch 30/150 | Batch 1700/3450 | Loss: 0.0145\n",
      "Epoch 30/150 | Batch 1750/3450 | Loss: 0.0208\n",
      "Epoch 30/150 | Batch 1800/3450 | Loss: 0.0325\n",
      "Epoch 30/150 | Batch 1850/3450 | Loss: 0.0312\n",
      "Epoch 30/150 | Batch 1900/3450 | Loss: 0.0209\n",
      "Epoch 30/150 | Batch 1950/3450 | Loss: 0.0387\n",
      "Epoch 30/150 | Batch 2000/3450 | Loss: 0.0238\n",
      "Epoch 30/150 | Batch 2050/3450 | Loss: 0.0185\n",
      "Epoch 30/150 | Batch 2100/3450 | Loss: 0.0275\n",
      "Epoch 30/150 | Batch 2150/3450 | Loss: 0.0369\n",
      "Epoch 30/150 | Batch 2200/3450 | Loss: 0.0488\n",
      "Epoch 30/150 | Batch 2250/3450 | Loss: 0.0153\n",
      "Epoch 30/150 | Batch 2300/3450 | Loss: 0.0207\n",
      "Epoch 30/150 | Batch 2350/3450 | Loss: 0.0452\n",
      "Epoch 30/150 | Batch 2400/3450 | Loss: 0.0254\n",
      "Epoch 30/150 | Batch 2450/3450 | Loss: 0.0631\n",
      "Epoch 30/150 | Batch 2500/3450 | Loss: 0.0683\n",
      "Epoch 30/150 | Batch 2550/3450 | Loss: 0.0202\n",
      "Epoch 30/150 | Batch 2600/3450 | Loss: 0.0339\n",
      "Epoch 30/150 | Batch 2650/3450 | Loss: 0.0253\n",
      "Epoch 30/150 | Batch 2700/3450 | Loss: 0.0262\n",
      "Epoch 30/150 | Batch 2750/3450 | Loss: 0.0160\n",
      "Epoch 30/150 | Batch 2800/3450 | Loss: 0.0164\n",
      "Epoch 30/150 | Batch 2850/3450 | Loss: 0.0207\n",
      "Epoch 30/150 | Batch 2900/3450 | Loss: 0.0320\n",
      "Epoch 30/150 | Batch 2950/3450 | Loss: 0.0251\n",
      "Epoch 30/150 | Batch 3000/3450 | Loss: 0.0185\n",
      "Epoch 30/150 | Batch 3050/3450 | Loss: 0.0154\n",
      "Epoch 30/150 | Batch 3100/3450 | Loss: 0.0186\n",
      "Epoch 30/150 | Batch 3150/3450 | Loss: 0.0199\n",
      "Epoch 30/150 | Batch 3200/3450 | Loss: 0.0511\n",
      "Epoch 30/150 | Batch 3250/3450 | Loss: 0.0161\n",
      "Epoch 30/150 | Batch 3300/3450 | Loss: 0.0685\n",
      "Epoch 30/150 | Batch 3350/3450 | Loss: 0.0138\n",
      "Epoch 30/150 | Batch 3400/3450 | Loss: 0.0655\n",
      "Epoch 30/150 | Batch 3450/3450 | Loss: 0.0249\n",
      "\n",
      "Epoch 30/150 Summary:\n",
      "Time: 5023.55s | Total: 1 day, 17:56:00\n",
      "LR: 0.00018678\n",
      "Train Loss: 0.0325\n",
      "Val Loss: 0.0170\n",
      "PSNR: 32.24 | SSIM: 0.9316 | LPIPS: 1.8454\n",
      "Best PSNR so far: 32.43 at epoch 25\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 31/150 | Batch 50/3450 | Loss: 0.0252\n",
      "Epoch 31/150 | Batch 100/3450 | Loss: 0.0120\n",
      "Epoch 31/150 | Batch 150/3450 | Loss: 0.0113\n",
      "Epoch 31/150 | Batch 200/3450 | Loss: 0.0258\n",
      "Epoch 31/150 | Batch 250/3450 | Loss: 0.0177\n",
      "Epoch 31/150 | Batch 300/3450 | Loss: 0.0260\n",
      "Epoch 31/150 | Batch 350/3450 | Loss: 0.0506\n",
      "Epoch 31/150 | Batch 400/3450 | Loss: 0.0257\n",
      "Epoch 31/150 | Batch 450/3450 | Loss: 0.0195\n",
      "Epoch 31/150 | Batch 500/3450 | Loss: 0.0257\n",
      "Epoch 31/150 | Batch 550/3450 | Loss: 0.0669\n",
      "Epoch 31/150 | Batch 600/3450 | Loss: 0.0447\n",
      "Epoch 31/150 | Batch 650/3450 | Loss: 0.0245\n",
      "Epoch 31/150 | Batch 700/3450 | Loss: 0.0238\n",
      "Epoch 31/150 | Batch 750/3450 | Loss: 0.0335\n",
      "Epoch 31/150 | Batch 800/3450 | Loss: 0.0119\n",
      "Epoch 31/150 | Batch 850/3450 | Loss: 0.0105\n",
      "Epoch 31/150 | Batch 900/3450 | Loss: 0.0278\n",
      "Epoch 31/150 | Batch 950/3450 | Loss: 0.0151\n",
      "Epoch 31/150 | Batch 1000/3450 | Loss: 0.0154\n",
      "Epoch 31/150 | Batch 1050/3450 | Loss: 0.0190\n",
      "Epoch 31/150 | Batch 1100/3450 | Loss: 0.0546\n",
      "Epoch 31/150 | Batch 1150/3450 | Loss: 0.0192\n",
      "Epoch 31/150 | Batch 1200/3450 | Loss: 0.0123\n",
      "Epoch 31/150 | Batch 1250/3450 | Loss: 0.0558\n",
      "Epoch 31/150 | Batch 1300/3450 | Loss: 0.0282\n",
      "Epoch 31/150 | Batch 1350/3450 | Loss: 0.0196\n",
      "Epoch 31/150 | Batch 1400/3450 | Loss: 0.0665\n",
      "Epoch 31/150 | Batch 1450/3450 | Loss: 0.0090\n",
      "Epoch 31/150 | Batch 1500/3450 | Loss: 0.0215\n",
      "Epoch 31/150 | Batch 1550/3450 | Loss: 0.0188\n",
      "Epoch 31/150 | Batch 1600/3450 | Loss: 0.0477\n",
      "Epoch 31/150 | Batch 1650/3450 | Loss: 0.0143\n",
      "Epoch 31/150 | Batch 1700/3450 | Loss: 0.0172\n",
      "Epoch 31/150 | Batch 1750/3450 | Loss: 0.0325\n",
      "Epoch 31/150 | Batch 1800/3450 | Loss: 0.0541\n",
      "Epoch 31/150 | Batch 1850/3450 | Loss: 0.0414\n",
      "Epoch 31/150 | Batch 1900/3450 | Loss: 0.0559\n",
      "Epoch 31/150 | Batch 1950/3450 | Loss: 0.0111\n",
      "Epoch 31/150 | Batch 2000/3450 | Loss: 0.0535\n",
      "Epoch 31/150 | Batch 2050/3450 | Loss: 0.0113\n",
      "Epoch 31/150 | Batch 2100/3450 | Loss: 0.0061\n",
      "Epoch 31/150 | Batch 2150/3450 | Loss: 0.0978\n",
      "Epoch 31/150 | Batch 2200/3450 | Loss: 0.0345\n",
      "Epoch 31/150 | Batch 2250/3450 | Loss: 0.0590\n",
      "Epoch 31/150 | Batch 2300/3450 | Loss: 0.0124\n",
      "Epoch 31/150 | Batch 2350/3450 | Loss: 0.0525\n",
      "Epoch 31/150 | Batch 2400/3450 | Loss: 0.0120\n",
      "Epoch 31/150 | Batch 2450/3450 | Loss: 0.0248\n",
      "Epoch 31/150 | Batch 2500/3450 | Loss: 0.0300\n",
      "Epoch 31/150 | Batch 2550/3450 | Loss: 0.0168\n",
      "Epoch 31/150 | Batch 2600/3450 | Loss: 0.0719\n",
      "Epoch 31/150 | Batch 2650/3450 | Loss: 0.0358\n",
      "Epoch 31/150 | Batch 2700/3450 | Loss: 0.0382\n",
      "Epoch 31/150 | Batch 2750/3450 | Loss: 0.0520\n",
      "Epoch 31/150 | Batch 2800/3450 | Loss: 0.0143\n",
      "Epoch 31/150 | Batch 2850/3450 | Loss: 0.0580\n",
      "Epoch 31/150 | Batch 2900/3450 | Loss: 0.0420\n",
      "Epoch 31/150 | Batch 2950/3450 | Loss: 0.0181\n",
      "Epoch 31/150 | Batch 3000/3450 | Loss: 0.0278\n",
      "Epoch 31/150 | Batch 3050/3450 | Loss: 0.0322\n",
      "Epoch 31/150 | Batch 3100/3450 | Loss: 0.0399\n",
      "Epoch 31/150 | Batch 3150/3450 | Loss: 0.0250\n",
      "Epoch 31/150 | Batch 3200/3450 | Loss: 0.0304\n",
      "Epoch 31/150 | Batch 3250/3450 | Loss: 0.0128\n",
      "Epoch 31/150 | Batch 3300/3450 | Loss: 0.0143\n",
      "Epoch 31/150 | Batch 3350/3450 | Loss: 0.0337\n",
      "Epoch 31/150 | Batch 3400/3450 | Loss: 0.0180\n",
      "Epoch 31/150 | Batch 3450/3450 | Loss: 0.0135\n",
      "\n",
      "Epoch 31/150 Summary:\n",
      "Time: 5023.68s | Total: 1 day, 19:19:45\n",
      "LR: 0.00018569\n",
      "Train Loss: 0.0319\n",
      "Val Loss: 0.0165\n",
      "PSNR: 32.56 | SSIM: 0.9329 | LPIPS: 1.8001\n",
      "[INFO] New best model saved with PSNR: 32.56\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 32/150 | Batch 50/3450 | Loss: 0.0122\n",
      "Epoch 32/150 | Batch 100/3450 | Loss: 0.0193\n",
      "Epoch 32/150 | Batch 150/3450 | Loss: 0.0255\n",
      "Epoch 32/150 | Batch 200/3450 | Loss: 0.0289\n",
      "Epoch 32/150 | Batch 250/3450 | Loss: 0.0207\n",
      "Epoch 32/150 | Batch 300/3450 | Loss: 0.0119\n",
      "Epoch 32/150 | Batch 350/3450 | Loss: 0.0093\n",
      "Epoch 32/150 | Batch 400/3450 | Loss: 0.0134\n",
      "Epoch 32/150 | Batch 450/3450 | Loss: 0.0792\n",
      "Epoch 32/150 | Batch 500/3450 | Loss: 0.0142\n",
      "Epoch 32/150 | Batch 550/3450 | Loss: 0.0495\n",
      "Epoch 32/150 | Batch 600/3450 | Loss: 0.0234\n",
      "Epoch 32/150 | Batch 650/3450 | Loss: 0.0653\n",
      "Epoch 32/150 | Batch 700/3450 | Loss: 0.0152\n",
      "Epoch 32/150 | Batch 750/3450 | Loss: 0.0678\n",
      "Epoch 32/150 | Batch 800/3450 | Loss: 0.0367\n",
      "Epoch 32/150 | Batch 850/3450 | Loss: 0.0188\n",
      "Epoch 32/150 | Batch 900/3450 | Loss: 0.0257\n",
      "Epoch 32/150 | Batch 950/3450 | Loss: 0.0382\n",
      "Epoch 32/150 | Batch 1000/3450 | Loss: 0.0124\n",
      "Epoch 32/150 | Batch 1050/3450 | Loss: 0.0145\n",
      "Epoch 32/150 | Batch 1100/3450 | Loss: 0.0549\n",
      "Epoch 32/150 | Batch 1150/3450 | Loss: 0.0098\n",
      "Epoch 32/150 | Batch 1200/3450 | Loss: 0.0250\n",
      "Epoch 32/150 | Batch 1250/3450 | Loss: 0.0502\n",
      "Epoch 32/150 | Batch 1300/3450 | Loss: 0.0537\n",
      "Epoch 32/150 | Batch 1350/3450 | Loss: 0.0327\n",
      "Epoch 32/150 | Batch 1400/3450 | Loss: 0.0740\n",
      "Epoch 32/150 | Batch 1450/3450 | Loss: 0.0160\n",
      "Epoch 32/150 | Batch 1500/3450 | Loss: 0.0452\n",
      "Epoch 32/150 | Batch 1550/3450 | Loss: 0.0224\n",
      "Epoch 32/150 | Batch 1600/3450 | Loss: 0.0119\n",
      "Epoch 32/150 | Batch 1650/3450 | Loss: 0.0505\n",
      "Epoch 32/150 | Batch 1700/3450 | Loss: 0.0166\n",
      "Epoch 32/150 | Batch 1750/3450 | Loss: 0.0061\n",
      "Epoch 32/150 | Batch 1800/3450 | Loss: 0.0561\n",
      "Epoch 32/150 | Batch 1850/3450 | Loss: 0.0674\n",
      "Epoch 32/150 | Batch 1900/3450 | Loss: 0.0163\n",
      "Epoch 32/150 | Batch 1950/3450 | Loss: 0.0090\n",
      "Epoch 32/150 | Batch 2000/3450 | Loss: 0.0460\n",
      "Epoch 32/150 | Batch 2050/3450 | Loss: 0.0158\n",
      "Epoch 32/150 | Batch 2100/3450 | Loss: 0.0314\n",
      "Epoch 32/150 | Batch 2150/3450 | Loss: 0.0348\n",
      "Epoch 32/150 | Batch 2200/3450 | Loss: 0.0214\n",
      "Epoch 32/150 | Batch 2250/3450 | Loss: 0.0289\n",
      "Epoch 32/150 | Batch 2300/3450 | Loss: 0.0283\n",
      "Epoch 32/150 | Batch 2350/3450 | Loss: 0.0157\n",
      "Epoch 32/150 | Batch 2400/3450 | Loss: 0.0337\n",
      "Epoch 32/150 | Batch 2450/3450 | Loss: 0.0185\n",
      "Epoch 32/150 | Batch 2500/3450 | Loss: 0.0381\n",
      "Epoch 32/150 | Batch 2550/3450 | Loss: 0.0226\n",
      "Epoch 32/150 | Batch 2600/3450 | Loss: 0.0298\n",
      "Epoch 32/150 | Batch 2650/3450 | Loss: 0.0178\n",
      "Epoch 32/150 | Batch 2700/3450 | Loss: 0.0295\n",
      "Epoch 32/150 | Batch 2750/3450 | Loss: 0.0272\n",
      "Epoch 32/150 | Batch 2800/3450 | Loss: 0.0299\n",
      "Epoch 32/150 | Batch 2850/3450 | Loss: 0.0490\n",
      "Epoch 32/150 | Batch 2900/3450 | Loss: 0.0156\n",
      "Epoch 32/150 | Batch 2950/3450 | Loss: 0.0729\n",
      "Epoch 32/150 | Batch 3000/3450 | Loss: 0.0131\n",
      "Epoch 32/150 | Batch 3050/3450 | Loss: 0.0100\n",
      "Epoch 32/150 | Batch 3100/3450 | Loss: 0.0174\n",
      "Epoch 32/150 | Batch 3150/3450 | Loss: 0.0353\n",
      "Epoch 32/150 | Batch 3200/3450 | Loss: 0.0205\n",
      "Epoch 32/150 | Batch 3250/3450 | Loss: 0.0307\n",
      "Epoch 32/150 | Batch 3300/3450 | Loss: 0.0474\n",
      "Epoch 32/150 | Batch 3350/3450 | Loss: 0.0477\n",
      "Epoch 32/150 | Batch 3400/3450 | Loss: 0.0109\n",
      "Epoch 32/150 | Batch 3450/3450 | Loss: 0.0166\n",
      "\n",
      "Epoch 32/150 Summary:\n",
      "Time: 5016.60s | Total: 1 day, 20:43:24\n",
      "LR: 0.00018455\n",
      "Train Loss: 0.0322\n",
      "Val Loss: 0.0212\n",
      "PSNR: 30.75 | SSIM: 0.9239 | LPIPS: 1.8827\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 33/150 | Batch 50/3450 | Loss: 0.0582\n",
      "Epoch 33/150 | Batch 100/3450 | Loss: 0.0260\n",
      "Epoch 33/150 | Batch 150/3450 | Loss: 0.0149\n",
      "Epoch 33/150 | Batch 200/3450 | Loss: 0.0477\n",
      "Epoch 33/150 | Batch 250/3450 | Loss: 0.0792\n",
      "Epoch 33/150 | Batch 300/3450 | Loss: 0.0352\n",
      "Epoch 33/150 | Batch 350/3450 | Loss: 0.0210\n",
      "Epoch 33/150 | Batch 400/3450 | Loss: 0.0227\n",
      "Epoch 33/150 | Batch 450/3450 | Loss: 0.0470\n",
      "Epoch 33/150 | Batch 500/3450 | Loss: 0.0271\n",
      "Epoch 33/150 | Batch 550/3450 | Loss: 0.0185\n",
      "Epoch 33/150 | Batch 600/3450 | Loss: 0.0182\n",
      "Epoch 33/150 | Batch 650/3450 | Loss: 0.0367\n",
      "Epoch 33/150 | Batch 700/3450 | Loss: 0.0381\n",
      "Epoch 33/150 | Batch 750/3450 | Loss: 0.0205\n",
      "Epoch 33/150 | Batch 800/3450 | Loss: 0.0212\n",
      "Epoch 33/150 | Batch 850/3450 | Loss: 0.0333\n",
      "Epoch 33/150 | Batch 900/3450 | Loss: 0.0111\n",
      "Epoch 33/150 | Batch 950/3450 | Loss: 0.0873\n",
      "Epoch 33/150 | Batch 1000/3450 | Loss: 0.0808\n",
      "Epoch 33/150 | Batch 1050/3450 | Loss: 0.0285\n",
      "Epoch 33/150 | Batch 1100/3450 | Loss: 0.0664\n",
      "Epoch 33/150 | Batch 1150/3450 | Loss: 0.0169\n",
      "Epoch 33/150 | Batch 1200/3450 | Loss: 0.0215\n",
      "Epoch 33/150 | Batch 1250/3450 | Loss: 0.0110\n",
      "Epoch 33/150 | Batch 1300/3450 | Loss: 0.0376\n",
      "Epoch 33/150 | Batch 1350/3450 | Loss: 0.0149\n",
      "Epoch 33/150 | Batch 1400/3450 | Loss: 0.0105\n",
      "Epoch 33/150 | Batch 1450/3450 | Loss: 0.0195\n",
      "Epoch 33/150 | Batch 1500/3450 | Loss: 0.0239\n",
      "Epoch 33/150 | Batch 1550/3450 | Loss: 0.0154\n",
      "Epoch 33/150 | Batch 1600/3450 | Loss: 0.0108\n",
      "Epoch 33/150 | Batch 1650/3450 | Loss: 0.0123\n",
      "Epoch 33/150 | Batch 1700/3450 | Loss: 0.0163\n",
      "Epoch 33/150 | Batch 1750/3450 | Loss: 0.0382\n",
      "Epoch 33/150 | Batch 1800/3450 | Loss: 0.0212\n",
      "Epoch 33/150 | Batch 1850/3450 | Loss: 0.0325\n",
      "Epoch 33/150 | Batch 1900/3450 | Loss: 0.0254\n",
      "Epoch 33/150 | Batch 1950/3450 | Loss: 0.0760\n",
      "Epoch 33/150 | Batch 2000/3450 | Loss: 0.0210\n",
      "Epoch 33/150 | Batch 2050/3450 | Loss: 0.0289\n",
      "Epoch 33/150 | Batch 2100/3450 | Loss: 0.0235\n",
      "Epoch 33/150 | Batch 2150/3450 | Loss: 0.0538\n",
      "Epoch 33/150 | Batch 2200/3450 | Loss: 0.0513\n",
      "Epoch 33/150 | Batch 2250/3450 | Loss: 0.0206\n",
      "Epoch 33/150 | Batch 2300/3450 | Loss: 0.0288\n",
      "Epoch 33/150 | Batch 2350/3450 | Loss: 0.0136\n",
      "Epoch 33/150 | Batch 2400/3450 | Loss: 0.0233\n",
      "Epoch 33/150 | Batch 2450/3450 | Loss: 0.0407\n",
      "Epoch 33/150 | Batch 2500/3450 | Loss: 0.0243\n",
      "Epoch 33/150 | Batch 2550/3450 | Loss: 0.0328\n",
      "Epoch 33/150 | Batch 2600/3450 | Loss: 0.0132\n",
      "Epoch 33/150 | Batch 2650/3450 | Loss: 0.0200\n",
      "Epoch 33/150 | Batch 2700/3450 | Loss: 0.0556\n",
      "Epoch 33/150 | Batch 2750/3450 | Loss: 0.0557\n",
      "Epoch 33/150 | Batch 2800/3450 | Loss: 0.0193\n",
      "Epoch 33/150 | Batch 2850/3450 | Loss: 0.0469\n",
      "Epoch 33/150 | Batch 2900/3450 | Loss: 0.0375\n",
      "Epoch 33/150 | Batch 2950/3450 | Loss: 0.0154\n",
      "Epoch 33/150 | Batch 3000/3450 | Loss: 0.0488\n",
      "Epoch 33/150 | Batch 3050/3450 | Loss: 0.0264\n",
      "Epoch 33/150 | Batch 3100/3450 | Loss: 0.0522\n",
      "Epoch 33/150 | Batch 3150/3450 | Loss: 0.0215\n",
      "Epoch 33/150 | Batch 3200/3450 | Loss: 0.0134\n",
      "Epoch 33/150 | Batch 3250/3450 | Loss: 0.0207\n",
      "Epoch 33/150 | Batch 3300/3450 | Loss: 0.0427\n",
      "Epoch 33/150 | Batch 3350/3450 | Loss: 0.0142\n",
      "Epoch 33/150 | Batch 3400/3450 | Loss: 0.0349\n",
      "Epoch 33/150 | Batch 3450/3450 | Loss: 0.0208\n",
      "\n",
      "Epoch 33/150 Summary:\n",
      "Time: 5022.43s | Total: 1 day, 22:07:09\n",
      "LR: 0.00018337\n",
      "Train Loss: 0.0320\n",
      "Val Loss: 0.0187\n",
      "PSNR: 31.69 | SSIM: 0.9324 | LPIPS: 1.8254\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 34/150 | Batch 50/3450 | Loss: 0.0708\n",
      "Epoch 34/150 | Batch 100/3450 | Loss: 0.0422\n",
      "Epoch 34/150 | Batch 150/3450 | Loss: 0.0495\n",
      "Epoch 34/150 | Batch 200/3450 | Loss: 0.0628\n",
      "Epoch 34/150 | Batch 250/3450 | Loss: 0.0657\n",
      "Epoch 34/150 | Batch 300/3450 | Loss: 0.0336\n",
      "Epoch 34/150 | Batch 350/3450 | Loss: 0.0211\n",
      "Epoch 34/150 | Batch 400/3450 | Loss: 0.0416\n",
      "Epoch 34/150 | Batch 450/3450 | Loss: 0.0444\n",
      "Epoch 34/150 | Batch 500/3450 | Loss: 0.0390\n",
      "Epoch 34/150 | Batch 550/3450 | Loss: 0.0233\n",
      "Epoch 34/150 | Batch 600/3450 | Loss: 0.0194\n",
      "Epoch 34/150 | Batch 650/3450 | Loss: 0.0336\n",
      "Epoch 34/150 | Batch 700/3450 | Loss: 0.0299\n",
      "Epoch 34/150 | Batch 750/3450 | Loss: 0.0141\n",
      "Epoch 34/150 | Batch 800/3450 | Loss: 0.0149\n",
      "Epoch 34/150 | Batch 850/3450 | Loss: 0.0127\n",
      "Epoch 34/150 | Batch 900/3450 | Loss: 0.0390\n",
      "Epoch 34/150 | Batch 950/3450 | Loss: 0.0559\n",
      "Epoch 34/150 | Batch 1000/3450 | Loss: 0.0148\n",
      "Epoch 34/150 | Batch 1050/3450 | Loss: 0.0055\n",
      "Epoch 34/150 | Batch 1100/3450 | Loss: 0.0462\n",
      "Epoch 34/150 | Batch 1150/3450 | Loss: 0.0098\n",
      "Epoch 34/150 | Batch 1200/3450 | Loss: 0.0412\n",
      "Epoch 34/150 | Batch 1250/3450 | Loss: 0.0136\n",
      "Epoch 34/150 | Batch 1300/3450 | Loss: 0.0660\n",
      "Epoch 34/150 | Batch 1350/3450 | Loss: 0.0246\n",
      "Epoch 34/150 | Batch 1400/3450 | Loss: 0.0234\n",
      "Epoch 34/150 | Batch 1450/3450 | Loss: 0.0133\n",
      "Epoch 34/150 | Batch 1500/3450 | Loss: 0.0642\n",
      "Epoch 34/150 | Batch 1550/3450 | Loss: 0.0261\n",
      "Epoch 34/150 | Batch 1600/3450 | Loss: 0.0188\n",
      "Epoch 34/150 | Batch 1650/3450 | Loss: 0.0109\n",
      "Epoch 34/150 | Batch 1700/3450 | Loss: 0.0335\n",
      "Epoch 34/150 | Batch 1750/3450 | Loss: 0.0265\n",
      "Epoch 34/150 | Batch 1800/3450 | Loss: 0.0237\n",
      "Epoch 34/150 | Batch 1850/3450 | Loss: 0.0236\n",
      "Epoch 34/150 | Batch 1900/3450 | Loss: 0.0220\n",
      "Epoch 34/150 | Batch 1950/3450 | Loss: 0.0519\n",
      "Epoch 34/150 | Batch 2000/3450 | Loss: 0.0906\n",
      "Epoch 34/150 | Batch 2050/3450 | Loss: 0.0496\n",
      "Epoch 34/150 | Batch 2100/3450 | Loss: 0.0121\n",
      "Epoch 34/150 | Batch 2150/3450 | Loss: 0.0184\n",
      "Epoch 34/150 | Batch 2200/3450 | Loss: 0.0402\n",
      "Epoch 34/150 | Batch 2250/3450 | Loss: 0.0158\n",
      "Epoch 34/150 | Batch 2300/3450 | Loss: 0.0527\n",
      "Epoch 34/150 | Batch 2350/3450 | Loss: 0.0165\n",
      "Epoch 34/150 | Batch 2400/3450 | Loss: 0.0386\n",
      "Epoch 34/150 | Batch 2450/3450 | Loss: 0.0166\n",
      "Epoch 34/150 | Batch 2500/3450 | Loss: 0.0321\n",
      "Epoch 34/150 | Batch 2550/3450 | Loss: 0.0717\n",
      "Epoch 34/150 | Batch 2600/3450 | Loss: 0.0215\n",
      "Epoch 34/150 | Batch 2650/3450 | Loss: 0.0654\n",
      "Epoch 34/150 | Batch 2700/3450 | Loss: 0.0332\n",
      "Epoch 34/150 | Batch 2750/3450 | Loss: 0.0440\n",
      "Epoch 34/150 | Batch 2800/3450 | Loss: 0.0168\n",
      "Epoch 34/150 | Batch 2850/3450 | Loss: 0.0158\n",
      "Epoch 34/150 | Batch 2900/3450 | Loss: 0.0243\n",
      "Epoch 34/150 | Batch 2950/3450 | Loss: 0.0159\n",
      "Epoch 34/150 | Batch 3000/3450 | Loss: 0.0296\n",
      "Epoch 34/150 | Batch 3050/3450 | Loss: 0.0076\n",
      "Epoch 34/150 | Batch 3100/3450 | Loss: 0.0381\n",
      "Epoch 34/150 | Batch 3150/3450 | Loss: 0.0326\n",
      "Epoch 34/150 | Batch 3200/3450 | Loss: 0.0158\n",
      "Epoch 34/150 | Batch 3250/3450 | Loss: 0.0547\n",
      "Epoch 34/150 | Batch 3300/3450 | Loss: 0.0134\n",
      "Epoch 34/150 | Batch 3350/3450 | Loss: 0.0325\n",
      "Epoch 34/150 | Batch 3400/3450 | Loss: 0.0093\n",
      "Epoch 34/150 | Batch 3450/3450 | Loss: 0.0456\n",
      "\n",
      "Epoch 34/150 Summary:\n",
      "Time: 5020.65s | Total: 1 day, 23:30:51\n",
      "LR: 0.00018216\n",
      "Train Loss: 0.0320\n",
      "Val Loss: 0.0189\n",
      "PSNR: 31.29 | SSIM: 0.9225 | LPIPS: 1.9241\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 35/150 | Batch 50/3450 | Loss: 0.0237\n",
      "Epoch 35/150 | Batch 100/3450 | Loss: 0.0292\n",
      "Epoch 35/150 | Batch 150/3450 | Loss: 0.0203\n",
      "Epoch 35/150 | Batch 200/3450 | Loss: 0.0804\n",
      "Epoch 35/150 | Batch 250/3450 | Loss: 0.0493\n",
      "Epoch 35/150 | Batch 300/3450 | Loss: 0.0292\n",
      "Epoch 35/150 | Batch 350/3450 | Loss: 0.0331\n",
      "Epoch 35/150 | Batch 400/3450 | Loss: 0.0120\n",
      "Epoch 35/150 | Batch 450/3450 | Loss: 0.0434\n",
      "Epoch 35/150 | Batch 500/3450 | Loss: 0.0436\n",
      "Epoch 35/150 | Batch 550/3450 | Loss: 0.0605\n",
      "Epoch 35/150 | Batch 600/3450 | Loss: 0.0160\n",
      "Epoch 35/150 | Batch 650/3450 | Loss: 0.0674\n",
      "Epoch 35/150 | Batch 700/3450 | Loss: 0.0093\n",
      "Epoch 35/150 | Batch 750/3450 | Loss: 0.0352\n",
      "Epoch 35/150 | Batch 800/3450 | Loss: 0.0321\n",
      "Epoch 35/150 | Batch 850/3450 | Loss: 0.0765\n",
      "Epoch 35/150 | Batch 900/3450 | Loss: 0.0676\n",
      "Epoch 35/150 | Batch 950/3450 | Loss: 0.0340\n",
      "Epoch 35/150 | Batch 1000/3450 | Loss: 0.0390\n",
      "Epoch 35/150 | Batch 1050/3450 | Loss: 0.0171\n",
      "Epoch 35/150 | Batch 1100/3450 | Loss: 0.0412\n",
      "Epoch 35/150 | Batch 1150/3450 | Loss: 0.0382\n",
      "Epoch 35/150 | Batch 1200/3450 | Loss: 0.0130\n",
      "Epoch 35/150 | Batch 1250/3450 | Loss: 0.0388\n",
      "Epoch 35/150 | Batch 1300/3450 | Loss: 0.0308\n",
      "Epoch 35/150 | Batch 1350/3450 | Loss: 0.0155\n",
      "Epoch 35/150 | Batch 1400/3450 | Loss: 0.0360\n",
      "Epoch 35/150 | Batch 1450/3450 | Loss: 0.0062\n",
      "Epoch 35/150 | Batch 1500/3450 | Loss: 0.0735\n",
      "Epoch 35/150 | Batch 1550/3450 | Loss: 0.0192\n",
      "Epoch 35/150 | Batch 1600/3450 | Loss: 0.0350\n",
      "Epoch 35/150 | Batch 1650/3450 | Loss: 0.0508\n",
      "Epoch 35/150 | Batch 1700/3450 | Loss: 0.0732\n",
      "Epoch 35/150 | Batch 1750/3450 | Loss: 0.0840\n",
      "Epoch 35/150 | Batch 1800/3450 | Loss: 0.0947\n",
      "Epoch 35/150 | Batch 1850/3450 | Loss: 0.0352\n",
      "Epoch 35/150 | Batch 1900/3450 | Loss: 0.0463\n",
      "Epoch 35/150 | Batch 1950/3450 | Loss: 0.0325\n",
      "Epoch 35/150 | Batch 2000/3450 | Loss: 0.0137\n",
      "Epoch 35/150 | Batch 2050/3450 | Loss: 0.0439\n",
      "Epoch 35/150 | Batch 2100/3450 | Loss: 0.0612\n",
      "Epoch 35/150 | Batch 2150/3450 | Loss: 0.0388\n",
      "Epoch 35/150 | Batch 2200/3450 | Loss: 0.0767\n",
      "Epoch 35/150 | Batch 2250/3450 | Loss: 0.0284\n",
      "Epoch 35/150 | Batch 2300/3450 | Loss: 0.0524\n",
      "Epoch 35/150 | Batch 2350/3450 | Loss: 0.0641\n",
      "Epoch 35/150 | Batch 2400/3450 | Loss: 0.0333\n",
      "Epoch 35/150 | Batch 2450/3450 | Loss: 0.0098\n",
      "Epoch 35/150 | Batch 2500/3450 | Loss: 0.0085\n",
      "Epoch 35/150 | Batch 2550/3450 | Loss: 0.0254\n",
      "Epoch 35/150 | Batch 2600/3450 | Loss: 0.0211\n",
      "Epoch 35/150 | Batch 2650/3450 | Loss: 0.0098\n",
      "Epoch 35/150 | Batch 2700/3450 | Loss: 0.0425\n",
      "Epoch 35/150 | Batch 2750/3450 | Loss: 0.0272\n",
      "Epoch 35/150 | Batch 2800/3450 | Loss: 0.0403\n",
      "Epoch 35/150 | Batch 2850/3450 | Loss: 0.0259\n",
      "Epoch 35/150 | Batch 2900/3450 | Loss: 0.0456\n",
      "Epoch 35/150 | Batch 2950/3450 | Loss: 0.0726\n",
      "Epoch 35/150 | Batch 3000/3450 | Loss: 0.0268\n",
      "Epoch 35/150 | Batch 3050/3450 | Loss: 0.0600\n",
      "Epoch 35/150 | Batch 3100/3450 | Loss: 0.0183\n",
      "Epoch 35/150 | Batch 3150/3450 | Loss: 0.0155\n",
      "Epoch 35/150 | Batch 3200/3450 | Loss: 0.0151\n",
      "Epoch 35/150 | Batch 3250/3450 | Loss: 0.0656\n",
      "Epoch 35/150 | Batch 3300/3450 | Loss: 0.0105\n",
      "Epoch 35/150 | Batch 3350/3450 | Loss: 0.0429\n",
      "Epoch 35/150 | Batch 3400/3450 | Loss: 0.0252\n",
      "Epoch 35/150 | Batch 3450/3450 | Loss: 0.0198\n",
      "\n",
      "Epoch 35/150 Summary:\n",
      "Time: 5024.37s | Total: 2 days, 0:54:37\n",
      "LR: 0.00018090\n",
      "Train Loss: 0.0322\n",
      "Val Loss: 0.0185\n",
      "PSNR: 31.47 | SSIM: 0.9269 | LPIPS: 1.8868\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 36/150 | Batch 50/3450 | Loss: 0.0448\n",
      "Epoch 36/150 | Batch 100/3450 | Loss: 0.0412\n",
      "Epoch 36/150 | Batch 150/3450 | Loss: 0.0144\n",
      "Epoch 36/150 | Batch 200/3450 | Loss: 0.0562\n",
      "Epoch 36/150 | Batch 250/3450 | Loss: 0.0455\n",
      "Epoch 36/150 | Batch 300/3450 | Loss: 0.0243\n",
      "Epoch 36/150 | Batch 350/3450 | Loss: 0.0524\n",
      "Epoch 36/150 | Batch 400/3450 | Loss: 0.0256\n",
      "Epoch 36/150 | Batch 450/3450 | Loss: 0.0209\n",
      "Epoch 36/150 | Batch 500/3450 | Loss: 0.0422\n",
      "Epoch 36/150 | Batch 550/3450 | Loss: 0.0294\n",
      "Epoch 36/150 | Batch 600/3450 | Loss: 0.0106\n",
      "Epoch 36/150 | Batch 650/3450 | Loss: 0.0261\n",
      "Epoch 36/150 | Batch 700/3450 | Loss: 0.0243\n",
      "Epoch 36/150 | Batch 750/3450 | Loss: 0.0144\n",
      "Epoch 36/150 | Batch 800/3450 | Loss: 0.0469\n",
      "Epoch 36/150 | Batch 850/3450 | Loss: 0.0154\n",
      "Epoch 36/150 | Batch 900/3450 | Loss: 0.0151\n",
      "Epoch 36/150 | Batch 950/3450 | Loss: 0.0367\n",
      "Epoch 36/150 | Batch 1000/3450 | Loss: 0.0149\n",
      "Epoch 36/150 | Batch 1050/3450 | Loss: 0.0735\n",
      "Epoch 36/150 | Batch 1100/3450 | Loss: 0.0230\n",
      "Epoch 36/150 | Batch 1150/3450 | Loss: 0.0316\n",
      "Epoch 36/150 | Batch 1200/3450 | Loss: 0.0594\n",
      "Epoch 36/150 | Batch 1250/3450 | Loss: 0.0345\n",
      "Epoch 36/150 | Batch 1300/3450 | Loss: 0.0698\n",
      "Epoch 36/150 | Batch 1350/3450 | Loss: 0.0156\n",
      "Epoch 36/150 | Batch 1400/3450 | Loss: 0.0213\n",
      "Epoch 36/150 | Batch 1450/3450 | Loss: 0.0103\n",
      "Epoch 36/150 | Batch 1500/3450 | Loss: 0.0262\n",
      "Epoch 36/150 | Batch 1550/3450 | Loss: 0.0393\n",
      "Epoch 36/150 | Batch 1600/3450 | Loss: 0.0125\n",
      "Epoch 36/150 | Batch 1650/3450 | Loss: 0.0519\n",
      "Epoch 36/150 | Batch 1700/3450 | Loss: 0.0280\n",
      "Epoch 36/150 | Batch 1750/3450 | Loss: 0.0289\n",
      "Epoch 36/150 | Batch 1800/3450 | Loss: 0.0298\n",
      "Epoch 36/150 | Batch 1850/3450 | Loss: 0.0168\n",
      "Epoch 36/150 | Batch 1900/3450 | Loss: 0.0113\n",
      "Epoch 36/150 | Batch 1950/3450 | Loss: 0.0173\n",
      "Epoch 36/150 | Batch 2000/3450 | Loss: 0.0314\n",
      "Epoch 36/150 | Batch 2050/3450 | Loss: 0.0320\n",
      "Epoch 36/150 | Batch 2100/3450 | Loss: 0.0544\n",
      "Epoch 36/150 | Batch 2150/3450 | Loss: 0.0544\n",
      "Epoch 36/150 | Batch 2200/3450 | Loss: 0.0187\n",
      "Epoch 36/150 | Batch 2250/3450 | Loss: 0.0138\n",
      "Epoch 36/150 | Batch 2300/3450 | Loss: 0.0292\n",
      "Epoch 36/150 | Batch 2350/3450 | Loss: 0.0197\n",
      "Epoch 36/150 | Batch 2400/3450 | Loss: 0.0327\n",
      "Epoch 36/150 | Batch 2450/3450 | Loss: 0.0688\n",
      "Epoch 36/150 | Batch 2500/3450 | Loss: 0.0596\n",
      "Epoch 36/150 | Batch 2550/3450 | Loss: 0.0214\n",
      "Epoch 36/150 | Batch 2600/3450 | Loss: 0.0233\n",
      "Epoch 36/150 | Batch 2650/3450 | Loss: 0.0246\n",
      "Epoch 36/150 | Batch 2700/3450 | Loss: 0.0089\n",
      "Epoch 36/150 | Batch 2750/3450 | Loss: 0.0310\n",
      "Epoch 36/150 | Batch 2800/3450 | Loss: 0.0160\n",
      "Epoch 36/150 | Batch 2850/3450 | Loss: 0.0147\n",
      "Epoch 36/150 | Batch 2900/3450 | Loss: 0.0173\n",
      "Epoch 36/150 | Batch 2950/3450 | Loss: 0.0433\n",
      "Epoch 36/150 | Batch 3000/3450 | Loss: 0.0275\n",
      "Epoch 36/150 | Batch 3050/3450 | Loss: 0.0341\n",
      "Epoch 36/150 | Batch 3100/3450 | Loss: 0.0285\n",
      "Epoch 36/150 | Batch 3150/3450 | Loss: 0.0193\n",
      "Epoch 36/150 | Batch 3200/3450 | Loss: 0.0655\n",
      "Epoch 36/150 | Batch 3250/3450 | Loss: 0.0571\n",
      "Epoch 36/150 | Batch 3300/3450 | Loss: 0.0247\n",
      "Epoch 36/150 | Batch 3350/3450 | Loss: 0.0066\n",
      "Epoch 36/150 | Batch 3400/3450 | Loss: 0.0412\n",
      "Epoch 36/150 | Batch 3450/3450 | Loss: 0.0488\n",
      "\n",
      "Epoch 36/150 Summary:\n",
      "Time: 5017.95s | Total: 2 days, 2:18:17\n",
      "LR: 0.00017961\n",
      "Train Loss: 0.0321\n",
      "Val Loss: 0.0200\n",
      "PSNR: 31.51 | SSIM: 0.9347 | LPIPS: 1.7645\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 37/150 | Batch 50/3450 | Loss: 0.0257\n",
      "Epoch 37/150 | Batch 100/3450 | Loss: 0.0657\n",
      "Epoch 37/150 | Batch 150/3450 | Loss: 0.0574\n",
      "Epoch 37/150 | Batch 200/3450 | Loss: 0.0181\n",
      "Epoch 37/150 | Batch 250/3450 | Loss: 0.0108\n",
      "Epoch 37/150 | Batch 300/3450 | Loss: 0.0529\n",
      "Epoch 37/150 | Batch 350/3450 | Loss: 0.0411\n",
      "Epoch 37/150 | Batch 400/3450 | Loss: 0.0676\n",
      "Epoch 37/150 | Batch 450/3450 | Loss: 0.0580\n",
      "Epoch 37/150 | Batch 500/3450 | Loss: 0.0365\n",
      "Epoch 37/150 | Batch 550/3450 | Loss: 0.0482\n",
      "Epoch 37/150 | Batch 600/3450 | Loss: 0.0218\n",
      "Epoch 37/150 | Batch 650/3450 | Loss: 0.0235\n",
      "Epoch 37/150 | Batch 700/3450 | Loss: 0.0702\n",
      "Epoch 37/150 | Batch 750/3450 | Loss: 0.0614\n",
      "Epoch 37/150 | Batch 800/3450 | Loss: 0.0545\n",
      "Epoch 37/150 | Batch 850/3450 | Loss: 0.0163\n",
      "Epoch 37/150 | Batch 900/3450 | Loss: 0.0230\n",
      "Epoch 37/150 | Batch 950/3450 | Loss: 0.0488\n",
      "Epoch 37/150 | Batch 1000/3450 | Loss: 0.0075\n",
      "Epoch 37/150 | Batch 1050/3450 | Loss: 0.0381\n",
      "Epoch 37/150 | Batch 1100/3450 | Loss: 0.0207\n",
      "Epoch 37/150 | Batch 1150/3450 | Loss: 0.0239\n",
      "Epoch 37/150 | Batch 1200/3450 | Loss: 0.0351\n",
      "Epoch 37/150 | Batch 1250/3450 | Loss: 0.0191\n",
      "Epoch 37/150 | Batch 1300/3450 | Loss: 0.0461\n",
      "Epoch 37/150 | Batch 1350/3450 | Loss: 0.0443\n",
      "Epoch 37/150 | Batch 1400/3450 | Loss: 0.0425\n",
      "Epoch 37/150 | Batch 1450/3450 | Loss: 0.0248\n",
      "Epoch 37/150 | Batch 1500/3450 | Loss: 0.0749\n",
      "Epoch 37/150 | Batch 1550/3450 | Loss: 0.0410\n",
      "Epoch 37/150 | Batch 1600/3450 | Loss: 0.0276\n",
      "Epoch 37/150 | Batch 1650/3450 | Loss: 0.0126\n",
      "Epoch 37/150 | Batch 1700/3450 | Loss: 0.0556\n",
      "Epoch 37/150 | Batch 1750/3450 | Loss: 0.0173\n",
      "Epoch 37/150 | Batch 1800/3450 | Loss: 0.0177\n",
      "Epoch 37/150 | Batch 1850/3450 | Loss: 0.0432\n",
      "Epoch 37/150 | Batch 1900/3450 | Loss: 0.0074\n",
      "Epoch 37/150 | Batch 1950/3450 | Loss: 0.0279\n",
      "Epoch 37/150 | Batch 2000/3450 | Loss: 0.0316\n",
      "Epoch 37/150 | Batch 2050/3450 | Loss: 0.0854\n",
      "Epoch 37/150 | Batch 2100/3450 | Loss: 0.0266\n",
      "Epoch 37/150 | Batch 2150/3450 | Loss: 0.0491\n",
      "Epoch 37/150 | Batch 2200/3450 | Loss: 0.0395\n",
      "Epoch 37/150 | Batch 2250/3450 | Loss: 0.0640\n",
      "Epoch 37/150 | Batch 2300/3450 | Loss: 0.0338\n",
      "Epoch 37/150 | Batch 2350/3450 | Loss: 0.0149\n",
      "Epoch 37/150 | Batch 2400/3450 | Loss: 0.0163\n",
      "Epoch 37/150 | Batch 2450/3450 | Loss: 0.0374\n",
      "Epoch 37/150 | Batch 2500/3450 | Loss: 0.0422\n",
      "Epoch 37/150 | Batch 2550/3450 | Loss: 0.0195\n",
      "Epoch 37/150 | Batch 2600/3450 | Loss: 0.0366\n",
      "Epoch 37/150 | Batch 2650/3450 | Loss: 0.0682\n",
      "Epoch 37/150 | Batch 2700/3450 | Loss: 0.0490\n",
      "Epoch 37/150 | Batch 2750/3450 | Loss: 0.0443\n",
      "Epoch 37/150 | Batch 2800/3450 | Loss: 0.0368\n",
      "Epoch 37/150 | Batch 2850/3450 | Loss: 0.0136\n",
      "Epoch 37/150 | Batch 2900/3450 | Loss: 0.0211\n",
      "Epoch 37/150 | Batch 2950/3450 | Loss: 0.0296\n",
      "Epoch 37/150 | Batch 3000/3450 | Loss: 0.0098\n",
      "Epoch 37/150 | Batch 3050/3450 | Loss: 0.0270\n",
      "Epoch 37/150 | Batch 3100/3450 | Loss: 0.0719\n",
      "Epoch 37/150 | Batch 3150/3450 | Loss: 0.0428\n",
      "Epoch 37/150 | Batch 3200/3450 | Loss: 0.0395\n",
      "Epoch 37/150 | Batch 3250/3450 | Loss: 0.0693\n",
      "Epoch 37/150 | Batch 3300/3450 | Loss: 0.0398\n",
      "Epoch 37/150 | Batch 3350/3450 | Loss: 0.0133\n",
      "Epoch 37/150 | Batch 3400/3450 | Loss: 0.0136\n",
      "Epoch 37/150 | Batch 3450/3450 | Loss: 0.0401\n",
      "\n",
      "Epoch 37/150 Summary:\n",
      "Time: 5022.52s | Total: 2 days, 3:42:02\n",
      "LR: 0.00017828\n",
      "Train Loss: 0.0321\n",
      "Val Loss: 0.0165\n",
      "PSNR: 32.49 | SSIM: 0.9346 | LPIPS: 1.7856\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 38/150 | Batch 50/3450 | Loss: 0.0292\n",
      "Epoch 38/150 | Batch 100/3450 | Loss: 0.0116\n",
      "Epoch 38/150 | Batch 150/3450 | Loss: 0.0160\n",
      "Epoch 38/150 | Batch 200/3450 | Loss: 0.0141\n",
      "Epoch 38/150 | Batch 250/3450 | Loss: 0.0087\n",
      "Epoch 38/150 | Batch 300/3450 | Loss: 0.0221\n",
      "Epoch 38/150 | Batch 350/3450 | Loss: 0.0361\n",
      "Epoch 38/150 | Batch 400/3450 | Loss: 0.0223\n",
      "Epoch 38/150 | Batch 450/3450 | Loss: 0.0393\n",
      "Epoch 38/150 | Batch 500/3450 | Loss: 0.0214\n",
      "Epoch 38/150 | Batch 550/3450 | Loss: 0.0077\n",
      "Epoch 38/150 | Batch 600/3450 | Loss: 0.0232\n",
      "Epoch 38/150 | Batch 650/3450 | Loss: 0.0177\n",
      "Epoch 38/150 | Batch 700/3450 | Loss: 0.0549\n",
      "Epoch 38/150 | Batch 750/3450 | Loss: 0.0375\n",
      "Epoch 38/150 | Batch 800/3450 | Loss: 0.0142\n",
      "Epoch 38/150 | Batch 850/3450 | Loss: 0.0347\n",
      "Epoch 38/150 | Batch 900/3450 | Loss: 0.0563\n",
      "Epoch 38/150 | Batch 950/3450 | Loss: 0.0186\n",
      "Epoch 38/150 | Batch 1000/3450 | Loss: 0.0134\n",
      "Epoch 38/150 | Batch 1050/3450 | Loss: 0.0540\n",
      "Epoch 38/150 | Batch 1100/3450 | Loss: 0.0107\n",
      "Epoch 38/150 | Batch 1150/3450 | Loss: 0.0141\n",
      "Epoch 38/150 | Batch 1200/3450 | Loss: 0.0997\n",
      "Epoch 38/150 | Batch 1250/3450 | Loss: 0.0649\n",
      "Epoch 38/150 | Batch 1300/3450 | Loss: 0.0181\n",
      "Epoch 38/150 | Batch 1350/3450 | Loss: 0.0482\n",
      "Epoch 38/150 | Batch 1400/3450 | Loss: 0.0089\n",
      "Epoch 38/150 | Batch 1450/3450 | Loss: 0.0660\n",
      "Epoch 38/150 | Batch 1500/3450 | Loss: 0.0179\n",
      "Epoch 38/150 | Batch 1550/3450 | Loss: 0.0164\n",
      "Epoch 38/150 | Batch 1600/3450 | Loss: 0.0118\n",
      "Epoch 38/150 | Batch 1650/3450 | Loss: 0.0091\n",
      "Epoch 38/150 | Batch 1700/3450 | Loss: 0.0107\n",
      "Epoch 38/150 | Batch 1750/3450 | Loss: 0.0256\n",
      "Epoch 38/150 | Batch 1800/3450 | Loss: 0.0199\n",
      "Epoch 38/150 | Batch 1850/3450 | Loss: 0.0299\n",
      "Epoch 38/150 | Batch 1900/3450 | Loss: 0.0102\n",
      "Epoch 38/150 | Batch 1950/3450 | Loss: 0.0194\n",
      "Epoch 38/150 | Batch 2000/3450 | Loss: 0.0399\n",
      "Epoch 38/150 | Batch 2050/3450 | Loss: 0.0142\n",
      "Epoch 38/150 | Batch 2100/3450 | Loss: 0.0111\n",
      "Epoch 38/150 | Batch 2150/3450 | Loss: 0.0143\n",
      "Epoch 38/150 | Batch 2200/3450 | Loss: 0.0167\n",
      "Epoch 38/150 | Batch 2250/3450 | Loss: 0.0226\n",
      "Epoch 38/150 | Batch 2300/3450 | Loss: 0.0478\n",
      "Epoch 38/150 | Batch 2350/3450 | Loss: 0.0173\n",
      "Epoch 38/150 | Batch 2400/3450 | Loss: 0.0240\n",
      "Epoch 38/150 | Batch 2450/3450 | Loss: 0.0319\n",
      "Epoch 38/150 | Batch 2500/3450 | Loss: 0.0400\n",
      "Epoch 38/150 | Batch 2550/3450 | Loss: 0.0745\n",
      "Epoch 38/150 | Batch 2600/3450 | Loss: 0.0181\n",
      "Epoch 38/150 | Batch 2650/3450 | Loss: 0.0168\n",
      "Epoch 38/150 | Batch 2700/3450 | Loss: 0.0438\n",
      "Epoch 38/150 | Batch 2750/3450 | Loss: 0.0131\n",
      "Epoch 38/150 | Batch 2800/3450 | Loss: 0.0356\n",
      "Epoch 38/150 | Batch 2850/3450 | Loss: 0.0172\n",
      "Epoch 38/150 | Batch 2900/3450 | Loss: 0.0206\n",
      "Epoch 38/150 | Batch 2950/3450 | Loss: 0.0419\n",
      "Epoch 38/150 | Batch 3000/3450 | Loss: 0.0150\n",
      "Epoch 38/150 | Batch 3050/3450 | Loss: 0.0099\n",
      "Epoch 38/150 | Batch 3100/3450 | Loss: 0.0434\n",
      "Epoch 38/150 | Batch 3150/3450 | Loss: 0.0109\n",
      "Epoch 38/150 | Batch 3200/3450 | Loss: 0.0515\n",
      "Epoch 38/150 | Batch 3250/3450 | Loss: 0.0221\n",
      "Epoch 38/150 | Batch 3300/3450 | Loss: 0.0725\n",
      "Epoch 38/150 | Batch 3350/3450 | Loss: 0.0428\n",
      "Epoch 38/150 | Batch 3400/3450 | Loss: 0.0187\n",
      "Epoch 38/150 | Batch 3450/3450 | Loss: 0.0222\n",
      "\n",
      "Epoch 38/150 Summary:\n",
      "Time: 5024.27s | Total: 2 days, 5:05:48\n",
      "LR: 0.00017691\n",
      "Train Loss: 0.0321\n",
      "Val Loss: 0.0219\n",
      "PSNR: 30.69 | SSIM: 0.9310 | LPIPS: 1.8313\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 39/150 | Batch 50/3450 | Loss: 0.0171\n",
      "Epoch 39/150 | Batch 100/3450 | Loss: 0.0311\n",
      "Epoch 39/150 | Batch 150/3450 | Loss: 0.0681\n",
      "Epoch 39/150 | Batch 200/3450 | Loss: 0.0167\n",
      "Epoch 39/150 | Batch 250/3450 | Loss: 0.0306\n",
      "Epoch 39/150 | Batch 300/3450 | Loss: 0.0225\n",
      "Epoch 39/150 | Batch 350/3450 | Loss: 0.0502\n",
      "Epoch 39/150 | Batch 400/3450 | Loss: 0.0388\n",
      "Epoch 39/150 | Batch 450/3450 | Loss: 0.0131\n",
      "Epoch 39/150 | Batch 500/3450 | Loss: 0.0159\n",
      "Epoch 39/150 | Batch 550/3450 | Loss: 0.0165\n",
      "Epoch 39/150 | Batch 600/3450 | Loss: 0.0621\n",
      "Epoch 39/150 | Batch 650/3450 | Loss: 0.0335\n",
      "Epoch 39/150 | Batch 700/3450 | Loss: 0.0243\n",
      "Epoch 39/150 | Batch 750/3450 | Loss: 0.0471\n",
      "Epoch 39/150 | Batch 800/3450 | Loss: 0.0227\n",
      "Epoch 39/150 | Batch 850/3450 | Loss: 0.0099\n",
      "Epoch 39/150 | Batch 900/3450 | Loss: 0.0194\n",
      "Epoch 39/150 | Batch 950/3450 | Loss: 0.0520\n",
      "Epoch 39/150 | Batch 1000/3450 | Loss: 0.0162\n",
      "Epoch 39/150 | Batch 1050/3450 | Loss: 0.0330\n",
      "Epoch 39/150 | Batch 1100/3450 | Loss: 0.0506\n",
      "Epoch 39/150 | Batch 1150/3450 | Loss: 0.0178\n",
      "Epoch 39/150 | Batch 1200/3450 | Loss: 0.0199\n",
      "Epoch 39/150 | Batch 1250/3450 | Loss: 0.0815\n",
      "Epoch 39/150 | Batch 1300/3450 | Loss: 0.0231\n",
      "Epoch 39/150 | Batch 1350/3450 | Loss: 0.0522\n",
      "Epoch 39/150 | Batch 1400/3450 | Loss: 0.0278\n",
      "Epoch 39/150 | Batch 1450/3450 | Loss: 0.0566\n",
      "Epoch 39/150 | Batch 1500/3450 | Loss: 0.0249\n",
      "Epoch 39/150 | Batch 1550/3450 | Loss: 0.0140\n",
      "Epoch 39/150 | Batch 1600/3450 | Loss: 0.0416\n",
      "Epoch 39/150 | Batch 1650/3450 | Loss: 0.0142\n",
      "Epoch 39/150 | Batch 1700/3450 | Loss: 0.0510\n",
      "Epoch 39/150 | Batch 1750/3450 | Loss: 0.0221\n",
      "Epoch 39/150 | Batch 1800/3450 | Loss: 0.0172\n",
      "Epoch 39/150 | Batch 1850/3450 | Loss: 0.0750\n",
      "Epoch 39/150 | Batch 1900/3450 | Loss: 0.0272\n",
      "Epoch 39/150 | Batch 1950/3450 | Loss: 0.0331\n",
      "Epoch 39/150 | Batch 2000/3450 | Loss: 0.0452\n",
      "Epoch 39/150 | Batch 2050/3450 | Loss: 0.0329\n",
      "Epoch 39/150 | Batch 2100/3450 | Loss: 0.0334\n",
      "Epoch 39/150 | Batch 2150/3450 | Loss: 0.0254\n",
      "Epoch 39/150 | Batch 2200/3450 | Loss: 0.0572\n",
      "Epoch 39/150 | Batch 2250/3450 | Loss: 0.0221\n",
      "Epoch 39/150 | Batch 2300/3450 | Loss: 0.0191\n",
      "Epoch 39/150 | Batch 2350/3450 | Loss: 0.0549\n",
      "Epoch 39/150 | Batch 2400/3450 | Loss: 0.0184\n",
      "Epoch 39/150 | Batch 2450/3450 | Loss: 0.0324\n",
      "Epoch 39/150 | Batch 2500/3450 | Loss: 0.0515\n",
      "Epoch 39/150 | Batch 2550/3450 | Loss: 0.0171\n",
      "Epoch 39/150 | Batch 2600/3450 | Loss: 0.0133\n",
      "Epoch 39/150 | Batch 2650/3450 | Loss: 0.0399\n",
      "Epoch 39/150 | Batch 2700/3450 | Loss: 0.0270\n",
      "Epoch 39/150 | Batch 2750/3450 | Loss: 0.0196\n",
      "Epoch 39/150 | Batch 2800/3450 | Loss: 0.0143\n",
      "Epoch 39/150 | Batch 2850/3450 | Loss: 0.0118\n",
      "Epoch 39/150 | Batch 2900/3450 | Loss: 0.0156\n",
      "Epoch 39/150 | Batch 2950/3450 | Loss: 0.0167\n",
      "Epoch 39/150 | Batch 3000/3450 | Loss: 0.0347\n",
      "Epoch 39/150 | Batch 3050/3450 | Loss: 0.0370\n",
      "Epoch 39/150 | Batch 3100/3450 | Loss: 0.0161\n",
      "Epoch 39/150 | Batch 3150/3450 | Loss: 0.0396\n",
      "Epoch 39/150 | Batch 3200/3450 | Loss: 0.0175\n",
      "Epoch 39/150 | Batch 3250/3450 | Loss: 0.0652\n",
      "Epoch 39/150 | Batch 3300/3450 | Loss: 0.0199\n",
      "Epoch 39/150 | Batch 3350/3450 | Loss: 0.0420\n",
      "Epoch 39/150 | Batch 3400/3450 | Loss: 0.0308\n",
      "Epoch 39/150 | Batch 3450/3450 | Loss: 0.0477\n",
      "\n",
      "Epoch 39/150 Summary:\n",
      "Time: 5019.68s | Total: 2 days, 6:29:29\n",
      "LR: 0.00017551\n",
      "Train Loss: 0.0319\n",
      "Val Loss: 0.0179\n",
      "PSNR: 31.59 | SSIM: 0.9281 | LPIPS: 1.8591\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 40/150 | Batch 50/3450 | Loss: 0.0688\n",
      "Epoch 40/150 | Batch 100/3450 | Loss: 0.0097\n",
      "Epoch 40/150 | Batch 150/3450 | Loss: 0.0183\n",
      "Epoch 40/150 | Batch 200/3450 | Loss: 0.0317\n",
      "Epoch 40/150 | Batch 250/3450 | Loss: 0.0513\n",
      "Epoch 40/150 | Batch 300/3450 | Loss: 0.0707\n",
      "Epoch 40/150 | Batch 350/3450 | Loss: 0.0113\n",
      "Epoch 40/150 | Batch 400/3450 | Loss: 0.0213\n",
      "Epoch 40/150 | Batch 450/3450 | Loss: 0.0212\n",
      "Epoch 40/150 | Batch 500/3450 | Loss: 0.0066\n",
      "Epoch 40/150 | Batch 550/3450 | Loss: 0.0136\n",
      "Epoch 40/150 | Batch 600/3450 | Loss: 0.0121\n",
      "Epoch 40/150 | Batch 650/3450 | Loss: 0.0150\n",
      "Epoch 40/150 | Batch 700/3450 | Loss: 0.0413\n",
      "Epoch 40/150 | Batch 750/3450 | Loss: 0.0168\n",
      "Epoch 40/150 | Batch 800/3450 | Loss: 0.0374\n",
      "Epoch 40/150 | Batch 850/3450 | Loss: 0.0323\n",
      "Epoch 40/150 | Batch 900/3450 | Loss: 0.0093\n",
      "Epoch 40/150 | Batch 950/3450 | Loss: 0.0426\n",
      "Epoch 40/150 | Batch 1000/3450 | Loss: 0.0212\n",
      "Epoch 40/150 | Batch 1050/3450 | Loss: 0.0057\n",
      "Epoch 40/150 | Batch 1100/3450 | Loss: 0.0161\n",
      "Epoch 40/150 | Batch 1150/3450 | Loss: 0.0334\n",
      "Epoch 40/150 | Batch 1200/3450 | Loss: 0.0230\n",
      "Epoch 40/150 | Batch 1250/3450 | Loss: 0.0274\n",
      "Epoch 40/150 | Batch 1300/3450 | Loss: 0.0519\n",
      "Epoch 40/150 | Batch 1350/3450 | Loss: 0.0436\n",
      "Epoch 40/150 | Batch 1400/3450 | Loss: 0.0205\n",
      "Epoch 40/150 | Batch 1450/3450 | Loss: 0.0150\n",
      "Epoch 40/150 | Batch 1500/3450 | Loss: 0.0368\n",
      "Epoch 40/150 | Batch 1550/3450 | Loss: 0.0312\n",
      "Epoch 40/150 | Batch 1600/3450 | Loss: 0.0578\n",
      "Epoch 40/150 | Batch 1650/3450 | Loss: 0.0130\n",
      "Epoch 40/150 | Batch 1700/3450 | Loss: 0.0194\n",
      "Epoch 40/150 | Batch 1750/3450 | Loss: 0.0185\n",
      "Epoch 40/150 | Batch 1800/3450 | Loss: 0.0662\n",
      "Epoch 40/150 | Batch 1850/3450 | Loss: 0.0203\n",
      "Epoch 40/150 | Batch 1900/3450 | Loss: 0.0145\n",
      "Epoch 40/150 | Batch 1950/3450 | Loss: 0.0239\n",
      "Epoch 40/150 | Batch 2000/3450 | Loss: 0.0220\n",
      "Epoch 40/150 | Batch 2050/3450 | Loss: 0.0273\n",
      "Epoch 40/150 | Batch 2100/3450 | Loss: 0.0145\n",
      "Epoch 40/150 | Batch 2150/3450 | Loss: 0.0135\n",
      "Epoch 40/150 | Batch 2200/3450 | Loss: 0.0647\n",
      "Epoch 40/150 | Batch 2250/3450 | Loss: 0.0204\n",
      "Epoch 40/150 | Batch 2300/3450 | Loss: 0.0489\n",
      "Epoch 40/150 | Batch 2350/3450 | Loss: 0.0078\n",
      "Epoch 40/150 | Batch 2400/3450 | Loss: 0.0121\n",
      "Epoch 40/150 | Batch 2450/3450 | Loss: 0.0196\n",
      "Epoch 40/150 | Batch 2500/3450 | Loss: 0.0085\n",
      "Epoch 40/150 | Batch 2550/3450 | Loss: 0.0378\n",
      "Epoch 40/150 | Batch 2600/3450 | Loss: 0.0161\n",
      "Epoch 40/150 | Batch 2650/3450 | Loss: 0.0341\n",
      "Epoch 40/150 | Batch 2700/3450 | Loss: 0.0109\n",
      "Epoch 40/150 | Batch 2750/3450 | Loss: 0.0711\n",
      "Epoch 40/150 | Batch 2800/3450 | Loss: 0.0059\n",
      "Epoch 40/150 | Batch 2850/3450 | Loss: 0.0391\n",
      "Epoch 40/150 | Batch 2900/3450 | Loss: 0.0277\n",
      "Epoch 40/150 | Batch 2950/3450 | Loss: 0.0625\n",
      "Epoch 40/150 | Batch 3000/3450 | Loss: 0.0331\n",
      "Epoch 40/150 | Batch 3050/3450 | Loss: 0.0139\n",
      "Epoch 40/150 | Batch 3100/3450 | Loss: 0.0163\n",
      "Epoch 40/150 | Batch 3150/3450 | Loss: 0.0133\n",
      "Epoch 40/150 | Batch 3200/3450 | Loss: 0.0604\n",
      "Epoch 40/150 | Batch 3250/3450 | Loss: 0.0250\n",
      "Epoch 40/150 | Batch 3300/3450 | Loss: 0.0486\n",
      "Epoch 40/150 | Batch 3350/3450 | Loss: 0.0197\n",
      "Epoch 40/150 | Batch 3400/3450 | Loss: 0.0476\n",
      "Epoch 40/150 | Batch 3450/3450 | Loss: 0.0591\n",
      "\n",
      "Epoch 40/150 Summary:\n",
      "Time: 5030.87s | Total: 2 days, 7:53:22\n",
      "LR: 0.00017407\n",
      "Train Loss: 0.0322\n",
      "Val Loss: 0.0169\n",
      "PSNR: 32.20 | SSIM: 0.9320 | LPIPS: 1.8156\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 41/150 | Batch 50/3450 | Loss: 0.0158\n",
      "Epoch 41/150 | Batch 100/3450 | Loss: 0.0113\n",
      "Epoch 41/150 | Batch 150/3450 | Loss: 0.0371\n",
      "Epoch 41/150 | Batch 200/3450 | Loss: 0.0181\n",
      "Epoch 41/150 | Batch 250/3450 | Loss: 0.0213\n",
      "Epoch 41/150 | Batch 300/3450 | Loss: 0.0693\n",
      "Epoch 41/150 | Batch 350/3450 | Loss: 0.0278\n",
      "Epoch 41/150 | Batch 400/3450 | Loss: 0.0264\n",
      "Epoch 41/150 | Batch 450/3450 | Loss: 0.0171\n",
      "Epoch 41/150 | Batch 500/3450 | Loss: 0.0663\n",
      "Epoch 41/150 | Batch 550/3450 | Loss: 0.0314\n",
      "Epoch 41/150 | Batch 600/3450 | Loss: 0.0173\n",
      "Epoch 41/150 | Batch 650/3450 | Loss: 0.0818\n",
      "Epoch 41/150 | Batch 700/3450 | Loss: 0.0176\n",
      "Epoch 41/150 | Batch 750/3450 | Loss: 0.0447\n",
      "Epoch 41/150 | Batch 800/3450 | Loss: 0.0164\n",
      "Epoch 41/150 | Batch 850/3450 | Loss: 0.0209\n",
      "Epoch 41/150 | Batch 900/3450 | Loss: 0.0238\n",
      "Epoch 41/150 | Batch 950/3450 | Loss: 0.0910\n",
      "Epoch 41/150 | Batch 1000/3450 | Loss: 0.0167\n",
      "Epoch 41/150 | Batch 1050/3450 | Loss: 0.0283\n",
      "Epoch 41/150 | Batch 1100/3450 | Loss: 0.0157\n",
      "Epoch 41/150 | Batch 1150/3450 | Loss: 0.0146\n",
      "Epoch 41/150 | Batch 1200/3450 | Loss: 0.0484\n",
      "Epoch 41/150 | Batch 1250/3450 | Loss: 0.0676\n",
      "Epoch 41/150 | Batch 1300/3450 | Loss: 0.0104\n",
      "Epoch 41/150 | Batch 1350/3450 | Loss: 0.0733\n",
      "Epoch 41/150 | Batch 1400/3450 | Loss: 0.0332\n",
      "Epoch 41/150 | Batch 1450/3450 | Loss: 0.0689\n",
      "Epoch 41/150 | Batch 1500/3450 | Loss: 0.0407\n",
      "Epoch 41/150 | Batch 1550/3450 | Loss: 0.0138\n",
      "Epoch 41/150 | Batch 1600/3450 | Loss: 0.0991\n",
      "Epoch 41/150 | Batch 1650/3450 | Loss: 0.0160\n",
      "Epoch 41/150 | Batch 1700/3450 | Loss: 0.0184\n",
      "Epoch 41/150 | Batch 1750/3450 | Loss: 0.0269\n",
      "Epoch 41/150 | Batch 1800/3450 | Loss: 0.0066\n",
      "Epoch 41/150 | Batch 1850/3450 | Loss: 0.0516\n",
      "Epoch 41/150 | Batch 1900/3450 | Loss: 0.0129\n",
      "Epoch 41/150 | Batch 1950/3450 | Loss: 0.0304\n",
      "Epoch 41/150 | Batch 2000/3450 | Loss: 0.0471\n",
      "Epoch 41/150 | Batch 2050/3450 | Loss: 0.0298\n",
      "Epoch 41/150 | Batch 2100/3450 | Loss: 0.0488\n",
      "Epoch 41/150 | Batch 2150/3450 | Loss: 0.0184\n",
      "Epoch 41/150 | Batch 2200/3450 | Loss: 0.0176\n",
      "Epoch 41/150 | Batch 2250/3450 | Loss: 0.0302\n",
      "Epoch 41/150 | Batch 2300/3450 | Loss: 0.0083\n",
      "Epoch 41/150 | Batch 2350/3450 | Loss: 0.0457\n",
      "Epoch 41/150 | Batch 2400/3450 | Loss: 0.0481\n",
      "Epoch 41/150 | Batch 2450/3450 | Loss: 0.0150\n",
      "Epoch 41/150 | Batch 2500/3450 | Loss: 0.0430\n",
      "Epoch 41/150 | Batch 2550/3450 | Loss: 0.0188\n",
      "Epoch 41/150 | Batch 2600/3450 | Loss: 0.0410\n",
      "Epoch 41/150 | Batch 2650/3450 | Loss: 0.0482\n",
      "Epoch 41/150 | Batch 2700/3450 | Loss: 0.0371\n",
      "Epoch 41/150 | Batch 2750/3450 | Loss: 0.0115\n",
      "Epoch 41/150 | Batch 2800/3450 | Loss: 0.0251\n",
      "Epoch 41/150 | Batch 2850/3450 | Loss: 0.0079\n",
      "Epoch 41/150 | Batch 2900/3450 | Loss: 0.0364\n",
      "Epoch 41/150 | Batch 2950/3450 | Loss: 0.0520\n",
      "Epoch 41/150 | Batch 3000/3450 | Loss: 0.0150\n",
      "Epoch 41/150 | Batch 3050/3450 | Loss: 0.0203\n",
      "Epoch 41/150 | Batch 3100/3450 | Loss: 0.0310\n",
      "Epoch 41/150 | Batch 3150/3450 | Loss: 0.0398\n",
      "Epoch 41/150 | Batch 3200/3450 | Loss: 0.0153\n",
      "Epoch 41/150 | Batch 3250/3450 | Loss: 0.0420\n",
      "Epoch 41/150 | Batch 3300/3450 | Loss: 0.0284\n",
      "Epoch 41/150 | Batch 3350/3450 | Loss: 0.0415\n",
      "Epoch 41/150 | Batch 3400/3450 | Loss: 0.0392\n",
      "Epoch 41/150 | Batch 3450/3450 | Loss: 0.0334\n",
      "\n",
      "Epoch 41/150 Summary:\n",
      "Time: 5025.41s | Total: 2 days, 9:17:09\n",
      "LR: 0.00017260\n",
      "Train Loss: 0.0313\n",
      "Val Loss: 0.0167\n",
      "PSNR: 32.48 | SSIM: 0.9364 | LPIPS: 1.7579\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 42/150 | Batch 50/3450 | Loss: 0.0337\n",
      "Epoch 42/150 | Batch 100/3450 | Loss: 0.0129\n",
      "Epoch 42/150 | Batch 150/3450 | Loss: 0.0556\n",
      "Epoch 42/150 | Batch 200/3450 | Loss: 0.0101\n",
      "Epoch 42/150 | Batch 250/3450 | Loss: 0.0089\n",
      "Epoch 42/150 | Batch 300/3450 | Loss: 0.0171\n",
      "Epoch 42/150 | Batch 350/3450 | Loss: 0.0466\n",
      "Epoch 42/150 | Batch 400/3450 | Loss: 0.0091\n",
      "Epoch 42/150 | Batch 450/3450 | Loss: 0.0399\n",
      "Epoch 42/150 | Batch 500/3450 | Loss: 0.0461\n",
      "Epoch 42/150 | Batch 550/3450 | Loss: 0.0218\n",
      "Epoch 42/150 | Batch 600/3450 | Loss: 0.0381\n",
      "Epoch 42/150 | Batch 650/3450 | Loss: 0.0127\n",
      "Epoch 42/150 | Batch 700/3450 | Loss: 0.0323\n",
      "Epoch 42/150 | Batch 750/3450 | Loss: 0.0448\n",
      "Epoch 42/150 | Batch 800/3450 | Loss: 0.0333\n",
      "Epoch 42/150 | Batch 850/3450 | Loss: 0.0552\n",
      "Epoch 42/150 | Batch 900/3450 | Loss: 0.0415\n",
      "Epoch 42/150 | Batch 950/3450 | Loss: 0.0693\n",
      "Epoch 42/150 | Batch 1000/3450 | Loss: 0.0208\n",
      "Epoch 42/150 | Batch 1050/3450 | Loss: 0.0224\n",
      "Epoch 42/150 | Batch 1100/3450 | Loss: 0.0477\n",
      "Epoch 42/150 | Batch 1150/3450 | Loss: 0.0206\n",
      "Epoch 42/150 | Batch 1200/3450 | Loss: 0.0108\n",
      "Epoch 42/150 | Batch 1250/3450 | Loss: 0.0422\n",
      "Epoch 42/150 | Batch 1300/3450 | Loss: 0.0241\n",
      "Epoch 42/150 | Batch 1350/3450 | Loss: 0.0610\n",
      "Epoch 42/150 | Batch 1400/3450 | Loss: 0.0397\n",
      "Epoch 42/150 | Batch 1450/3450 | Loss: 0.0327\n",
      "Epoch 42/150 | Batch 1500/3450 | Loss: 0.0720\n",
      "Epoch 42/150 | Batch 1550/3450 | Loss: 0.0401\n",
      "Epoch 42/150 | Batch 1600/3450 | Loss: 0.0299\n",
      "Epoch 42/150 | Batch 1650/3450 | Loss: 0.0174\n",
      "Epoch 42/150 | Batch 1700/3450 | Loss: 0.0367\n",
      "Epoch 42/150 | Batch 1750/3450 | Loss: 0.0375\n",
      "Epoch 42/150 | Batch 1800/3450 | Loss: 0.0673\n",
      "Epoch 42/150 | Batch 1850/3450 | Loss: 0.0583\n",
      "Epoch 42/150 | Batch 1900/3450 | Loss: 0.0326\n",
      "Epoch 42/150 | Batch 1950/3450 | Loss: 0.0297\n",
      "Epoch 42/150 | Batch 2000/3450 | Loss: 0.0150\n",
      "Epoch 42/150 | Batch 2050/3450 | Loss: 0.0320\n",
      "Epoch 42/150 | Batch 2100/3450 | Loss: 0.0629\n",
      "Epoch 42/150 | Batch 2150/3450 | Loss: 0.0211\n",
      "Epoch 42/150 | Batch 2200/3450 | Loss: 0.0165\n",
      "Epoch 42/150 | Batch 2250/3450 | Loss: 0.0454\n",
      "Epoch 42/150 | Batch 2300/3450 | Loss: 0.0456\n",
      "Epoch 42/150 | Batch 2350/3450 | Loss: 0.0274\n",
      "Epoch 42/150 | Batch 2400/3450 | Loss: 0.0103\n",
      "Epoch 42/150 | Batch 2450/3450 | Loss: 0.0242\n",
      "Epoch 42/150 | Batch 2500/3450 | Loss: 0.0573\n",
      "Epoch 42/150 | Batch 2550/3450 | Loss: 0.0648\n",
      "Epoch 42/150 | Batch 2600/3450 | Loss: 0.0284\n",
      "Epoch 42/150 | Batch 2650/3450 | Loss: 0.0342\n",
      "Epoch 42/150 | Batch 2700/3450 | Loss: 0.0088\n",
      "Epoch 42/150 | Batch 2750/3450 | Loss: 0.0137\n",
      "Epoch 42/150 | Batch 2800/3450 | Loss: 0.0605\n",
      "Epoch 42/150 | Batch 2850/3450 | Loss: 0.0210\n",
      "Epoch 42/150 | Batch 2900/3450 | Loss: 0.0153\n",
      "Epoch 42/150 | Batch 2950/3450 | Loss: 0.0159\n",
      "Epoch 42/150 | Batch 3000/3450 | Loss: 0.0250\n",
      "Epoch 42/150 | Batch 3050/3450 | Loss: 0.0685\n",
      "Epoch 42/150 | Batch 3100/3450 | Loss: 0.0322\n",
      "Epoch 42/150 | Batch 3150/3450 | Loss: 0.0557\n",
      "Epoch 42/150 | Batch 3200/3450 | Loss: 0.0350\n",
      "Epoch 42/150 | Batch 3250/3450 | Loss: 0.0376\n",
      "Epoch 42/150 | Batch 3300/3450 | Loss: 0.0404\n",
      "Epoch 42/150 | Batch 3350/3450 | Loss: 0.0362\n",
      "Epoch 42/150 | Batch 3400/3450 | Loss: 0.0297\n",
      "Epoch 42/150 | Batch 3450/3450 | Loss: 0.0160\n",
      "\n",
      "Epoch 42/150 Summary:\n",
      "Time: 5033.47s | Total: 2 days, 10:41:05\n",
      "LR: 0.00017109\n",
      "Train Loss: 0.0315\n",
      "Val Loss: 0.0189\n",
      "PSNR: 31.40 | SSIM: 0.9249 | LPIPS: 1.8447\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 43/150 | Batch 50/3450 | Loss: 0.0164\n",
      "Epoch 43/150 | Batch 100/3450 | Loss: 0.0268\n",
      "Epoch 43/150 | Batch 150/3450 | Loss: 0.0628\n",
      "Epoch 43/150 | Batch 200/3450 | Loss: 0.0627\n",
      "Epoch 43/150 | Batch 250/3450 | Loss: 0.0272\n",
      "Epoch 43/150 | Batch 300/3450 | Loss: 0.0466\n",
      "Epoch 43/150 | Batch 350/3450 | Loss: 0.0419\n",
      "Epoch 43/150 | Batch 400/3450 | Loss: 0.0091\n",
      "Epoch 43/150 | Batch 450/3450 | Loss: 0.0156\n",
      "Epoch 43/150 | Batch 500/3450 | Loss: 0.0146\n",
      "Epoch 43/150 | Batch 550/3450 | Loss: 0.0290\n",
      "Epoch 43/150 | Batch 600/3450 | Loss: 0.0108\n",
      "Epoch 43/150 | Batch 650/3450 | Loss: 0.0155\n",
      "Epoch 43/150 | Batch 700/3450 | Loss: 0.0672\n",
      "Epoch 43/150 | Batch 750/3450 | Loss: 0.0377\n",
      "Epoch 43/150 | Batch 800/3450 | Loss: 0.0543\n",
      "Epoch 43/150 | Batch 850/3450 | Loss: 0.0400\n",
      "Epoch 43/150 | Batch 900/3450 | Loss: 0.0333\n",
      "Epoch 43/150 | Batch 950/3450 | Loss: 0.0179\n",
      "Epoch 43/150 | Batch 1000/3450 | Loss: 0.0351\n",
      "Epoch 43/150 | Batch 1050/3450 | Loss: 0.0421\n",
      "Epoch 43/150 | Batch 1100/3450 | Loss: 0.0110\n",
      "Epoch 43/150 | Batch 1150/3450 | Loss: 0.0076\n",
      "Epoch 43/150 | Batch 1200/3450 | Loss: 0.0261\n",
      "Epoch 43/150 | Batch 1250/3450 | Loss: 0.0141\n",
      "Epoch 43/150 | Batch 1300/3450 | Loss: 0.0124\n",
      "Epoch 43/150 | Batch 1350/3450 | Loss: 0.0410\n",
      "Epoch 43/150 | Batch 1400/3450 | Loss: 0.0632\n",
      "Epoch 43/150 | Batch 1450/3450 | Loss: 0.0142\n",
      "Epoch 43/150 | Batch 1500/3450 | Loss: 0.0284\n",
      "Epoch 43/150 | Batch 1550/3450 | Loss: 0.0660\n",
      "Epoch 43/150 | Batch 1600/3450 | Loss: 0.0351\n",
      "Epoch 43/150 | Batch 1650/3450 | Loss: 0.0171\n",
      "Epoch 43/150 | Batch 1700/3450 | Loss: 0.0152\n",
      "Epoch 43/150 | Batch 1750/3450 | Loss: 0.0469\n",
      "Epoch 43/150 | Batch 1800/3450 | Loss: 0.0123\n",
      "Epoch 43/150 | Batch 1850/3450 | Loss: 0.0136\n",
      "Epoch 43/150 | Batch 1900/3450 | Loss: 0.0619\n",
      "Epoch 43/150 | Batch 1950/3450 | Loss: 0.0267\n",
      "Epoch 43/150 | Batch 2000/3450 | Loss: 0.0209\n",
      "Epoch 43/150 | Batch 2050/3450 | Loss: 0.0189\n",
      "Epoch 43/150 | Batch 2100/3450 | Loss: 0.0618\n",
      "Epoch 43/150 | Batch 2150/3450 | Loss: 0.0438\n",
      "Epoch 43/150 | Batch 2200/3450 | Loss: 0.0482\n",
      "Epoch 43/150 | Batch 2250/3450 | Loss: 0.0127\n",
      "Epoch 43/150 | Batch 2300/3450 | Loss: 0.0182\n",
      "Epoch 43/150 | Batch 2350/3450 | Loss: 0.0136\n",
      "Epoch 43/150 | Batch 2400/3450 | Loss: 0.0485\n",
      "Epoch 43/150 | Batch 2450/3450 | Loss: 0.0316\n",
      "Epoch 43/150 | Batch 2500/3450 | Loss: 0.0161\n",
      "Epoch 43/150 | Batch 2550/3450 | Loss: 0.0218\n",
      "Epoch 43/150 | Batch 2600/3450 | Loss: 0.0141\n",
      "Epoch 43/150 | Batch 2650/3450 | Loss: 0.0560\n",
      "Epoch 43/150 | Batch 2700/3450 | Loss: 0.0210\n",
      "Epoch 43/150 | Batch 2750/3450 | Loss: 0.0129\n",
      "Epoch 43/150 | Batch 2800/3450 | Loss: 0.0232\n",
      "Epoch 43/150 | Batch 2850/3450 | Loss: 0.0177\n",
      "Epoch 43/150 | Batch 2900/3450 | Loss: 0.0175\n",
      "Epoch 43/150 | Batch 2950/3450 | Loss: 0.0532\n",
      "Epoch 43/150 | Batch 3000/3450 | Loss: 0.0369\n",
      "Epoch 43/150 | Batch 3050/3450 | Loss: 0.0118\n",
      "Epoch 43/150 | Batch 3100/3450 | Loss: 0.0808\n",
      "Epoch 43/150 | Batch 3150/3450 | Loss: 0.0537\n",
      "Epoch 43/150 | Batch 3200/3450 | Loss: 0.0182\n",
      "Epoch 43/150 | Batch 3250/3450 | Loss: 0.0106\n",
      "Epoch 43/150 | Batch 3300/3450 | Loss: 0.0177\n",
      "Epoch 43/150 | Batch 3350/3450 | Loss: 0.0254\n",
      "Epoch 43/150 | Batch 3400/3450 | Loss: 0.0099\n",
      "Epoch 43/150 | Batch 3450/3450 | Loss: 0.0194\n",
      "\n",
      "Epoch 43/150 Summary:\n",
      "Time: 5026.40s | Total: 2 days, 12:04:53\n",
      "LR: 0.00016955\n",
      "Train Loss: 0.0321\n",
      "Val Loss: 0.0177\n",
      "PSNR: 32.27 | SSIM: 0.9366 | LPIPS: 1.7584\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 44/150 | Batch 50/3450 | Loss: 0.0686\n",
      "Epoch 44/150 | Batch 100/3450 | Loss: 0.0097\n",
      "Epoch 44/150 | Batch 150/3450 | Loss: 0.0141\n",
      "Epoch 44/150 | Batch 200/3450 | Loss: 0.0442\n",
      "Epoch 44/150 | Batch 250/3450 | Loss: 0.0112\n",
      "Epoch 44/150 | Batch 300/3450 | Loss: 0.0364\n",
      "Epoch 44/150 | Batch 350/3450 | Loss: 0.0233\n",
      "Epoch 44/150 | Batch 400/3450 | Loss: 0.0595\n",
      "Epoch 44/150 | Batch 450/3450 | Loss: 0.0957\n",
      "Epoch 44/150 | Batch 500/3450 | Loss: 0.0356\n",
      "Epoch 44/150 | Batch 550/3450 | Loss: 0.0991\n",
      "Epoch 44/150 | Batch 600/3450 | Loss: 0.0227\n",
      "Epoch 44/150 | Batch 650/3450 | Loss: 0.0246\n",
      "Epoch 44/150 | Batch 700/3450 | Loss: 0.0152\n",
      "Epoch 44/150 | Batch 750/3450 | Loss: 0.0168\n",
      "Epoch 44/150 | Batch 800/3450 | Loss: 0.0338\n",
      "Epoch 44/150 | Batch 850/3450 | Loss: 0.0132\n",
      "Epoch 44/150 | Batch 900/3450 | Loss: 0.0165\n",
      "Epoch 44/150 | Batch 950/3450 | Loss: 0.0222\n",
      "Epoch 44/150 | Batch 1000/3450 | Loss: 0.0610\n",
      "Epoch 44/150 | Batch 1050/3450 | Loss: 0.0200\n",
      "Epoch 44/150 | Batch 1100/3450 | Loss: 0.0413\n",
      "Epoch 44/150 | Batch 1150/3450 | Loss: 0.0138\n",
      "Epoch 44/150 | Batch 1200/3450 | Loss: 0.0200\n",
      "Epoch 44/150 | Batch 1250/3450 | Loss: 0.0152\n",
      "Epoch 44/150 | Batch 1300/3450 | Loss: 0.0196\n",
      "Epoch 44/150 | Batch 1350/3450 | Loss: 0.0340\n",
      "Epoch 44/150 | Batch 1400/3450 | Loss: 0.0313\n",
      "Epoch 44/150 | Batch 1450/3450 | Loss: 0.0488\n",
      "Epoch 44/150 | Batch 1500/3450 | Loss: 0.0259\n",
      "Epoch 44/150 | Batch 1550/3450 | Loss: 0.0789\n",
      "Epoch 44/150 | Batch 1600/3450 | Loss: 0.0128\n",
      "Epoch 44/150 | Batch 1650/3450 | Loss: 0.0243\n",
      "Epoch 44/150 | Batch 1700/3450 | Loss: 0.0191\n",
      "Epoch 44/150 | Batch 1750/3450 | Loss: 0.0411\n",
      "Epoch 44/150 | Batch 1800/3450 | Loss: 0.0125\n",
      "Epoch 44/150 | Batch 1850/3450 | Loss: 0.0228\n",
      "Epoch 44/150 | Batch 1900/3450 | Loss: 0.0109\n",
      "Epoch 44/150 | Batch 1950/3450 | Loss: 0.0421\n",
      "Epoch 44/150 | Batch 2000/3450 | Loss: 0.0261\n",
      "Epoch 44/150 | Batch 2050/3450 | Loss: 0.0487\n",
      "Epoch 44/150 | Batch 2100/3450 | Loss: 0.0370\n",
      "Epoch 44/150 | Batch 2150/3450 | Loss: 0.0159\n",
      "Epoch 44/150 | Batch 2200/3450 | Loss: 0.0075\n",
      "Epoch 44/150 | Batch 2250/3450 | Loss: 0.0495\n",
      "Epoch 44/150 | Batch 2300/3450 | Loss: 0.0447\n",
      "Epoch 44/150 | Batch 2350/3450 | Loss: 0.0325\n",
      "Epoch 44/150 | Batch 2400/3450 | Loss: 0.0222\n",
      "Epoch 44/150 | Batch 2450/3450 | Loss: 0.0215\n",
      "Epoch 44/150 | Batch 2500/3450 | Loss: 0.0469\n",
      "Epoch 44/150 | Batch 2550/3450 | Loss: 0.0162\n",
      "Epoch 44/150 | Batch 2600/3450 | Loss: 0.0303\n",
      "Epoch 44/150 | Batch 2650/3450 | Loss: 0.0640\n",
      "Epoch 44/150 | Batch 2700/3450 | Loss: 0.0416\n",
      "Epoch 44/150 | Batch 2750/3450 | Loss: 0.0123\n",
      "Epoch 44/150 | Batch 2800/3450 | Loss: 0.0183\n",
      "Epoch 44/150 | Batch 2850/3450 | Loss: 0.0144\n",
      "Epoch 44/150 | Batch 2900/3450 | Loss: 0.0115\n",
      "Epoch 44/150 | Batch 2950/3450 | Loss: 0.0208\n",
      "Epoch 44/150 | Batch 3000/3450 | Loss: 0.0145\n",
      "Epoch 44/150 | Batch 3050/3450 | Loss: 0.0340\n",
      "Epoch 44/150 | Batch 3100/3450 | Loss: 0.0127\n",
      "Epoch 44/150 | Batch 3150/3450 | Loss: 0.0294\n",
      "Epoch 44/150 | Batch 3200/3450 | Loss: 0.0284\n",
      "Epoch 44/150 | Batch 3250/3450 | Loss: 0.0248\n",
      "Epoch 44/150 | Batch 3300/3450 | Loss: 0.0363\n",
      "Epoch 44/150 | Batch 3350/3450 | Loss: 0.0074\n",
      "Epoch 44/150 | Batch 3400/3450 | Loss: 0.0106\n",
      "Epoch 44/150 | Batch 3450/3450 | Loss: 0.0178\n",
      "\n",
      "Epoch 44/150 Summary:\n",
      "Time: 5044.84s | Total: 2 days, 13:28:59\n",
      "LR: 0.00016798\n",
      "Train Loss: 0.0314\n",
      "Val Loss: 0.0177\n",
      "PSNR: 31.86 | SSIM: 0.9301 | LPIPS: 1.8315\n",
      "Best PSNR so far: 32.56 at epoch 31\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 45/150 | Batch 50/3450 | Loss: 0.0354\n",
      "Epoch 45/150 | Batch 100/3450 | Loss: 0.0140\n",
      "Epoch 45/150 | Batch 150/3450 | Loss: 0.0784\n",
      "Epoch 45/150 | Batch 200/3450 | Loss: 0.0441\n",
      "Epoch 45/150 | Batch 250/3450 | Loss: 0.0456\n",
      "Epoch 45/150 | Batch 300/3450 | Loss: 0.0119\n",
      "Epoch 45/150 | Batch 350/3450 | Loss: 0.0067\n",
      "Epoch 45/150 | Batch 400/3450 | Loss: 0.0770\n",
      "Epoch 45/150 | Batch 450/3450 | Loss: 0.0412\n",
      "Epoch 45/150 | Batch 500/3450 | Loss: 0.0604\n",
      "Epoch 45/150 | Batch 550/3450 | Loss: 0.0398\n",
      "Epoch 45/150 | Batch 600/3450 | Loss: 0.0218\n",
      "Epoch 45/150 | Batch 650/3450 | Loss: 0.0520\n",
      "Epoch 45/150 | Batch 700/3450 | Loss: 0.0210\n",
      "Epoch 45/150 | Batch 750/3450 | Loss: 0.0228\n",
      "Epoch 45/150 | Batch 800/3450 | Loss: 0.0407\n",
      "Epoch 45/150 | Batch 850/3450 | Loss: 0.0286\n",
      "Epoch 45/150 | Batch 900/3450 | Loss: 0.0459\n",
      "Epoch 45/150 | Batch 950/3450 | Loss: 0.0159\n",
      "Epoch 45/150 | Batch 1000/3450 | Loss: 0.0094\n",
      "Epoch 45/150 | Batch 1050/3450 | Loss: 0.0267\n",
      "Epoch 45/150 | Batch 1100/3450 | Loss: 0.0606\n",
      "Epoch 45/150 | Batch 1150/3450 | Loss: 0.0166\n",
      "Epoch 45/150 | Batch 1200/3450 | Loss: 0.0351\n",
      "Epoch 45/150 | Batch 1250/3450 | Loss: 0.0484\n",
      "Epoch 45/150 | Batch 1300/3450 | Loss: 0.0507\n",
      "Epoch 45/150 | Batch 1350/3450 | Loss: 0.0544\n",
      "Epoch 45/150 | Batch 1400/3450 | Loss: 0.0159\n",
      "Epoch 45/150 | Batch 1450/3450 | Loss: 0.0476\n",
      "Epoch 45/150 | Batch 1500/3450 | Loss: 0.0557\n",
      "Epoch 45/150 | Batch 1550/3450 | Loss: 0.0487\n",
      "Epoch 45/150 | Batch 1600/3450 | Loss: 0.0277\n",
      "Epoch 45/150 | Batch 1650/3450 | Loss: 0.0602\n",
      "Epoch 45/150 | Batch 1700/3450 | Loss: 0.0357\n",
      "Epoch 45/150 | Batch 1750/3450 | Loss: 0.0117\n",
      "Epoch 45/150 | Batch 1800/3450 | Loss: 0.0623\n",
      "Epoch 45/150 | Batch 1850/3450 | Loss: 0.0177\n",
      "Epoch 45/150 | Batch 1900/3450 | Loss: 0.0250\n",
      "Epoch 45/150 | Batch 1950/3450 | Loss: 0.0752\n",
      "Epoch 45/150 | Batch 2000/3450 | Loss: 0.0109\n",
      "Epoch 45/150 | Batch 2050/3450 | Loss: 0.0089\n",
      "Epoch 45/150 | Batch 2100/3450 | Loss: 0.0175\n",
      "Epoch 45/150 | Batch 2150/3450 | Loss: 0.0145\n",
      "Epoch 45/150 | Batch 2200/3450 | Loss: 0.0533\n",
      "Epoch 45/150 | Batch 2250/3450 | Loss: 0.0373\n",
      "Epoch 45/150 | Batch 2300/3450 | Loss: 0.0237\n",
      "Epoch 45/150 | Batch 2350/3450 | Loss: 0.0415\n",
      "Epoch 45/150 | Batch 2400/3450 | Loss: 0.0450\n",
      "Epoch 45/150 | Batch 2450/3450 | Loss: 0.0286\n",
      "Epoch 45/150 | Batch 2500/3450 | Loss: 0.0819\n",
      "Epoch 45/150 | Batch 2550/3450 | Loss: 0.0229\n",
      "Epoch 45/150 | Batch 2600/3450 | Loss: 0.0387\n",
      "Epoch 45/150 | Batch 2650/3450 | Loss: 0.0200\n",
      "Epoch 45/150 | Batch 2700/3450 | Loss: 0.0718\n",
      "Epoch 45/150 | Batch 2750/3450 | Loss: 0.0090\n",
      "Epoch 45/150 | Batch 2800/3450 | Loss: 0.0254\n",
      "Epoch 45/150 | Batch 2850/3450 | Loss: 0.0485\n",
      "Epoch 45/150 | Batch 2900/3450 | Loss: 0.0472\n",
      "Epoch 45/150 | Batch 2950/3450 | Loss: 0.0145\n",
      "Epoch 45/150 | Batch 3000/3450 | Loss: 0.0348\n",
      "Epoch 45/150 | Batch 3050/3450 | Loss: 0.0189\n",
      "Epoch 45/150 | Batch 3100/3450 | Loss: 0.0876\n",
      "Epoch 45/150 | Batch 3150/3450 | Loss: 0.0199\n",
      "Epoch 45/150 | Batch 3200/3450 | Loss: 0.0333\n",
      "Epoch 45/150 | Batch 3250/3450 | Loss: 0.0625\n",
      "Epoch 45/150 | Batch 3300/3450 | Loss: 0.0744\n",
      "Epoch 45/150 | Batch 3350/3450 | Loss: 0.0209\n",
      "Epoch 45/150 | Batch 3400/3450 | Loss: 0.0205\n",
      "Epoch 45/150 | Batch 3450/3450 | Loss: 0.0200\n",
      "\n",
      "Epoch 45/150 Summary:\n",
      "Time: 5054.78s | Total: 2 days, 14:53:16\n",
      "LR: 0.00016637\n",
      "Train Loss: 0.0311\n",
      "Val Loss: 0.0159\n",
      "PSNR: 32.80 | SSIM: 0.9342 | LPIPS: 1.7566\n",
      "[INFO] New best model saved with PSNR: 32.80\n",
      "Best PSNR so far: 32.80 at epoch 45\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 46/150 | Batch 50/3450 | Loss: 0.0120\n",
      "Epoch 46/150 | Batch 100/3450 | Loss: 0.0197\n",
      "Epoch 46/150 | Batch 150/3450 | Loss: 0.0297\n",
      "Epoch 46/150 | Batch 200/3450 | Loss: 0.0596\n",
      "Epoch 46/150 | Batch 250/3450 | Loss: 0.0248\n",
      "Epoch 46/150 | Batch 300/3450 | Loss: 0.0376\n",
      "Epoch 46/150 | Batch 350/3450 | Loss: 0.0090\n",
      "Epoch 46/150 | Batch 400/3450 | Loss: 0.0223\n",
      "Epoch 46/150 | Batch 450/3450 | Loss: 0.0232\n",
      "Epoch 46/150 | Batch 500/3450 | Loss: 0.0525\n",
      "Epoch 46/150 | Batch 550/3450 | Loss: 0.0192\n",
      "Epoch 46/150 | Batch 600/3450 | Loss: 0.0433\n",
      "Epoch 46/150 | Batch 650/3450 | Loss: 0.0186\n",
      "Epoch 46/150 | Batch 700/3450 | Loss: 0.0414\n",
      "Epoch 46/150 | Batch 750/3450 | Loss: 0.0858\n",
      "Epoch 46/150 | Batch 800/3450 | Loss: 0.0790\n",
      "Epoch 46/150 | Batch 850/3450 | Loss: 0.0261\n",
      "Epoch 46/150 | Batch 900/3450 | Loss: 0.0421\n",
      "Epoch 46/150 | Batch 950/3450 | Loss: 0.0665\n",
      "Epoch 46/150 | Batch 1000/3450 | Loss: 0.0245\n",
      "Epoch 46/150 | Batch 1050/3450 | Loss: 0.0107\n",
      "Epoch 46/150 | Batch 1100/3450 | Loss: 0.0336\n",
      "Epoch 46/150 | Batch 1150/3450 | Loss: 0.0469\n",
      "Epoch 46/150 | Batch 1200/3450 | Loss: 0.0698\n",
      "Epoch 46/150 | Batch 1250/3450 | Loss: 0.0215\n",
      "Epoch 46/150 | Batch 1300/3450 | Loss: 0.0393\n",
      "Epoch 46/150 | Batch 1350/3450 | Loss: 0.0110\n",
      "Epoch 46/150 | Batch 1400/3450 | Loss: 0.0414\n",
      "Epoch 46/150 | Batch 1450/3450 | Loss: 0.0520\n",
      "Epoch 46/150 | Batch 1500/3450 | Loss: 0.0500\n",
      "Epoch 46/150 | Batch 1550/3450 | Loss: 0.0101\n",
      "Epoch 46/150 | Batch 1600/3450 | Loss: 0.0188\n",
      "Epoch 46/150 | Batch 1650/3450 | Loss: 0.0491\n",
      "Epoch 46/150 | Batch 1700/3450 | Loss: 0.0279\n",
      "Epoch 46/150 | Batch 1750/3450 | Loss: 0.1128\n",
      "Epoch 46/150 | Batch 1800/3450 | Loss: 0.0321\n",
      "Epoch 46/150 | Batch 1850/3450 | Loss: 0.0314\n",
      "Epoch 46/150 | Batch 1900/3450 | Loss: 0.0152\n",
      "Epoch 46/150 | Batch 1950/3450 | Loss: 0.0207\n",
      "Epoch 46/150 | Batch 2000/3450 | Loss: 0.0287\n",
      "Epoch 46/150 | Batch 2050/3450 | Loss: 0.0332\n",
      "Epoch 46/150 | Batch 2100/3450 | Loss: 0.0163\n",
      "Epoch 46/150 | Batch 2150/3450 | Loss: 0.0548\n",
      "Epoch 46/150 | Batch 2200/3450 | Loss: 0.0292\n",
      "Epoch 46/150 | Batch 2250/3450 | Loss: 0.0130\n",
      "Epoch 46/150 | Batch 2300/3450 | Loss: 0.0412\n",
      "Epoch 46/150 | Batch 2350/3450 | Loss: 0.0566\n",
      "Epoch 46/150 | Batch 2400/3450 | Loss: 0.0141\n",
      "Epoch 46/150 | Batch 2450/3450 | Loss: 0.0175\n",
      "Epoch 46/150 | Batch 2500/3450 | Loss: 0.0419\n",
      "Epoch 46/150 | Batch 2550/3450 | Loss: 0.0154\n",
      "Epoch 46/150 | Batch 2600/3450 | Loss: 0.0250\n",
      "Epoch 46/150 | Batch 2650/3450 | Loss: 0.0130\n",
      "Epoch 46/150 | Batch 2700/3450 | Loss: 0.0201\n",
      "Epoch 46/150 | Batch 2750/3450 | Loss: 0.0432\n",
      "Epoch 46/150 | Batch 2800/3450 | Loss: 0.0211\n",
      "Epoch 46/150 | Batch 2850/3450 | Loss: 0.0256\n",
      "Epoch 46/150 | Batch 2900/3450 | Loss: 0.0516\n",
      "Epoch 46/150 | Batch 2950/3450 | Loss: 0.0413\n",
      "Epoch 46/150 | Batch 3000/3450 | Loss: 0.0154\n",
      "Epoch 46/150 | Batch 3050/3450 | Loss: 0.0276\n",
      "Epoch 46/150 | Batch 3100/3450 | Loss: 0.0376\n",
      "Epoch 46/150 | Batch 3150/3450 | Loss: 0.0135\n",
      "Epoch 46/150 | Batch 3200/3450 | Loss: 0.0939\n",
      "Epoch 46/150 | Batch 3250/3450 | Loss: 0.0399\n",
      "Epoch 46/150 | Batch 3300/3450 | Loss: 0.0242\n",
      "Epoch 46/150 | Batch 3350/3450 | Loss: 0.0318\n",
      "Epoch 46/150 | Batch 3400/3450 | Loss: 0.0585\n",
      "Epoch 46/150 | Batch 3450/3450 | Loss: 0.0085\n",
      "\n",
      "Epoch 46/150 Summary:\n",
      "Time: 5058.58s | Total: 2 days, 16:17:37\n",
      "LR: 0.00016474\n",
      "Train Loss: 0.0320\n",
      "Val Loss: 0.0204\n",
      "PSNR: 31.16 | SSIM: 0.9342 | LPIPS: 1.7752\n",
      "Best PSNR so far: 32.80 at epoch 45\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 47/150 | Batch 50/3450 | Loss: 0.0524\n",
      "Epoch 47/150 | Batch 100/3450 | Loss: 0.0482\n",
      "Epoch 47/150 | Batch 150/3450 | Loss: 0.0287\n",
      "Epoch 47/150 | Batch 200/3450 | Loss: 0.0096\n",
      "Epoch 47/150 | Batch 250/3450 | Loss: 0.0162\n",
      "Epoch 47/150 | Batch 300/3450 | Loss: 0.0093\n",
      "Epoch 47/150 | Batch 350/3450 | Loss: 0.0278\n",
      "Epoch 47/150 | Batch 400/3450 | Loss: 0.0128\n",
      "Epoch 47/150 | Batch 450/3450 | Loss: 0.0749\n",
      "Epoch 47/150 | Batch 500/3450 | Loss: 0.0111\n",
      "Epoch 47/150 | Batch 550/3450 | Loss: 0.0193\n",
      "Epoch 47/150 | Batch 600/3450 | Loss: 0.0186\n",
      "Epoch 47/150 | Batch 650/3450 | Loss: 0.0244\n",
      "Epoch 47/150 | Batch 700/3450 | Loss: 0.0210\n",
      "Epoch 47/150 | Batch 750/3450 | Loss: 0.0789\n",
      "Epoch 47/150 | Batch 800/3450 | Loss: 0.0273\n",
      "Epoch 47/150 | Batch 850/3450 | Loss: 0.0377\n",
      "Epoch 47/150 | Batch 900/3450 | Loss: 0.0132\n",
      "Epoch 47/150 | Batch 950/3450 | Loss: 0.0115\n",
      "Epoch 47/150 | Batch 1000/3450 | Loss: 0.0092\n",
      "Epoch 47/150 | Batch 1050/3450 | Loss: 0.0187\n",
      "Epoch 47/150 | Batch 1100/3450 | Loss: 0.0115\n",
      "Epoch 47/150 | Batch 1150/3450 | Loss: 0.0507\n",
      "Epoch 47/150 | Batch 1200/3450 | Loss: 0.0132\n",
      "Epoch 47/150 | Batch 1250/3450 | Loss: 0.0663\n",
      "Epoch 47/150 | Batch 1300/3450 | Loss: 0.0568\n",
      "Epoch 47/150 | Batch 1350/3450 | Loss: 0.0247\n",
      "Epoch 47/150 | Batch 1400/3450 | Loss: 0.0216\n",
      "Epoch 47/150 | Batch 1450/3450 | Loss: 0.0602\n",
      "Epoch 47/150 | Batch 1500/3450 | Loss: 0.0392\n",
      "Epoch 47/150 | Batch 1550/3450 | Loss: 0.0498\n",
      "Epoch 47/150 | Batch 1600/3450 | Loss: 0.0345\n",
      "Epoch 47/150 | Batch 1650/3450 | Loss: 0.0228\n",
      "Epoch 47/150 | Batch 1700/3450 | Loss: 0.0255\n",
      "Epoch 47/150 | Batch 1750/3450 | Loss: 0.0206\n",
      "Epoch 47/150 | Batch 1800/3450 | Loss: 0.0696\n",
      "Epoch 47/150 | Batch 1850/3450 | Loss: 0.0196\n",
      "Epoch 47/150 | Batch 1900/3450 | Loss: 0.0514\n",
      "Epoch 47/150 | Batch 1950/3450 | Loss: 0.0536\n",
      "Epoch 47/150 | Batch 2000/3450 | Loss: 0.0100\n",
      "Epoch 47/150 | Batch 2050/3450 | Loss: 0.0667\n",
      "Epoch 47/150 | Batch 2100/3450 | Loss: 0.0165\n",
      "Epoch 47/150 | Batch 2150/3450 | Loss: 0.0470\n",
      "Epoch 47/150 | Batch 2200/3450 | Loss: 0.0243\n",
      "Epoch 47/150 | Batch 2250/3450 | Loss: 0.0109\n",
      "Epoch 47/150 | Batch 2300/3450 | Loss: 0.0097\n",
      "Epoch 47/150 | Batch 2350/3450 | Loss: 0.0331\n",
      "Epoch 47/150 | Batch 2400/3450 | Loss: 0.0232\n",
      "Epoch 47/150 | Batch 2450/3450 | Loss: 0.0228\n",
      "Epoch 47/150 | Batch 2500/3450 | Loss: 0.0514\n",
      "Epoch 47/150 | Batch 2550/3450 | Loss: 0.0536\n",
      "Epoch 47/150 | Batch 2600/3450 | Loss: 0.0146\n",
      "Epoch 47/150 | Batch 2650/3450 | Loss: 0.0349\n",
      "Epoch 47/150 | Batch 2700/3450 | Loss: 0.0238\n",
      "Epoch 47/150 | Batch 2750/3450 | Loss: 0.0461\n",
      "Epoch 47/150 | Batch 2800/3450 | Loss: 0.0551\n",
      "Epoch 47/150 | Batch 2850/3450 | Loss: 0.0475\n",
      "Epoch 47/150 | Batch 2900/3450 | Loss: 0.0125\n",
      "Epoch 47/150 | Batch 2950/3450 | Loss: 0.0094\n",
      "Epoch 47/150 | Batch 3000/3450 | Loss: 0.0262\n",
      "Epoch 47/150 | Batch 3050/3450 | Loss: 0.0272\n",
      "Epoch 47/150 | Batch 3100/3450 | Loss: 0.0150\n",
      "Epoch 47/150 | Batch 3150/3450 | Loss: 0.0200\n",
      "Epoch 47/150 | Batch 3200/3450 | Loss: 0.0172\n",
      "Epoch 47/150 | Batch 3250/3450 | Loss: 0.0267\n",
      "Epoch 47/150 | Batch 3300/3450 | Loss: 0.0183\n",
      "Epoch 47/150 | Batch 3350/3450 | Loss: 0.0296\n",
      "Epoch 47/150 | Batch 3400/3450 | Loss: 0.0217\n",
      "Epoch 47/150 | Batch 3450/3450 | Loss: 0.0164\n",
      "\n",
      "Epoch 47/150 Summary:\n",
      "Time: 5058.23s | Total: 2 days, 17:41:57\n",
      "LR: 0.00016307\n",
      "Train Loss: 0.0318\n",
      "Val Loss: 0.0162\n",
      "PSNR: 32.58 | SSIM: 0.9356 | LPIPS: 1.7854\n",
      "Best PSNR so far: 32.80 at epoch 45\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 48/150 | Batch 50/3450 | Loss: 0.0268\n",
      "Epoch 48/150 | Batch 100/3450 | Loss: 0.0495\n",
      "Epoch 48/150 | Batch 150/3450 | Loss: 0.0198\n",
      "Epoch 48/150 | Batch 200/3450 | Loss: 0.0162\n",
      "Epoch 48/150 | Batch 250/3450 | Loss: 0.0357\n",
      "Epoch 48/150 | Batch 300/3450 | Loss: 0.0450\n",
      "Epoch 48/150 | Batch 350/3450 | Loss: 0.0253\n",
      "Epoch 48/150 | Batch 400/3450 | Loss: 0.0103\n",
      "Epoch 48/150 | Batch 450/3450 | Loss: 0.0221\n",
      "Epoch 48/150 | Batch 500/3450 | Loss: 0.0214\n",
      "Epoch 48/150 | Batch 550/3450 | Loss: 0.0249\n",
      "Epoch 48/150 | Batch 600/3450 | Loss: 0.0371\n",
      "Epoch 48/150 | Batch 650/3450 | Loss: 0.0401\n",
      "Epoch 48/150 | Batch 700/3450 | Loss: 0.0287\n",
      "Epoch 48/150 | Batch 750/3450 | Loss: 0.0564\n",
      "Epoch 48/150 | Batch 800/3450 | Loss: 0.0447\n",
      "Epoch 48/150 | Batch 850/3450 | Loss: 0.0506\n",
      "Epoch 48/150 | Batch 900/3450 | Loss: 0.0541\n",
      "Epoch 48/150 | Batch 950/3450 | Loss: 0.0452\n",
      "Epoch 48/150 | Batch 1000/3450 | Loss: 0.0073\n",
      "Epoch 48/150 | Batch 1050/3450 | Loss: 0.0085\n",
      "Epoch 48/150 | Batch 1100/3450 | Loss: 0.0411\n",
      "Epoch 48/150 | Batch 1150/3450 | Loss: 0.0375\n",
      "Epoch 48/150 | Batch 1200/3450 | Loss: 0.0463\n",
      "Epoch 48/150 | Batch 1250/3450 | Loss: 0.0234\n",
      "Epoch 48/150 | Batch 1300/3450 | Loss: 0.0163\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Set non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import skimage.metrics\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "\n",
    "print(\"[INFO] Libraries imported.\")\n",
    "\n",
    "# LPIPS for evaluation\n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg19(weights=VGG19_Weights.DEFAULT).features[:30].eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "        self.weights = nn.Parameter(torch.ones(5))\n",
    "        self.layers = [0, 5, 10, 19, 28]\n",
    "    \n",
    "    def _normalize(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        features_x = []\n",
    "        features_y = []\n",
    "        x = self._normalize(x)\n",
    "        y = self._normalize(y)\n",
    "        for i, layer in enumerate(self.vgg):\n",
    "            x = layer(x)\n",
    "            y = layer(y)\n",
    "            if i in self.layers:\n",
    "                features_x.append(x)\n",
    "                features_y.append(y)\n",
    "        dists = [F.l1_loss(fx, fy) for fx, fy in zip(features_x, features_y)]\n",
    "        weighted_dist = sum(w * d for w, d in zip(self.weights, dists))\n",
    "        return weighted_dist\n",
    "\n",
    "# LayerNorm2d\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "# SpatialResidualModule\n",
    "class SpatialResidualModule(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "        self.spatial_att = nn.Sequential(\n",
    "            nn.Conv2d(channels, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([0.1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv(x)\n",
    "        att = self.spatial_att(out)\n",
    "        return residual + self.scale * (out * att)\n",
    "\n",
    "# EnhancedResidualBlock\n",
    "class EnhancedResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.norm1 = LayerNorm2d(channels)\n",
    "        self.norm2 = LayerNorm2d(channels)\n",
    "        self.act = nn.PReLU()\n",
    "        self.spatial_residual = SpatialResidualModule(channels)\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([0.1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.spatial_residual(out)\n",
    "        return residual + self.scale * out\n",
    "\n",
    "# EnhancedResidualGroup\n",
    "class EnhancedResidualGroup(nn.Module):\n",
    "    def __init__(self, channels, n_blocks):\n",
    "        super().__init__()\n",
    "        blocks = [EnhancedResidualBlock(channels) for _ in range(n_blocks)]\n",
    "        self.body = nn.Sequential(*blocks)\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([0.1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.body(x)\n",
    "        out = self.conv(out)\n",
    "        return residual + self.scale * out\n",
    "\n",
    "# EnhancedESPCN\n",
    "class EnhancedESPCN(nn.Module):\n",
    "    def __init__(self, in_channels, scale_factor=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 128, kernel_size=3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(64, 3 * (scale_factor ** 2), kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(scale_factor)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# UltraEnhancedSR\n",
    "class UltraEnhancedSR(nn.Module):\n",
    "    def __init__(self, scale=2):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, 3, padding=1),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.body = nn.ModuleList([\n",
    "            EnhancedResidualGroup(128, 10) for _ in range(5)\n",
    "        ])\n",
    "        self.global_residual = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.upscale = EnhancedESPCN(128, scale)\n",
    "        self.direct_path = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(16, 3*(scale**2), 3, padding=1),\n",
    "            nn.PixelShuffle(scale)\n",
    "        )\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, 3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(32, 3, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        direct = self.direct_path(x)\n",
    "        shallow = self.head(x)\n",
    "        deep = shallow\n",
    "        for block in self.body:\n",
    "            deep = block(deep)\n",
    "        global_res = self.global_residual(deep)\n",
    "        fused = shallow + global_res\n",
    "        upscaled = self.upscale(fused)\n",
    "        combined = torch.cat([direct, upscaled], dim=1)\n",
    "        return self.refine(combined)\n",
    "\n",
    "# RobustSRDataset\n",
    "class RobustSRDataset(Dataset):\n",
    "    def __init__(self, hr_dir, scale=2, augment=True):\n",
    "        self.hr_paths = sorted(glob.glob(os.path.join(hr_dir, '*')))\n",
    "        if not self.hr_paths:\n",
    "            raise ValueError(f\"No files found in directory: {hr_dir}. Please check the path or ensure the directory contains images.\")\n",
    "        self.scale = scale\n",
    "        self.augment = augment\n",
    "        self.color_jitter = transforms.ColorJitter(0.15, 0.15, 0.15, 0.05)\n",
    "        self.blur_kernels = [\n",
    "            ImageFilter.GaussianBlur(0.5),\n",
    "            ImageFilter.GaussianBlur(0.8),\n",
    "            ImageFilter.GaussianBlur(1.0),\n",
    "            ImageFilter.BoxBlur(1),\n",
    "            ImageFilter.BoxBlur(2)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hr_path = self.hr_paths[idx]\n",
    "        try:\n",
    "            hr = Image.open(hr_path).convert('RGB')\n",
    "            if hr.width < 1024 or hr.height < 1024:\n",
    "                hr = TF.resize(hr, [1024, 1024])\n",
    "            hr = TF.resize(hr, [1024, 1024])\n",
    "            if self.augment:\n",
    "                blur_kernel = random.choice(self.blur_kernels)\n",
    "                hr_blurred = hr.filter(blur_kernel)\n",
    "                if random.random() > 0.5:\n",
    "                    hr_blurred = self.color_jitter(hr_blurred)\n",
    "            else:\n",
    "                hr_blurred = hr\n",
    "            lr_size = 1024 // self.scale\n",
    "            lr = hr_blurred.resize((lr_size, lr_size), Image.BICUBIC)\n",
    "            lr_np = np.array(lr).astype(np.float32) / 255.0\n",
    "            noise_level = np.random.uniform(0.005, 0.015)\n",
    "            noise = np.random.normal(0, noise_level, lr_np.shape) * np.sqrt(lr_np + 0.001)\n",
    "            lr_np = np.clip(lr_np + noise, 0, 1)\n",
    "            lr = Image.fromarray((lr_np * 255).astype(np.uint8))\n",
    "            if self.augment:\n",
    "                if random.random() > 0.5:\n",
    "                    lr, hr = TF.hflip(lr), TF.hflip(hr)\n",
    "                if random.random() > 0.5:\n",
    "                    lr, hr = TF.vflip(lr), TF.vflip(hr)\n",
    "                if random.random() > 0.5:\n",
    "                    angle = random.choice([90, 180, 270])\n",
    "                    lr, hr = TF.rotate(lr, angle), TF.rotate(hr, angle)\n",
    "            return TF.to_tensor(lr), TF.to_tensor(hr), os.path.basename(hr_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {hr_path}: {e}\")\n",
    "            lr = torch.zeros(3, 1024//self.scale, 1024//self.scale)\n",
    "            hr = torch.zeros(3, 1024, 1024)\n",
    "            return lr, hr, \"blank_fallback\"\n",
    "\n",
    "# Setup Training\n",
    "def setup_advanced_training():\n",
    "    print(\"[INFO] Setting up advanced training...\")\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    # Validate dataset directories\n",
    "    train_dir = 'DF2K_train_HR'\n",
    "    val_dir = 'DF2K_valid_HR'\n",
    "    if not os.path.exists(train_dir):\n",
    "        raise ValueError(f\"Training directory {train_dir} does not exist.\")\n",
    "    if not os.path.exists(val_dir):\n",
    "        raise ValueError(f\"Validation directory {val_dir} does not exist.\")\n",
    "\n",
    "    train_dataset = RobustSRDataset(train_dir, scale=2)\n",
    "    val_dataset = RobustSRDataset(val_dir, scale=2, augment=False)\n",
    "\n",
    "    print(f\"[INFO] Loaded {len(train_dataset)} training images and {len(val_dataset)} validation images.\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    model = UltraEnhancedSR(scale=2).cuda()\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        print(f\"[INFO] Using {torch.cuda.device_count()} GPUs\")\n",
    "    criterion = nn.L1Loss().cuda()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=2e-4,\n",
    "        weight_decay=1e-4,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    scaler = GradScaler()\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < 5:\n",
    "            return (epoch + 1) / 5\n",
    "        else:\n",
    "            return 0.5 * (1 + math.cos(math.pi * (epoch - 5) / (150 - 5)))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"[INFO] Advanced training setup complete.\")\n",
    "    return train_loader, val_loader, model, criterion, optimizer, scheduler, scaler\n",
    "\n",
    "# Calculate Metrics\n",
    "def calculate_metrics(sr, hr):\n",
    "    if not hasattr(calculate_metrics, \"lpips_model\"):\n",
    "        calculate_metrics.lpips_model = LPIPS().cuda()\n",
    "    sr_np = sr.squeeze(0).detach().cpu().clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "    hr_np = hr.squeeze(0).detach().cpu().permute(1, 2, 0).numpy()\n",
    "    psnr = skimage.metrics.peak_signal_noise_ratio(hr_np, sr_np, data_range=1.0)\n",
    "    ssim = skimage.metrics.structural_similarity(\n",
    "        hr_np, sr_np,\n",
    "        win_size=11,\n",
    "        multichannel=True,\n",
    "        channel_axis=2,\n",
    "        data_range=1.0,\n",
    "        gaussian_weights=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        lpips = calculate_metrics.lpips_model(sr, hr).item()\n",
    "    return psnr, ssim, lpips\n",
    "\n",
    "# Train Model\n",
    "def train_advanced_model(train_loader, val_loader, model, criterion, optimizer, scheduler, scaler, epochs=150):\n",
    "    print(\"[INFO] Starting advanced training...\")\n",
    "    best_psnr = 0\n",
    "    best_epoch = 0\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'psnr': [], 'ssim': [], 'lpips': [],\n",
    "        'best_psnr': 0, 'best_epoch': 0\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    accum_steps = 8\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        batch_count = 0\n",
    "        optimizer.zero_grad()\n",
    "        for i, (lr, hr, _) in enumerate(train_loader):\n",
    "            lr, hr = lr.cuda(), hr.cuda()\n",
    "            with autocast():\n",
    "                sr = model(lr)\n",
    "                loss = criterion(sr, hr)\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i + 1) % accum_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | Batch {i+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        psnr_total = 0\n",
    "        ssim_total = 0\n",
    "        lpips_total = 0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr, fname in val_loader:\n",
    "                lr, hr = lr.cuda(), hr.cuda()\n",
    "                with autocast():\n",
    "                    sr = model(lr)\n",
    "                    val_loss_batch = criterion(sr, hr)\n",
    "                val_loss += val_loss_batch.item()\n",
    "                psnr, ssim, lpips = calculate_metrics(sr, hr)\n",
    "                psnr_total += psnr\n",
    "                ssim_total += ssim\n",
    "                lpips_total += lpips\n",
    "                if fname[0] in [val_loader.dataset.hr_paths[i].split('/')[-1] for i in range(min(3, len(val_loader.dataset)))]:\n",
    "                    save_comparison(lr, sr, hr, f\"epoch_{epoch+1}_{fname[0]}\")\n",
    "        avg_train_loss = train_loss / batch_count\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_psnr = psnr_total / len(val_loader)\n",
    "        avg_ssim = ssim_total / len(val_loader)\n",
    "        avg_lpips = lpips_total / len(val_loader)\n",
    "        scheduler.step()\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['psnr'].append(avg_psnr)\n",
    "        history['ssim'].append(avg_ssim)\n",
    "        history['lpips'].append(avg_lpips)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} Summary:\")\n",
    "        print(f\"Time: {epoch_time:.2f}s | Total: {str(datetime.timedelta(seconds=int(total_time)))}\")\n",
    "        print(f\"LR: {current_lr:.8f}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"PSNR: {avg_psnr:.2f} | SSIM: {avg_ssim:.4f} | LPIPS: {avg_lpips:.4f}\")\n",
    "        checkpoint_path = f\"checkpoints/model_epoch_{epoch+1}.pth\"\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            torch.save(model.module.state_dict(), checkpoint_path)\n",
    "        else:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        if avg_psnr > best_psnr:\n",
    "            best_psnr = avg_psnr\n",
    "            best_epoch = epoch + 1\n",
    "            history['best_psnr'] = best_psnr\n",
    "            history['best_epoch'] = best_epoch\n",
    "            best_model_path = f\"checkpoints/best_model_epoch_{epoch+1}_psnr_{avg_psnr:.2f}.pth\"\n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                torch.save(model.module.state_dict(), best_model_path)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"[INFO] New best model saved with PSNR: {avg_psnr:.2f}\")\n",
    "        np.save(\"history.npy\", history)\n",
    "        plot_training_curves(history, epoch+1)\n",
    "        print(f\"Best PSNR so far: {best_psnr:.2f} at epoch {best_epoch}\")\n",
    "        print(\"-\" * 80)\n",
    "    final_model_path = \"checkpoints/final_model.pth\"\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        torch.save(model.module.state_dict(), final_model_path)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"[INFO] Training complete. Best PSNR: {best_psnr:.2f} at epoch {best_epoch}\")\n",
    "    print(f\"[INFO] Final model saved as '{final_model_path}'\")\n",
    "    return model, history\n",
    "\n",
    "# Save Comparison\n",
    "def save_comparison(lr, sr, hr, filename):\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    lr_img = lr.squeeze(0).cpu().permute(1, 2, 0).numpy().astype(np.float32)\n",
    "    sr_img = sr.squeeze(0).cpu().clamp(0, 1).permute(1, 2, 0).numpy().astype(np.float32)\n",
    "    hr_img = hr.squeeze(0).cpu().permute(1, 2, 0).numpy().astype(np.float32)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axes[0].imshow(lr_img)\n",
    "    axes[0].set_title('Low Resolution')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(sr_img)\n",
    "    axes[1].set_title('Super Resolution')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(hr_img)\n",
    "    axes[2].set_title('High Resolution (Ground Truth)')\n",
    "    axes[2].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Plot Training Curves\n",
    "def plot_training_curves(history, current_epoch):\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Loss Curves')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    axes[0, 1].plot(history['psnr'])\n",
    "    axes[0, 1].axhline(y=32, color='r', linestyle='--', label='Target PSNR (32dB)')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('PSNR (dB)')\n",
    "    axes[0, 1].set_title('PSNR Progress')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    axes[1, 0].plot(history['ssim'])\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('SSIM')\n",
    "    axes[1, 0].set_title('SSIM Progress')\n",
    "    axes[1, 0].grid(True)\n",
    "    axes[1, 1].plot(history['lpips'])\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('LPIPS')\n",
    "    axes[1, 1].set_title('LPIPS Progress (Lower is Better)')\n",
    "    axes[1, 1].grid(True)\n",
    "    fig.suptitle(f'Training Progress (Epoch {current_epoch})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/training_curves_epoch_{current_epoch}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loader, val_loader, model, criterion, optimizer, scheduler, scaler = setup_advanced_training()\n",
    "    trained_model, history = train_advanced_model(\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        scaler,\n",
    "        epochs=150\n",
    "    )\n",
    "    plot_training_curves(history, len(history['train_loss']))\n",
    "    print(\"[INFO] Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930904df-67d1-4de6-bbc5-879bf2db50cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
