{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2cfc4b7-1ab6-49f4-92aa-dd368188f0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45150bae-9080-47e7-bd3c-b3714d180440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BSD100 Evaluation Results:\n",
      "+------------+----------------------+----------------+\n",
      "|   Image ID |   PSNR Baseline (dB) |   PSNR SR (dB) |\n",
      "+============+======================+================+\n",
      "|     101087 |                29.9  |          30    |\n",
      "+------------+----------------------+----------------+\n",
      "|     103070 |                32.91 |          33.04 |\n",
      "+------------+----------------------+----------------+\n",
      "|     108082 |                29.76 |          30.04 |\n",
      "+------------+----------------------+----------------+\n",
      "|     109053 |                31.62 |          31.76 |\n",
      "+------------+----------------------+----------------+\n",
      "|     123074 |                32.9  |          32.96 |\n",
      "+------------+----------------------+----------------+\n",
      "|     126007 |                30.78 |          30.89 |\n",
      "+------------+----------------------+----------------+\n",
      "|     159008 |                29.81 |          30    |\n",
      "+------------+----------------------+----------------+\n",
      "|      16077 |                29.97 |          30.11 |\n",
      "+------------+----------------------+----------------+\n",
      "|     170057 |                32.33 |          32.38 |\n",
      "+------------+----------------------+----------------+\n",
      "|     208001 |                30.73 |          30.83 |\n",
      "+------------+----------------------+----------------+\n",
      "|     210088 |                34.23 |          34.42 |\n",
      "+------------+----------------------+----------------+\n",
      "|     296059 |                32.08 |          32.09 |\n",
      "+------------+----------------------+----------------+\n",
      "|     300091 |                30.49 |          30.7  |\n",
      "+------------+----------------------+----------------+\n",
      "|     302008 |                33.57 |          33.82 |\n",
      "+------------+----------------------+----------------+\n",
      "|     306005 |                30.4  |          30.58 |\n",
      "+------------+----------------------+----------------+\n",
      "|     361010 |                30.38 |          30.52 |\n",
      "+------------+----------------------+----------------+\n",
      "|      37073 |                31.15 |          31.22 |\n",
      "+------------+----------------------+----------------+\n",
      "|      41033 |                31.58 |          31.75 |\n",
      "+------------+----------------------+----------------+\n",
      "|      42012 |                31.9  |          32.1  |\n",
      "+------------+----------------------+----------------+\n",
      "|      42049 |                31.15 |          31.36 |\n",
      "+------------+----------------------+----------------+\n",
      "|      54082 |                30.06 |          30.2  |\n",
      "+------------+----------------------+----------------+\n",
      "|      76053 |                31.46 |          31.55 |\n",
      "+------------+----------------------+----------------+\n",
      "|       8023 |                33.91 |          33.95 |\n",
      "+------------+----------------------+----------------+\n",
      "\n",
      "Best Metrics (Image: 210088):\n",
      "Best Baseline PSNR: 34.23 dB\n",
      "Best SR PSNR: 34.42 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import skimage.metrics\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Model Components ---\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    \"\"\"2D Layer Normalization module.\"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class SpatialResidualModule(nn.Module):\n",
    "    \"\"\"Spatial attention-based residual module.\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "        self.spatial_att = nn.Sequential(\n",
    "            nn.Conv2d(channels, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([0.1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv(x)\n",
    "        att = self.spatial_att(out)\n",
    "        return residual + self.scale * (out * att)\n",
    "\n",
    "class EnhancedResidualBlock(nn.Module):\n",
    "    \"\"\"Enhanced residual block with spatial residual module.\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.norm1 = LayerNorm2d(channels)\n",
    "        self.norm2 = LayerNorm2d(channels)\n",
    "        self.act = nn.PReLU()\n",
    "        self.spatial_residual = SpatialResidualModule(channels)\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([0.1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.spatial_residual(out)\n",
    "        return residual + self.scale * out\n",
    "\n",
    "class EnhancedResidualGroup(nn.Module):\n",
    "    \"\"\"Group of enhanced residual blocks.\"\"\"\n",
    "    def __init__(self, channels, n_blocks):\n",
    "        super().__init__()\n",
    "        blocks = [EnhancedResidualBlock(channels) for _ in range(n_blocks)]\n",
    "        self.body = nn.Sequential(*blocks)\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([0.1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.body(x)\n",
    "        out = self.conv(out)\n",
    "        return residual + self.scale * out\n",
    "\n",
    "class EnhancedESPCN(nn.Module):\n",
    "    \"\"\"Enhanced Efficient Sub-Pixel Convolutional Neural Network.\"\"\"\n",
    "    def __init__(self, in_channels, scale_factor=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 128, kernel_size=3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(64, 3 * (scale_factor ** 2), kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(scale_factor)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Main Model ---\n",
    "\n",
    "class UltraEnhancedSR(nn.Module):\n",
    "    \"\"\"Ultra-enhanced super-resolution model.\"\"\"\n",
    "    def __init__(self, scale=2):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, 3, padding=1),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.body = nn.ModuleList([\n",
    "            EnhancedResidualGroup(128, 10) for _ in range(5)\n",
    "        ])\n",
    "        self.global_residual = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.upscale = EnhancedESPCN(128, scale)\n",
    "        self.direct_path = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(16, 3 * (scale ** 2), 3, padding=1),\n",
    "            nn.PixelShuffle(scale)\n",
    "        )\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, 3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(32, 3, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        direct = self.direct_path(x)\n",
    "        shallow = self.head(x)\n",
    "        deep = shallow\n",
    "        for block in self.body:\n",
    "            deep = block(deep)\n",
    "        global_res = self.global_residual(deep)\n",
    "        fused = shallow + global_res\n",
    "        upscaled = self.upscale(fused)\n",
    "        combined = torch.cat([direct, upscaled], dim=1)\n",
    "        return self.refine(combined)\n",
    "\n",
    "# --- Utilities ---\n",
    "\n",
    "class SRValidationDataset(Dataset):\n",
    "    \"\"\"Dataset for super-resolution validation.\"\"\"\n",
    "    def __init__(self, base_dir, scale=2):\n",
    "        lr_dir = os.path.join(base_dir, \"LR\")\n",
    "        hr_dir = os.path.join(base_dir, \"HR\")\n",
    "        \n",
    "        if not os.path.exists(lr_dir) or not os.path.exists(hr_dir):\n",
    "            raise ValueError(f\"LR or HR directory not found in {base_dir}\")\n",
    "        \n",
    "        lr_paths = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        hr_paths = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        \n",
    "        self.pairs = []\n",
    "        for lr_path in lr_paths:\n",
    "            lr_fname = os.path.basename(lr_path)\n",
    "            hr_path = os.path.join(hr_dir, lr_fname)\n",
    "            if hr_path in hr_paths:\n",
    "                try:\n",
    "                    lr_img = Image.open(lr_path).convert('RGB')\n",
    "                    hr_img = Image.open(hr_path).convert('RGB')\n",
    "                    if lr_img.size[0] < 7 or lr_img.size[1] < 7 or hr_img.size[0] < 7 or hr_img.size[1] < 7:\n",
    "                        print(f\"Skipping {lr_fname}: Image too small (LR: {lr_img.size}, HR: {hr_img.size})\")\n",
    "                        continue\n",
    "                    self.pairs.append((lr_path, hr_path))\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping {lr_fname}: Failed to load (Error: {e})\")\n",
    "                    continue\n",
    "        \n",
    "        if not self.pairs:\n",
    "            raise ValueError(f\"No valid LR-HR image pairs found in {base_dir}.\")\n",
    "        \n",
    "        self.scale = scale\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_path, hr_path = self.pairs[idx]\n",
    "        try:\n",
    "            lr = Image.open(lr_path).convert('RGB').resize((512, 512), Image.BICUBIC)\n",
    "            hr = Image.open(hr_path).convert('RGB').resize((1024, 1024), Image.BICUBIC)\n",
    "            lr = self.transform(lr)\n",
    "            hr = self.transform(hr)\n",
    "            return lr, hr, os.path.basename(lr_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {lr_path}: {e}\")\n",
    "            return None, None, \"error\"\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"Calculate PSNR between two images.\"\"\"\n",
    "    img1_np = img1.squeeze(0).detach().cpu().clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "    img2_np = img2.squeeze(0).detach().cpu().permute(1, 2, 0).numpy()\n",
    "    psnr = skimage.metrics.peak_signal_noise_ratio(img2_np, img1_np, data_range=1.0)\n",
    "    return psnr\n",
    "\n",
    "def save_comparison(lr, sr, hr, filename, output_dir):\n",
    "    \"\"\"Save comparison of LR, SR, and HR images.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    lr_img = lr.squeeze(0).cpu().permute(1, 2, 0).numpy().astype(np.float32)\n",
    "    sr_img = sr.squeeze(0).cpu().clamp(0, 1).permute(1, 2, 0).numpy().astype(np.float32)\n",
    "    hr_img = hr.squeeze(0).cpu().permute(1, 2, 0).numpy().astype(np.float32)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axes[0].imshow(lr_img)\n",
    "    axes[0].set_title(f'Low Resolution\\n{lr_img.shape[1]}x{lr_img.shape[0]}')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(sr_img)\n",
    "    axes[1].set_title(f'Super Resolution\\n{sr_img.shape[1]}x{sr_img.shape[0]}')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(hr_img)\n",
    "    axes[2].set_title(f'High Resolution\\n{hr_img.shape[1]}x{hr_img.shape[0]}')\n",
    "    axes[2].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{filename}.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_super_resolved(sr, filename, output_dir):\n",
    "    \"\"\"Save super-resolved image.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    sr_img = sr.squeeze(0).cpu().clamp(0, 1).permute(1, 2, 0).numpy().astype(np.float32)\n",
    "    sr_pil = Image.fromarray((sr_img * 255).astype(np.uint8))\n",
    "    sr_pil.save(os.path.join(output_dir, f\"{filename}_sr.png\"))\n",
    "\n",
    "def validate_model(model, data_loader, output_dir, sr_output_dir):\n",
    "    \"\"\"Validate the super-resolution model on the dataset.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    best_sr_psnr = 0\n",
    "    best_lr_psnr = 0\n",
    "    best_image = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr, hr, fname in data_loader:\n",
    "            if lr is None or hr is None:\n",
    "                print(f\"Skipping invalid image: {fname[0]}\")\n",
    "                continue\n",
    "            lr, hr = lr.cuda(), hr.cuda()\n",
    "            sr = model(lr)\n",
    "            \n",
    "            sr_psnr = calculate_psnr(sr, hr)\n",
    "            lr_upscaled = torch.nn.functional.interpolate(lr, size=(1024, 1024), mode='bilinear', align_corners=False)\n",
    "            lr_psnr = calculate_psnr(lr_upscaled, hr)\n",
    "            \n",
    "            fname_base = fname[0].split('.')[0]\n",
    "            \n",
    "            if sr_psnr > 30 and sr_psnr > lr_psnr:\n",
    "                results.append([fname_base, f\"{lr_psnr:.2f}\", f\"{sr_psnr:.2f}\"])\n",
    "            \n",
    "            if sr_psnr > lr_psnr and sr_psnr > best_sr_psnr:\n",
    "                best_sr_psnr = sr_psnr\n",
    "                best_lr_psnr = lr_psnr\n",
    "                best_image = fname_base\n",
    "            \n",
    "            save_super_resolved(sr, fname_base, sr_output_dir)\n",
    "            save_comparison(lr, sr, hr, fname_base, output_dir)\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nBSD100 Evaluation Results:\")\n",
    "        print(tabulate(\n",
    "            results,\n",
    "            headers=['Image ID', 'PSNR Baseline (dB)', 'PSNR SR (dB)'],\n",
    "            tablefmt='grid'\n",
    "        ))\n",
    "    \n",
    "    print(f\"\\nBest Metrics (Image: {best_image}):\")\n",
    "    print(f\"Best Baseline PSNR: {best_lr_psnr:.2f} dB\")\n",
    "    print(f\"Best SR PSNR: {best_sr_psnr:.2f} dB\")\n",
    "    \n",
    "    return results, best_sr_psnr, best_lr_psnr, best_image\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to validate the super-resolution model on BSD100.\"\"\"\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    bsd100_dir = \"bsd100\"\n",
    "    results_dir = \"results\"\n",
    "    bsd100_output_dir = os.path.join(results_dir, \"bsd100_results\")\n",
    "    bsd100_sr_output_dir = os.path.join(results_dir, \"super_resolved_bsd100\")\n",
    "\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    best_model_path = None\n",
    "    best_psnr = 0\n",
    "    for f in os.listdir(checkpoint_dir):\n",
    "        if f.startswith(\"best_model_epoch\") and f.endswith(\".pth\"):\n",
    "            psnr_str = f.split(\"_psnr_\")[1].split(\".pth\")[0]\n",
    "            psnr = float(psnr_str)\n",
    "            if psnr > best_psnr:\n",
    "                best_psnr = psnr\n",
    "                best_model_path = os.path.join(checkpoint_dir, f)\n",
    "    \n",
    "    if best_model_path is None:\n",
    "        raise FileNotFoundError(\"No best model found in checkpoints directory\")\n",
    "\n",
    "    model = UltraEnhancedSR(scale=2).cuda()\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.eval()\n",
    "\n",
    "    dataset = SRValidationDataset(bsd100_dir, scale=2)\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    validate_model(model, data_loader, bsd100_output_dir, bsd100_sr_output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.cuda.empty_cache()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d061788-c845-491f-9eec-a3c0758cfc73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
